{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Adaptive_SNR_Testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neelabhro/Deep-Learning-based-Wireless-Communications/blob/main/Adaptive_SNR_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIoYASfEf7go"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKl9e4A0H1lk",
        "outputId": "cd476009-a66f-4eb4-b9fe-6270df501dfb"
      },
      "source": [
        "# magic command to use TF 1.X in colaboraty when importing tensorflow\n",
        "%tensorflow_version 1.x \n",
        "import tensorflow as tf                       # imports the tensorflow library to the python kernel\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)    # sets the amount of debug information from TF (INFO, WARNING, ERROR)\n",
        "import time\n",
        "import random\n",
        "print(\"Using tensorflow version:\", tf.__version__)\n",
        "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using tensorflow version: 1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiCDuiGqf7gr"
      },
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "import math\n",
        "pi = tf.constant(math.pi)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrLsO1Nxf7g3"
      },
      "source": [
        "#### System parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spN0MeqFD7x-",
        "outputId": "015cd6df-1e91-40cf-b7cb-8a29d539fea7"
      },
      "source": [
        "batch_size = 1000\n",
        "########## BIT FLIPPING ON\n",
        "#print(tr)\n",
        "#plt.plot(t, tr)\n",
        "T1 = random.randint(1, 10)\n",
        "T2 = random.randint(1, 10)\n",
        "M = 256\n",
        "t = np.linspace(0, 1, batch_size)\n",
        "triangle1 = signal.sawtooth(T1 * np.pi * 5 * t, 0.5)\n",
        "triangle1 = M/2*(triangle1)\n",
        "triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "triangle2 = signal.sawtooth(T2 * np.pi * 5 * t, 0.5)\n",
        "triangle2 = M/2*(triangle2)\n",
        "triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "triangle3 = triangle1 + triangle2\n",
        "#triangle3 = triangle3/15*(7)\n",
        "tr = triangle3\n",
        "#bins = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])\n",
        "bins = np.arange(M-1)\n",
        "tr = np.digitize(triangle3, bins, right=True)\n",
        "#plt.plot(t, triangle3)\n",
        "\n",
        "#tr = np.flip(tr)\n",
        "#print(tr)\n",
        "tr = np.floor(np.random.uniform(0,M, 10000))\n",
        "print(tr)\n",
        "#replacements = {0:7, 1:6, 2:5, 3:4, 4:3, 5:2, 6:1, 7:0}\n",
        "#replacements = {0:15, 1:14, 2:13, 3:12, 4:11, 5:10, 6:9, 7:8, 8:7, 9:6, 10:5, 11:4, 12:3, 13:2, 14:1, 15:0}\n",
        "rp1 = np.arange(M)\n",
        "rp2 = np.flip(np.arange(M))\n",
        "replacements = dict(zip(rp1,rp2))\n",
        "#print(rp2)\n",
        "replacer = replacements.get\n",
        "tr = ([replacer(n, n) for n in tr])\n",
        "\n",
        "#tr2 = ([replacer(n, n) for n in triangle3])\n",
        "print(tr)\n",
        "#plt.plot(t, tr)\n",
        "#plt.legend(['Input signal', 'Quantized and bit flipped'])\n",
        "\n",
        "#s_ind = np.empty(shape=(M,batch_size))\n",
        "s_ind = {}\n",
        "for j in range(M):\n",
        "  s_ind[j] = [i for i, x in enumerate(tr) if x == j]\n",
        "\n",
        "#print([tr[x] for x in s_ind_0])"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[156.  58. 226. ... 127. 105. 185.]\n",
            "[99, 197, 29, 207, 201, 133, 110, 32, 239, 129, 151, 80, 93, 86, 126, 227, 187, 232, 54, 81, 173, 181, 198, 212, 83, 29, 199, 206, 193, 71, 51, 160, 122, 111, 121, 168, 102, 175, 234, 149, 30, 141, 63, 203, 173, 41, 111, 178, 180, 172, 142, 41, 61, 82, 64, 105, 46, 240, 159, 237, 188, 73, 35, 173, 248, 161, 145, 100, 198, 217, 232, 250, 146, 179, 156, 23, 238, 159, 158, 169, 221, 183, 10, 50, 190, 215, 74, 36, 26, 166, 157, 99, 122, 211, 214, 21, 74, 142, 108, 66, 62, 109, 85, 42, 253, 140, 48, 239, 186, 29, 24, 131, 191, 79, 129, 255, 174, 223, 42, 60, 111, 134, 115, 191, 229, 121, 99, 162, 60, 71, 21, 189, 110, 9, 190, 227, 158, 7, 7, 194, 249, 117, 228, 187, 43, 138, 132, 6, 108, 233, 134, 94, 235, 252, 51, 32, 224, 175, 163, 218, 33, 171, 58, 28, 145, 111, 172, 13, 211, 43, 193, 48, 120, 114, 62, 119, 210, 6, 84, 82, 120, 5, 43, 230, 26, 137, 210, 217, 30, 39, 87, 63, 123, 247, 114, 246, 57, 87, 133, 97, 40, 158, 233, 253, 131, 191, 5, 33, 140, 167, 241, 157, 189, 55, 161, 207, 1, 179, 251, 35, 208, 5, 118, 219, 80, 82, 164, 198, 174, 255, 50, 221, 82, 194, 113, 153, 131, 255, 107, 36, 142, 227, 81, 179, 179, 141, 103, 55, 2, 243, 29, 32, 219, 45, 61, 176, 242, 111, 170, 8, 62, 133, 184, 188, 38, 35, 161, 49, 135, 3, 238, 41, 155, 169, 143, 233, 140, 160, 79, 236, 149, 18, 170, 189, 138, 206, 101, 78, 128, 157, 122, 146, 122, 21, 137, 242, 244, 20, 126, 151, 97, 143, 222, 42, 24, 105, 4, 201, 253, 158, 33, 202, 106, 191, 67, 200, 36, 93, 241, 130, 185, 60, 188, 128, 121, 167, 131, 9, 143, 9, 123, 184, 34, 3, 252, 82, 48, 20, 164, 212, 12, 116, 211, 25, 177, 162, 218, 208, 99, 171, 138, 117, 252, 147, 195, 52, 101, 29, 178, 252, 183, 107, 160, 111, 7, 199, 74, 32, 25, 109, 11, 32, 184, 140, 248, 72, 137, 212, 175, 204, 214, 92, 174, 154, 48, 138, 123, 123, 129, 200, 100, 127, 202, 78, 186, 164, 76, 27, 191, 209, 200, 104, 186, 76, 47, 60, 216, 105, 16, 79, 13, 148, 254, 103, 46, 59, 142, 92, 145, 118, 72, 241, 241, 201, 215, 113, 147, 11, 38, 227, 190, 37, 233, 137, 95, 170, 195, 152, 250, 213, 34, 219, 100, 90, 162, 161, 255, 231, 150, 52, 9, 110, 166, 82, 1, 26, 4, 85, 17, 31, 178, 129, 3, 137, 212, 132, 99, 163, 89, 88, 223, 169, 215, 109, 97, 69, 210, 158, 153, 84, 255, 167, 82, 95, 91, 82, 152, 26, 48, 53, 45, 177, 51, 0, 20, 50, 8, 241, 253, 32, 147, 94, 247, 170, 89, 149, 209, 24, 207, 48, 210, 75, 95, 60, 254, 81, 230, 248, 4, 170, 120, 221, 147, 30, 198, 202, 33, 34, 96, 111, 183, 62, 102, 102, 178, 73, 45, 11, 163, 76, 238, 65, 155, 39, 64, 25, 29, 93, 204, 28, 105, 13, 98, 108, 49, 146, 122, 164, 112, 63, 71, 54, 64, 234, 33, 70, 204, 247, 167, 61, 39, 229, 111, 10, 76, 22, 21, 42, 79, 0, 149, 187, 1, 11, 112, 251, 164, 97, 8, 254, 171, 144, 245, 23, 108, 92, 165, 98, 198, 82, 203, 60, 187, 130, 86, 127, 193, 184, 85, 56, 42, 169, 54, 56, 135, 216, 15, 52, 11, 22, 62, 183, 77, 102, 190, 187, 46, 22, 24, 113, 98, 82, 237, 203, 54, 242, 117, 53, 19, 71, 159, 190, 80, 172, 28, 149, 65, 91, 31, 53, 90, 221, 3, 117, 13, 180, 246, 236, 62, 46, 84, 151, 126, 165, 63, 110, 29, 10, 144, 16, 242, 97, 160, 220, 40, 30, 124, 60, 47, 174, 250, 205, 157, 5, 170, 156, 84, 146, 189, 82, 67, 197, 234, 144, 127, 8, 113, 38, 159, 122, 100, 167, 79, 199, 75, 36, 109, 4, 247, 151, 152, 253, 236, 79, 148, 188, 247, 36, 145, 16, 115, 178, 83, 164, 145, 3, 28, 95, 76, 12, 64, 227, 179, 1, 173, 181, 101, 161, 12, 248, 22, 169, 249, 235, 47, 162, 255, 110, 1, 32, 43, 249, 172, 148, 105, 153, 212, 210, 137, 52, 244, 20, 224, 47, 218, 95, 175, 215, 231, 60, 76, 227, 202, 197, 171, 44, 39, 213, 201, 251, 19, 177, 185, 240, 48, 254, 214, 0, 54, 249, 46, 242, 85, 132, 114, 134, 14, 114, 74, 215, 104, 107, 129, 17, 113, 153, 211, 84, 176, 93, 41, 79, 53, 137, 190, 108, 99, 108, 228, 26, 53, 44, 77, 211, 195, 155, 108, 26, 251, 95, 111, 114, 7, 209, 96, 200, 199, 211, 104, 198, 154, 120, 253, 168, 183, 21, 190, 126, 61, 92, 173, 37, 186, 54, 36, 203, 82, 133, 50, 246, 196, 99, 158, 107, 114, 215, 140, 88, 1, 58, 102, 241, 69, 93, 87, 232, 153, 32, 248, 1, 220, 35, 135, 130, 117, 175, 60, 200, 25, 91, 226, 169, 166, 207, 118, 252, 228, 208, 204, 216, 184, 162, 69, 45, 196, 64, 177, 83, 240, 56, 160, 248, 40, 45, 63, 242, 33, 11, 156, 131, 110, 62, 78, 143, 76, 202, 190, 76, 227, 82, 116, 247, 105, 106, 158, 218, 205, 45, 181, 237, 232, 2, 127, 115, 99, 75, 91, 191, 239, 97, 142, 32, 164, 92, 253, 109, 217, 11, 239, 176, 188, 247, 57, 215, 131, 250, 136, 100, 60, 218, 170, 190, 202, 234, 77, 31, 227, 239, 193, 21, 212, 150, 99, 38, 213, 19, 26, 219, 194, 98, 49, 138, 218, 14, 86, 40, 225, 132, 45, 146, 3, 158, 144, 107, 73, 161, 67, 152, 58, 145, 97, 138, 246, 255, 54, 34, 84, 144, 111, 44, 145, 87, 204, 11, 125, 26, 186, 62, 97, 96, 113, 240, 219, 172, 43, 201, 161, 237, 38, 105, 205, 23, 83, 178, 4, 9, 203, 191, 42, 26, 81, 91, 68, 245, 89, 43, 74, 73, 198, 91, 245, 119, 165, 146, 250, 67, 199, 28, 72, 55, 74, 142, 242, 104, 72, 236, 93, 39, 66, 116, 91, 234, 229, 134, 170, 56, 24, 188, 215, 232, 67, 61, 139, 167, 69, 84, 9, 204, 92, 232, 37, 120, 117, 212, 19, 210, 220, 194, 26, 147, 148, 51, 203, 198, 76, 49, 176, 181, 153, 220, 52, 101, 150, 17, 204, 241, 62, 234, 43, 196, 34, 218, 51, 79, 21, 59, 205, 141, 11, 124, 88, 179, 118, 226, 218, 123, 65, 234, 89, 209, 100, 12, 224, 104, 64, 204, 92, 171, 244, 68, 72, 149, 184, 102, 235, 10, 247, 82, 239, 245, 116, 92, 88, 122, 79, 84, 196, 201, 59, 220, 204, 254, 81, 55, 6, 3, 81, 140, 80, 50, 29, 3, 222, 26, 215, 200, 165, 187, 40, 103, 44, 24, 30, 124, 86, 215, 195, 127, 101, 126, 64, 95, 221, 73, 10, 146, 208, 121, 37, 230, 12, 213, 202, 120, 83, 187, 30, 42, 109, 161, 240, 182, 63, 228, 92, 251, 75, 201, 246, 248, 255, 31, 183, 144, 167, 11, 186, 8, 72, 154, 57, 62, 23, 96, 94, 78, 13, 103, 92, 70, 225, 237, 35, 244, 96, 148, 244, 2, 164, 16, 114, 161, 216, 172, 59, 230, 124, 195, 85, 178, 159, 134, 97, 90, 9, 221, 135, 53, 34, 50, 169, 137, 198, 49, 242, 119, 177, 11, 196, 186, 1, 58, 77, 172, 123, 36, 141, 48, 111, 145, 139, 194, 44, 126, 111, 232, 171, 153, 218, 31, 247, 104, 242, 0, 61, 121, 54, 40, 200, 113, 81, 132, 161, 176, 107, 1, 116, 220, 88, 119, 134, 23, 60, 194, 110, 49, 18, 215, 114, 245, 33, 141, 51, 99, 239, 230, 221, 156, 246, 52, 248, 161, 34, 138, 142, 168, 231, 138, 92, 132, 136, 165, 206, 91, 32, 6, 214, 199, 117, 70, 205, 36, 196, 132, 168, 169, 136, 147, 128, 49, 193, 34, 168, 199, 37, 51, 142, 53, 234, 137, 115, 52, 193, 225, 136, 73, 239, 196, 222, 6, 155, 240, 67, 208, 29, 105, 215, 159, 45, 172, 97, 145, 169, 70, 235, 39, 123, 199, 102, 44, 98, 249, 69, 232, 147, 252, 29, 43, 162, 43, 229, 35, 221, 13, 196, 239, 170, 56, 194, 140, 225, 210, 83, 147, 46, 232, 159, 19, 111, 138, 138, 172, 197, 170, 28, 42, 155, 73, 121, 150, 190, 236, 166, 143, 239, 196, 168, 217, 9, 181, 179, 29, 223, 96, 43, 247, 230, 157, 27, 69, 160, 150, 146, 1, 119, 142, 0, 73, 40, 59, 134, 179, 147, 43, 248, 71, 135, 234, 215, 168, 130, 72, 142, 4, 181, 16, 57, 64, 31, 152, 21, 242, 122, 116, 48, 6, 103, 249, 21, 110, 214, 41, 116, 111, 20, 41, 233, 71, 41, 94, 170, 62, 152, 134, 120, 15, 50, 187, 183, 134, 17, 155, 7, 6, 83, 106, 240, 159, 200, 195, 35, 60, 250, 43, 24, 150, 73, 223, 253, 213, 95, 95, 157, 156, 182, 106, 111, 42, 19, 52, 27, 231, 81, 197, 212, 133, 206, 239, 48, 34, 48, 0, 169, 89, 207, 31, 33, 186, 21, 147, 144, 219, 52, 187, 248, 90, 173, 54, 254, 217, 183, 254, 81, 56, 32, 104, 155, 207, 36, 92, 185, 84, 109, 38, 56, 22, 45, 53, 193, 138, 3, 43, 52, 176, 120, 206, 18, 140, 186, 76, 231, 142, 89, 244, 165, 119, 67, 69, 48, 176, 188, 46, 92, 86, 137, 163, 79, 1, 208, 231, 111, 78, 69, 162, 116, 11, 19, 2, 201, 128, 230, 241, 213, 101, 18, 205, 229, 251, 70, 111, 99, 53, 199, 196, 45, 53, 16, 242, 250, 114, 247, 65, 88, 208, 25, 233, 34, 58, 114, 25, 177, 0, 238, 214, 170, 239, 97, 163, 155, 186, 107, 20, 15, 239, 108, 78, 44, 123, 243, 55, 134, 76, 195, 196, 33, 205, 29, 119, 49, 218, 208, 112, 253, 138, 99, 234, 173, 123, 51, 149, 167, 56, 141, 16, 67, 90, 45, 90, 222, 159, 80, 182, 5, 241, 126, 40, 37, 220, 73, 173, 108, 111, 197, 126, 13, 120, 77, 97, 47, 17, 53, 91, 138, 69, 75, 104, 81, 39, 30, 204, 59, 233, 92, 82, 188, 192, 19, 192, 237, 114, 238, 172, 14, 6, 140, 130, 12, 86, 13, 226, 165, 93, 176, 240, 153, 201, 2, 43, 76, 146, 122, 201, 194, 107, 159, 107, 83, 56, 68, 112, 61, 233, 151, 52, 139, 182, 50, 236, 188, 168, 73, 109, 213, 34, 99, 53, 62, 42, 35, 78, 219, 143, 253, 21, 146, 121, 207, 225, 7, 40, 255, 250, 115, 31, 208, 115, 117, 178, 105, 24, 232, 134, 27, 117, 162, 87, 193, 199, 235, 238, 242, 155, 66, 180, 101, 139, 171, 224, 201, 205, 253, 8, 122, 245, 174, 4, 10, 148, 125, 14, 254, 221, 79, 200, 17, 80, 63, 122, 203, 249, 38, 59, 15, 27, 152, 84, 97, 141, 208, 27, 243, 76, 65, 144, 113, 2, 89, 248, 178, 100, 225, 162, 63, 52, 67, 89, 184, 85, 162, 226, 228, 86, 164, 219, 244, 175, 157, 34, 100, 230, 178, 111, 218, 172, 103, 226, 52, 64, 164, 246, 78, 210, 3, 37, 198, 236, 29, 71, 90, 29, 252, 142, 199, 68, 125, 42, 57, 254, 128, 133, 26, 253, 164, 142, 148, 252, 8, 240, 69, 154, 108, 79, 208, 112, 247, 122, 18, 107, 183, 1, 167, 249, 235, 12, 211, 1, 83, 240, 6, 195, 141, 15, 106, 87, 220, 222, 243, 0, 203, 251, 61, 44, 181, 4, 0, 199, 116, 123, 167, 42, 59, 94, 5, 213, 235, 63, 191, 201, 250, 225, 41, 62, 151, 246, 18, 20, 240, 204, 160, 162, 57, 110, 225, 201, 29, 150, 52, 93, 54, 52, 188, 205, 192, 189, 160, 178, 2, 223, 66, 226, 37, 208, 29, 120, 217, 155, 179, 222, 124, 49, 199, 210, 50, 249, 27, 36, 187, 115, 4, 180, 168, 73, 62, 27, 14, 145, 176, 249, 61, 11, 236, 141, 96, 124, 195, 99, 160, 174, 119, 202, 95, 97, 158, 195, 42, 25, 23, 96, 32, 112, 216, 142, 7, 115, 76, 229, 179, 16, 121, 103, 127, 119, 190, 80, 146, 38, 53, 209, 49, 220, 143, 20, 100, 87, 206, 65, 9, 115, 233, 39, 59, 249, 109, 127, 111, 220, 57, 34, 223, 128, 86, 115, 97, 120, 103, 144, 38, 149, 249, 117, 53, 58, 148, 156, 217, 102, 132, 227, 93, 223, 234, 75, 50, 52, 211, 13, 255, 209, 217, 35, 103, 149, 87, 218, 21, 229, 170, 77, 148, 123, 164, 243, 10, 153, 205, 28, 35, 115, 5, 151, 133, 44, 226, 191, 207, 152, 137, 59, 223, 121, 86, 50, 62, 205, 70, 42, 112, 74, 145, 215, 178, 162, 155, 142, 254, 38, 137, 201, 88, 92, 66, 180, 249, 180, 81, 21, 120, 253, 39, 154, 140, 55, 231, 36, 147, 249, 181, 161, 212, 194, 121, 251, 103, 230, 12, 9, 21, 161, 103, 109, 88, 1, 208, 136, 202, 119, 150, 247, 129, 63, 197, 241, 171, 172, 186, 181, 222, 246, 47, 196, 130, 111, 79, 140, 19, 37, 87, 217, 113, 186, 138, 86, 13, 134, 194, 177, 247, 147, 52, 49, 97, 96, 39, 182, 127, 32, 44, 42, 170, 48, 118, 224, 16, 84, 37, 43, 156, 182, 186, 212, 33, 85, 142, 82, 14, 200, 72, 7, 59, 100, 67, 168, 39, 164, 146, 226, 180, 198, 176, 112, 236, 30, 149, 15, 250, 33, 84, 39, 176, 200, 230, 106, 65, 192, 27, 57, 35, 137, 28, 28, 153, 204, 97, 1, 33, 213, 164, 109, 51, 203, 140, 137, 230, 74, 77, 205, 157, 201, 44, 156, 174, 224, 52, 102, 93, 182, 140, 11, 18, 171, 167, 188, 239, 202, 88, 45, 220, 242, 19, 130, 168, 160, 227, 56, 28, 230, 75, 137, 167, 232, 225, 136, 71, 17, 24, 9, 33, 144, 17, 176, 168, 200, 192, 191, 240, 33, 83, 238, 251, 58, 123, 245, 237, 164, 238, 233, 136, 166, 47, 197, 91, 121, 76, 4, 13, 237, 168, 48, 171, 101, 170, 175, 47, 205, 103, 240, 184, 12, 209, 9, 241, 16, 194, 232, 210, 41, 49, 131, 109, 97, 166, 223, 212, 27, 216, 46, 138, 169, 191, 89, 38, 238, 110, 224, 180, 121, 194, 178, 182, 193, 161, 105, 118, 21, 143, 203, 216, 142, 124, 181, 193, 1, 95, 166, 59, 77, 71, 63, 226, 208, 150, 94, 164, 214, 238, 203, 230, 45, 52, 55, 181, 79, 205, 235, 100, 12, 240, 43, 244, 119, 214, 179, 28, 252, 221, 235, 61, 34, 32, 74, 113, 171, 233, 250, 253, 27, 22, 105, 29, 92, 63, 55, 149, 189, 39, 216, 214, 50, 29, 225, 69, 153, 194, 107, 135, 183, 49, 141, 230, 119, 143, 10, 218, 77, 152, 251, 156, 222, 109, 111, 238, 97, 231, 162, 100, 56, 116, 97, 13, 114, 11, 233, 88, 247, 71, 50, 118, 75, 113, 36, 119, 106, 192, 53, 135, 22, 236, 155, 76, 74, 66, 177, 98, 228, 167, 6, 191, 85, 179, 220, 196, 241, 157, 110, 38, 189, 59, 13, 101, 84, 193, 137, 238, 49, 252, 128, 95, 247, 91, 159, 238, 72, 10, 71, 78, 9, 72, 2, 36, 148, 108, 236, 37, 12, 205, 232, 187, 57, 143, 185, 90, 54, 157, 24, 0, 161, 150, 69, 75, 39, 223, 125, 240, 29, 217, 238, 103, 85, 251, 10, 99, 59, 215, 166, 162, 90, 232, 158, 88, 13, 72, 95, 19, 208, 170, 16, 149, 245, 167, 20, 73, 241, 30, 110, 25, 13, 162, 49, 240, 147, 216, 134, 197, 115, 151, 161, 60, 32, 226, 241, 192, 106, 234, 72, 75, 119, 80, 52, 87, 202, 84, 225, 80, 88, 208, 186, 239, 178, 235, 12, 231, 178, 215, 203, 122, 134, 43, 106, 98, 187, 150, 82, 83, 80, 244, 129, 6, 75, 159, 143, 139, 196, 39, 136, 248, 150, 165, 204, 156, 138, 11, 103, 173, 225, 187, 122, 155, 136, 254, 220, 103, 148, 142, 183, 188, 4, 37, 192, 131, 130, 92, 45, 114, 106, 88, 49, 15, 5, 68, 17, 166, 224, 149, 160, 9, 98, 184, 113, 104, 47, 206, 128, 201, 87, 29, 55, 174, 80, 31, 210, 50, 198, 114, 183, 13, 90, 17, 44, 204, 38, 175, 82, 137, 144, 233, 0, 187, 41, 174, 84, 135, 126, 37, 120, 183, 79, 188, 82, 181, 11, 246, 85, 39, 243, 191, 6, 25, 52, 124, 5, 143, 115, 219, 174, 234, 195, 203, 42, 184, 152, 158, 76, 231, 89, 216, 195, 236, 97, 101, 171, 47, 119, 207, 110, 187, 131, 148, 124, 178, 137, 221, 248, 9, 192, 25, 188, 108, 55, 130, 102, 155, 172, 204, 216, 249, 56, 169, 23, 168, 37, 65, 201, 40, 222, 50, 5, 149, 17, 127, 240, 180, 72, 153, 29, 54, 226, 11, 179, 124, 121, 201, 70, 253, 4, 127, 31, 220, 18, 123, 48, 172, 20, 181, 3, 202, 241, 253, 192, 168, 118, 68, 34, 57, 41, 37, 193, 26, 144, 172, 191, 32, 192, 221, 24, 116, 90, 210, 246, 218, 21, 228, 13, 126, 232, 83, 145, 241, 186, 58, 15, 246, 85, 201, 4, 43, 115, 68, 165, 33, 158, 122, 23, 70, 61, 249, 154, 139, 167, 251, 133, 187, 161, 11, 155, 217, 151, 120, 15, 143, 43, 104, 200, 121, 28, 154, 111, 81, 78, 82, 227, 21, 163, 24, 75, 115, 177, 139, 1, 22, 133, 144, 116, 60, 187, 237, 164, 199, 60, 160, 19, 182, 14, 122, 205, 147, 45, 188, 98, 254, 137, 47, 227, 220, 169, 47, 13, 102, 133, 213, 6, 255, 156, 9, 157, 155, 177, 128, 39, 90, 93, 58, 13, 87, 5, 90, 53, 191, 140, 127, 42, 189, 71, 9, 99, 180, 204, 227, 81, 70, 144, 88, 56, 30, 168, 42, 199, 64, 193, 184, 90, 132, 225, 173, 203, 240, 95, 52, 140, 224, 80, 101, 43, 57, 8, 139, 38, 93, 150, 208, 130, 49, 44, 58, 243, 77, 6, 47, 204, 153, 140, 25, 153, 23, 211, 88, 72, 158, 117, 199, 35, 22, 213, 198, 167, 199, 23, 48, 193, 6, 89, 244, 230, 118, 228, 179, 187, 45, 153, 251, 63, 246, 51, 14, 56, 182, 70, 79, 110, 152, 110, 159, 165, 179, 105, 198, 3, 120, 95, 106, 221, 120, 217, 221, 234, 111, 55, 222, 30, 117, 67, 91, 93, 172, 28, 50, 56, 123, 211, 50, 117, 216, 59, 122, 61, 63, 204, 33, 29, 140, 111, 156, 44, 16, 97, 229, 189, 162, 174, 250, 66, 39, 135, 173, 97, 219, 101, 154, 153, 146, 66, 97, 250, 239, 213, 187, 66, 220, 142, 154, 189, 28, 82, 41, 83, 48, 46, 187, 127, 22, 65, 203, 189, 172, 201, 83, 149, 170, 93, 137, 139, 67, 247, 89, 6, 247, 177, 224, 60, 50, 44, 150, 140, 101, 143, 16, 214, 89, 231, 52, 144, 215, 80, 243, 209, 158, 228, 98, 41, 237, 3, 156, 80, 53, 96, 224, 232, 1, 8, 2, 227, 110, 27, 36, 109, 241, 122, 136, 140, 118, 99, 19, 222, 246, 69, 64, 181, 90, 44, 11, 63, 126, 41, 211, 66, 226, 28, 230, 108, 186, 12, 119, 19, 177, 51, 187, 209, 87, 69, 32, 179, 114, 224, 0, 57, 210, 48, 138, 46, 58, 144, 178, 87, 191, 217, 233, 192, 240, 161, 133, 187, 26, 145, 72, 252, 87, 230, 132, 47, 59, 181, 235, 97, 179, 33, 205, 108, 234, 217, 32, 53, 116, 215, 193, 250, 233, 21, 228, 241, 136, 164, 31, 177, 113, 39, 159, 37, 226, 166, 102, 90, 144, 216, 239, 162, 223, 18, 90, 103, 248, 154, 108, 183, 147, 192, 231, 72, 129, 186, 73, 165, 10, 151, 14, 35, 198, 196, 203, 39, 102, 162, 14, 197, 241, 41, 230, 66, 171, 198, 67, 238, 66, 74, 78, 87, 215, 43, 2, 246, 94, 246, 175, 7, 37, 63, 252, 40, 213, 187, 31, 235, 87, 235, 9, 163, 245, 81, 204, 3, 86, 53, 178, 125, 63, 52, 119, 223, 10, 66, 20, 151, 60, 7, 213, 252, 19, 153, 207, 124, 144, 208, 26, 115, 91, 250, 155, 245, 175, 125, 166, 6, 38, 167, 24, 4, 133, 11, 49, 65, 14, 123, 186, 15, 244, 224, 140, 195, 118, 37, 47, 128, 172, 49, 150, 255, 59, 51, 106, 158, 153, 123, 165, 5, 77, 38, 21, 30, 21, 86, 138, 179, 84, 166, 192, 195, 70, 150, 29, 177, 232, 136, 4, 170, 128, 231, 153, 76, 213, 97, 160, 44, 225, 244, 108, 136, 42, 5, 183, 218, 5, 0, 92, 118, 41, 102, 129, 246, 145, 152, 200, 178, 23, 174, 100, 159, 222, 231, 26, 151, 105, 113, 104, 159, 124, 7, 104, 254, 161, 138, 72, 148, 182, 202, 36, 182, 253, 81, 214, 14, 93, 191, 138, 109, 179, 242, 150, 165, 174, 45, 94, 205, 83, 191, 22, 82, 116, 177, 245, 140, 97, 3, 50, 158, 40, 166, 137, 92, 18, 29, 223, 155, 103, 107, 145, 57, 40, 113, 89, 31, 196, 241, 3, 27, 81, 209, 138, 186, 89, 6, 239, 80, 255, 82, 71, 252, 249, 212, 77, 196, 219, 224, 60, 17, 24, 154, 127, 9, 212, 204, 254, 105, 94, 251, 42, 215, 28, 203, 108, 198, 28, 2, 97, 63, 122, 192, 212, 230, 96, 159, 186, 117, 240, 226, 37, 103, 188, 21, 117, 210, 128, 66, 178, 189, 214, 91, 0, 165, 85, 59, 23, 252, 89, 117, 59, 253, 54, 183, 205, 189, 255, 153, 24, 165, 74, 195, 83, 154, 185, 243, 184, 98, 156, 76, 135, 127, 233, 202, 217, 254, 217, 54, 10, 51, 186, 207, 188, 132, 30, 124, 221, 162, 44, 233, 247, 24, 107, 218, 197, 189, 38, 78, 173, 129, 182, 63, 75, 95, 60, 14, 30, 199, 237, 5, 117, 30, 57, 244, 203, 58, 34, 22, 110, 123, 223, 154, 247, 72, 191, 105, 134, 170, 246, 33, 135, 126, 210, 81, 16, 16, 103, 73, 244, 218, 233, 174, 78, 181, 9, 68, 124, 11, 210, 134, 112, 47, 135, 235, 155, 164, 65, 54, 252, 67, 166, 231, 59, 56, 253, 144, 224, 212, 161, 140, 12, 219, 93, 27, 114, 96, 115, 31, 14, 230, 70, 200, 113, 239, 68, 158, 204, 115, 58, 247, 12, 184, 215, 54, 25, 150, 111, 46, 221, 208, 228, 130, 54, 64, 136, 112, 245, 31, 31, 230, 211, 47, 129, 105, 177, 138, 120, 128, 60, 232, 101, 134, 239, 201, 109, 220, 33, 10, 96, 56, 239, 254, 19, 176, 211, 3, 154, 222, 196, 202, 201, 18, 137, 207, 200, 102, 110, 42, 229, 132, 132, 167, 108, 87, 160, 206, 96, 33, 166, 129, 139, 72, 63, 201, 52, 140, 133, 9, 225, 207, 149, 33, 40, 166, 236, 111, 154, 45, 245, 58, 123, 207, 173, 82, 74, 32, 17, 66, 144, 168, 247, 132, 80, 27, 27, 7, 29, 129, 130, 169, 6, 203, 162, 170, 246, 52, 188, 27, 68, 13, 105, 253, 93, 204, 250, 89, 243, 12, 158, 189, 192, 90, 181, 228, 61, 244, 21, 107, 22, 90, 67, 204, 238, 187, 151, 203, 154, 52, 194, 52, 108, 114, 124, 177, 148, 172, 92, 205, 114, 2, 161, 42, 100, 35, 79, 73, 157, 5, 82, 36, 50, 4, 164, 24, 126, 254, 205, 180, 208, 74, 24, 86, 190, 14, 183, 4, 247, 216, 107, 116, 183, 231, 66, 223, 117, 95, 130, 115, 220, 124, 123, 58, 182, 248, 138, 189, 205, 57, 101, 5, 13, 235, 174, 45, 39, 95, 69, 42, 167, 58, 208, 55, 87, 139, 50, 44, 233, 81, 103, 36, 96, 217, 173, 208, 199, 67, 224, 140, 82, 214, 110, 217, 48, 191, 42, 75, 246, 196, 31, 150, 131, 112, 51, 154, 64, 39, 6, 88, 76, 6, 102, 29, 236, 248, 10, 0, 107, 69, 103, 131, 10, 101, 15, 2, 128, 223, 92, 38, 153, 143, 104, 159, 92, 100, 199, 122, 120, 47, 166, 25, 175, 60, 116, 12, 140, 188, 146, 33, 193, 87, 204, 201, 178, 134, 101, 130, 49, 68, 49, 207, 235, 33, 220, 126, 158, 135, 43, 99, 56, 6, 132, 90, 135, 143, 101, 201, 205, 43, 69, 168, 4, 180, 82, 22, 55, 208, 15, 73, 222, 19, 233, 160, 203, 6, 241, 50, 248, 188, 112, 67, 218, 48, 248, 127, 25, 19, 37, 195, 15, 134, 6, 189, 184, 161, 87, 184, 32, 65, 239, 48, 97, 222, 0, 115, 199, 58, 20, 137, 82, 69, 199, 45, 157, 127, 164, 105, 136, 44, 142, 183, 206, 43, 133, 13, 68, 243, 61, 221, 77, 116, 123, 197, 251, 15, 65, 191, 35, 249, 148, 137, 214, 93, 129, 41, 237, 121, 111, 59, 211, 179, 34, 126, 44, 202, 215, 148, 118, 11, 130, 10, 43, 120, 114, 143, 253, 176, 30, 17, 114, 60, 220, 209, 26, 106, 207, 113, 190, 189, 154, 84, 62, 3, 48, 184, 29, 16, 204, 240, 27, 100, 116, 80, 135, 73, 67, 168, 171, 131, 152, 131, 102, 174, 149, 164, 253, 185, 152, 15, 95, 132, 231, 238, 134, 237, 110, 30, 30, 89, 100, 143, 22, 90, 71, 22, 180, 167, 5, 240, 70, 112, 74, 129, 116, 152, 158, 184, 101, 120, 94, 197, 251, 125, 97, 170, 122, 135, 217, 225, 229, 175, 86, 45, 241, 37, 135, 88, 116, 32, 243, 39, 40, 103, 7, 202, 9, 236, 12, 183, 216, 184, 127, 124, 11, 213, 11, 66, 199, 210, 195, 161, 16, 155, 230, 27, 173, 2, 86, 4, 20, 103, 55, 125, 229, 182, 107, 10, 228, 23, 106, 178, 147, 152, 253, 54, 47, 39, 8, 223, 175, 55, 123, 152, 103, 95, 30, 151, 224, 218, 18, 213, 169, 100, 247, 255, 214, 26, 107, 183, 121, 105, 160, 225, 35, 14, 221, 75, 222, 77, 174, 248, 233, 76, 193, 143, 106, 213, 176, 114, 33, 118, 10, 77, 175, 157, 120, 73, 95, 99, 233, 190, 215, 45, 96, 174, 187, 5, 55, 198, 123, 213, 161, 120, 89, 14, 17, 217, 89, 232, 87, 216, 142, 28, 65, 86, 159, 188, 95, 171, 150, 4, 169, 28, 144, 144, 41, 50, 209, 255, 148, 250, 235, 30, 120, 188, 41, 1, 153, 109, 28, 141, 123, 126, 188, 158, 192, 38, 225, 239, 234, 186, 53, 92, 205, 179, 175, 199, 248, 149, 83, 65, 26, 0, 84, 91, 17, 228, 201, 79, 11, 119, 14, 156, 253, 223, 166, 111, 105, 178, 10, 242, 86, 229, 3, 202, 137, 179, 117, 222, 20, 68, 157, 89, 48, 246, 152, 10, 251, 179, 39, 120, 12, 192, 5, 93, 118, 28, 18, 201, 120, 214, 192, 33, 124, 190, 108, 203, 218, 35, 198, 4, 214, 30, 244, 234, 186, 160, 254, 228, 45, 153, 145, 218, 247, 244, 60, 5, 120, 109, 20, 172, 123, 170, 80, 7, 197, 188, 156, 187, 34, 95, 92, 99, 100, 189, 95, 27, 17, 15, 8, 190, 94, 135, 111, 125, 200, 70, 174, 53, 171, 247, 77, 96, 255, 210, 79, 228, 173, 72, 225, 176, 226, 111, 141, 93, 74, 214, 187, 178, 244, 27, 28, 35, 200, 72, 33, 103, 40, 190, 119, 65, 225, 167, 255, 32, 212, 176, 203, 34, 249, 179, 228, 180, 214, 145, 217, 147, 53, 36, 121, 47, 59, 185, 200, 18, 116, 16, 82, 63, 210, 221, 83, 213, 221, 2, 255, 44, 127, 228, 81, 244, 204, 78, 9, 60, 23, 41, 185, 3, 29, 59, 194, 117, 34, 124, 162, 62, 193, 135, 24, 224, 189, 195, 72, 123, 39, 104, 40, 176, 18, 201, 111, 199, 170, 195, 141, 246, 202, 229, 197, 53, 51, 218, 18, 35, 42, 34, 213, 10, 212, 118, 140, 133, 10, 64, 130, 31, 145, 142, 120, 102, 56, 8, 175, 192, 87, 180, 212, 188, 118, 65, 7, 59, 204, 227, 43, 27, 133, 185, 67, 57, 130, 183, 129, 23, 113, 2, 122, 53, 249, 250, 167, 147, 71, 101, 103, 63, 234, 89, 156, 67, 206, 99, 177, 64, 192, 149, 23, 23, 229, 240, 175, 108, 248, 53, 19, 96, 97, 135, 137, 34, 52, 139, 141, 130, 68, 92, 136, 70, 100, 70, 197, 27, 60, 152, 24, 99, 143, 242, 174, 71, 164, 91, 103, 235, 115, 121, 38, 116, 7, 112, 3, 21, 210, 144, 121, 16, 117, 26, 213, 21, 71, 205, 57, 117, 87, 19, 100, 114, 78, 187, 211, 161, 115, 208, 42, 4, 51, 240, 72, 46, 129, 182, 242, 4, 86, 162, 55, 123, 17, 6, 199, 73, 46, 248, 175, 69, 202, 76, 106, 242, 101, 146, 60, 50, 133, 236, 216, 56, 211, 182, 63, 171, 73, 7, 217, 218, 196, 107, 57, 221, 238, 92, 252, 228, 163, 6, 231, 20, 104, 15, 241, 146, 158, 26, 80, 18, 33, 188, 109, 95, 206, 125, 155, 44, 248, 252, 227, 101, 222, 198, 210, 205, 175, 224, 76, 59, 22, 96, 199, 39, 214, 26, 135, 99, 77, 114, 53, 51, 40, 73, 89, 242, 207, 41, 180, 104, 98, 31, 160, 159, 80, 112, 30, 101, 88, 65, 191, 66, 2, 44, 55, 129, 62, 138, 105, 135, 116, 252, 5, 229, 229, 244, 137, 26, 181, 120, 100, 40, 201, 149, 16, 53, 30, 84, 188, 7, 150, 35, 32, 253, 42, 191, 168, 245, 174, 114, 82, 58, 96, 149, 190, 119, 85, 202, 115, 164, 39, 28, 217, 254, 91, 219, 101, 70, 137, 229, 211, 84, 221, 204, 21, 0, 220, 125, 31, 152, 124, 0, 51, 161, 252, 201, 169, 139, 247, 62, 65, 156, 73, 190, 171, 251, 178, 122, 38, 142, 44, 131, 158, 87, 122, 104, 160, 1, 41, 88, 242, 65, 229, 123, 22, 67, 13, 139, 112, 231, 8, 239, 94, 237, 224, 154, 135, 112, 24, 28, 5, 143, 74, 178, 247, 171, 84, 142, 126, 97, 180, 181, 4, 92, 238, 168, 219, 136, 26, 210, 252, 35, 248, 85, 204, 83, 158, 241, 61, 25, 184, 123, 90, 35, 26, 144, 44, 224, 49, 24, 113, 224, 185, 90, 118, 244, 87, 180, 234, 54, 155, 113, 90, 228, 227, 105, 251, 173, 50, 82, 84, 189, 173, 78, 19, 16, 138, 73, 212, 165, 222, 14, 124, 29, 130, 39, 179, 55, 22, 48, 192, 215, 76, 35, 105, 8, 250, 13, 204, 193, 236, 41, 251, 236, 133, 20, 183, 63, 192, 212, 113, 167, 53, 176, 84, 51, 21, 15, 18, 12, 129, 14, 16, 180, 186, 102, 179, 55, 93, 47, 202, 16, 39, 222, 163, 77, 4, 31, 3, 88, 79, 18, 200, 119, 228, 124, 80, 173, 84, 109, 175, 236, 198, 147, 239, 0, 77, 254, 229, 49, 62, 204, 227, 108, 183, 206, 108, 114, 17, 128, 163, 182, 243, 16, 182, 81, 134, 203, 166, 186, 244, 77, 225, 128, 173, 10, 198, 21, 56, 36, 186, 79, 126, 173, 141, 165, 241, 222, 109, 99, 196, 252, 233, 221, 58, 111, 166, 45, 157, 233, 135, 25, 100, 186, 97, 0, 15, 109, 246, 67, 97, 172, 240, 208, 30, 69, 48, 41, 36, 110, 190, 3, 68, 236, 86, 60, 118, 18, 155, 255, 32, 95, 38, 224, 31, 110, 197, 167, 95, 240, 52, 134, 246, 84, 245, 161, 215, 167, 57, 213, 38, 27, 126, 188, 175, 170, 46, 60, 57, 216, 117, 68, 12, 5, 8, 143, 102, 153, 26, 192, 118, 44, 249, 53, 54, 217, 246, 133, 175, 210, 73, 121, 87, 45, 1, 174, 247, 196, 20, 62, 28, 157, 250, 139, 143, 50, 88, 201, 45, 69, 139, 247, 19, 80, 148, 132, 97, 151, 1, 90, 160, 13, 108, 152, 61, 167, 66, 40, 123, 41, 25, 93, 221, 67, 172, 117, 232, 187, 133, 13, 85, 110, 215, 11, 119, 212, 215, 201, 195, 87, 78, 129, 161, 196, 41, 222, 156, 49, 110, 209, 250, 219, 134, 245, 30, 201, 115, 104, 0, 203, 110, 212, 246, 220, 203, 90, 74, 12, 25, 147, 31, 146, 11, 182, 251, 108, 201, 68, 8, 17, 136, 121, 105, 32, 199, 225, 43, 166, 221, 250, 20, 157, 14, 211, 205, 209, 154, 4, 90, 66, 230, 126, 71, 232, 18, 121, 233, 173, 44, 142, 234, 29, 238, 147, 76, 166, 59, 76, 210, 212, 125, 217, 13, 1, 154, 194, 154, 207, 155, 116, 210, 14, 211, 150, 124, 50, 16, 184, 177, 76, 195, 153, 156, 13, 134, 164, 103, 154, 241, 240, 113, 167, 214, 100, 187, 223, 52, 100, 241, 230, 94, 40, 11, 247, 221, 119, 178, 112, 33, 72, 192, 141, 8, 252, 29, 57, 220, 184, 81, 177, 50, 12, 111, 203, 124, 108, 15, 19, 232, 193, 96, 175, 224, 208, 61, 179, 219, 143, 171, 80, 153, 81, 23, 203, 180, 185, 50, 173, 62, 0, 18, 212, 11, 74, 196, 122, 51, 146, 3, 229, 30, 128, 73, 163, 117, 230, 73, 24, 27, 160, 17, 214, 62, 159, 112, 140, 35, 97, 197, 117, 128, 13, 245, 239, 130, 165, 240, 57, 132, 162, 126, 107, 119, 146, 15, 152, 125, 60, 193, 34, 254, 97, 93, 128, 223, 17, 248, 39, 151, 211, 125, 6, 31, 100, 2, 70, 44, 172, 198, 2, 253, 109, 75, 104, 94, 75, 93, 135, 146, 94, 125, 239, 109, 136, 136, 160, 158, 69, 233, 14, 170, 242, 9, 101, 112, 214, 135, 24, 2, 96, 144, 196, 110, 47, 83, 231, 42, 86, 69, 136, 254, 89, 131, 67, 237, 188, 150, 102, 25, 133, 225, 56, 148, 34, 113, 76, 77, 143, 57, 111, 10, 85, 211, 15, 162, 146, 14, 216, 52, 130, 80, 97, 29, 211, 102, 159, 8, 140, 185, 126, 31, 179, 189, 202, 170, 236, 101, 183, 123, 241, 16, 64, 46, 52, 129, 233, 164, 69, 36, 206, 134, 129, 234, 193, 143, 36, 183, 200, 255, 114, 0, 81, 225, 188, 58, 226, 234, 59, 213, 112, 108, 14, 70, 222, 102, 79, 32, 0, 174, 253, 252, 167, 88, 233, 136, 205, 31, 165, 43, 181, 42, 140, 36, 28, 193, 110, 154, 203, 37, 93, 196, 117, 226, 181, 73, 187, 88, 75, 49, 161, 181, 158, 18, 250, 95, 21, 144, 230, 99, 212, 228, 117, 132, 138, 121, 16, 209, 64, 41, 125, 192, 145, 98, 98, 213, 220, 108, 19, 230, 34, 189, 37, 56, 16, 200, 104, 183, 216, 85, 217, 90, 153, 114, 20, 199, 253, 34, 2, 2, 100, 22, 104, 126, 68, 77, 120, 195, 227, 172, 193, 205, 44, 251, 156, 152, 183, 92, 102, 141, 125, 147, 151, 87, 50, 235, 243, 66, 120, 40, 70, 44, 150, 18, 17, 58, 95, 77, 52, 136, 88, 227, 126, 165, 178, 53, 71, 76, 107, 177, 119, 117, 76, 220, 143, 71, 70, 34, 6, 127, 202, 134, 118, 175, 227, 228, 172, 223, 33, 47, 7, 66, 79, 101, 154, 51, 251, 246, 58, 2, 176, 126, 183, 95, 130, 222, 81, 30, 174, 237, 134, 250, 4, 27, 195, 158, 127, 167, 44, 54, 82, 21, 13, 124, 37, 113, 44, 187, 236, 170, 125, 76, 205, 3, 169, 58, 41, 129, 172, 195, 236, 208, 34, 130, 223, 202, 34, 222, 88, 42, 243, 69, 207, 8, 157, 206, 105, 167, 142, 220, 144, 212, 4, 102, 97, 155, 207, 172, 110, 110, 201, 32, 65, 189, 148, 99, 81, 179, 83, 179, 5, 147, 48, 3, 106, 245, 111, 188, 14, 176, 47, 3, 249, 31, 82, 97, 251, 89, 114, 45, 204, 159, 177, 255, 58, 57, 107, 231, 121, 58, 62, 195, 50, 177, 132, 62, 95, 243, 153, 163, 76, 162, 178, 154, 121, 208, 179, 236, 204, 236, 89, 219, 106, 217, 64, 241, 18, 114, 149, 194, 56, 151, 156, 19, 85, 34, 241, 192, 214, 37, 30, 90, 53, 24, 179, 148, 107, 21, 138, 204, 63, 119, 77, 177, 33, 53, 19, 193, 196, 72, 62, 105, 190, 216, 210, 112, 40, 126, 98, 4, 169, 250, 188, 247, 160, 92, 77, 119, 168, 62, 178, 229, 208, 170, 139, 213, 151, 74, 181, 41, 0, 85, 133, 120, 114, 41, 218, 33, 75, 168, 226, 236, 54, 111, 126, 44, 245, 200, 135, 37, 55, 1, 231, 73, 77, 155, 199, 30, 0, 201, 141, 166, 255, 46, 246, 94, 107, 171, 133, 117, 175, 97, 196, 115, 185, 20, 67, 190, 83, 20, 71, 49, 202, 159, 88, 57, 174, 124, 52, 17, 243, 28, 76, 178, 184, 5, 108, 196, 196, 12, 20, 141, 250, 67, 7, 185, 172, 109, 38, 223, 16, 143, 236, 42, 132, 249, 129, 82, 208, 228, 235, 229, 2, 231, 229, 123, 2, 201, 72, 36, 37, 209, 4, 200, 42, 207, 74, 246, 87, 113, 13, 224, 120, 127, 162, 26, 65, 197, 150, 17, 34, 65, 74, 232, 187, 139, 152, 156, 202, 237, 12, 240, 150, 127, 31, 164, 110, 158, 154, 255, 177, 75, 187, 5, 59, 86, 68, 156, 44, 251, 189, 2, 154, 78, 14, 105, 66, 3, 9, 193, 62, 71, 112, 148, 93, 29, 197, 146, 210, 85, 60, 159, 17, 83, 24, 179, 133, 89, 246, 46, 51, 246, 10, 38, 217, 227, 118, 208, 157, 158, 126, 157, 70, 40, 250, 120, 46, 146, 218, 17, 36, 250, 225, 211, 33, 97, 111, 51, 31, 211, 26, 70, 131, 49, 25, 229, 2, 165, 5, 47, 14, 115, 159, 26, 148, 144, 236, 177, 204, 61, 155, 160, 118, 217, 17, 34, 31, 126, 143, 223, 212, 203, 165, 47, 203, 46, 8, 43, 191, 249, 60, 146, 97, 10, 84, 83, 103, 237, 222, 108, 193, 86, 105, 219, 240, 129, 163, 223, 102, 213, 86, 241, 22, 72, 171, 63, 179, 4, 113, 226, 11, 49, 107, 65, 110, 228, 75, 160, 145, 138, 65, 210, 144, 206, 29, 191, 59, 226, 122, 137, 9, 169, 84, 247, 136, 142, 11, 187, 216, 89, 69, 65, 44, 73, 238, 126, 197, 123, 225, 81, 70, 35, 138, 30, 67, 181, 249, 205, 7, 91, 76, 17, 148, 17, 57, 166, 192, 236, 160, 97, 117, 241, 162, 125, 113, 21, 31, 181, 251, 200, 183, 137, 136, 106, 158, 179, 13, 187, 21, 170, 53, 204, 198, 72, 187, 36, 195, 39, 234, 57, 130, 127, 128, 20, 28, 227, 103, 236, 16, 40, 218, 33, 204, 37, 36, 178, 28, 22, 110, 128, 8, 211, 203, 198, 200, 232, 92, 205, 79, 104, 100, 10, 35, 24, 171, 5, 27, 207, 229, 202, 42, 35, 11, 8, 36, 140, 73, 76, 167, 131, 232, 47, 169, 104, 233, 214, 249, 239, 97, 144, 154, 92, 197, 9, 103, 227, 213, 220, 3, 4, 251, 62, 56, 134, 62, 58, 84, 134, 111, 120, 26, 83, 45, 228, 193, 144, 209, 75, 218, 253, 15, 97, 73, 24, 237, 1, 27, 189, 86, 146, 29, 12, 81, 251, 26, 51, 115, 6, 243, 38, 5, 30, 131, 24, 182, 161, 211, 251, 161, 196, 41, 243, 75, 24, 227, 194, 54, 152, 56, 22, 50, 197, 128, 252, 13, 175, 161, 246, 143, 224, 187, 43, 167, 150, 195, 209, 139, 154, 171, 204, 47, 16, 106, 131, 36, 47, 249, 156, 82, 65, 9, 8, 87, 29, 21, 36, 127, 39, 164, 16, 55, 244, 171, 188, 69, 224, 227, 38, 164, 17, 170, 47, 158, 22, 159, 71, 90, 222, 9, 19, 168, 159, 91, 117, 25, 125, 13, 73, 159, 243, 87, 152, 132, 125, 120, 84, 119, 95, 234, 141, 15, 191, 158, 96, 76, 117, 198, 57, 92, 101, 249, 162, 38, 41, 203, 107, 172, 56, 159, 207, 171, 86, 207, 162, 135, 48, 74, 162, 223, 189, 17, 106, 206, 60, 55, 137, 39, 60, 199, 155, 225, 123, 74, 187, 16, 167, 59, 111, 205, 51, 29, 221, 90, 85, 37, 32, 146, 59, 10, 74, 4, 53, 119, 236, 169, 108, 96, 79, 228, 61, 253, 199, 174, 187, 185, 172, 147, 33, 251, 131, 252, 36, 210, 147, 230, 23, 99, 175, 7, 155, 73, 209, 110, 82, 198, 69, 237, 10, 138, 152, 134, 204, 137, 89, 199, 55, 96, 185, 170, 92, 6, 110, 30, 190, 113, 222, 195, 12, 11, 191, 191, 81, 20, 106, 190, 255, 40, 160, 125, 239, 85, 156, 171, 86, 110, 179, 200, 178, 239, 198, 239, 182, 10, 29, 5, 66, 36, 148, 71, 99, 90, 20, 163, 87, 236, 209, 16, 232, 140, 110, 88, 223, 51, 27, 147, 205, 235, 91, 113, 151, 32, 173, 3, 29, 208, 140, 37, 184, 124, 14, 135, 87, 58, 134, 89, 68, 153, 207, 121, 249, 64, 61, 84, 2, 41, 13, 12, 166, 104, 234, 163, 128, 113, 215, 136, 104, 223, 8, 86, 153, 74, 15, 78, 149, 216, 246, 195, 34, 122, 51, 237, 7, 254, 78, 247, 107, 88, 70, 252, 96, 141, 151, 4, 80, 0, 129, 24, 78, 143, 203, 195, 52, 161, 26, 43, 21, 141, 106, 17, 138, 118, 224, 252, 143, 199, 98, 66, 251, 102, 167, 158, 71, 205, 171, 153, 197, 32, 83, 55, 91, 146, 181, 242, 236, 229, 60, 56, 216, 163, 20, 6, 201, 235, 132, 12, 254, 229, 62, 235, 245, 222, 113, 166, 193, 56, 123, 47, 152, 45, 197, 105, 4, 90, 154, 234, 129, 96, 71, 100, 106, 214, 163, 110, 251, 98, 244, 71, 188, 10, 115, 80, 64, 85, 102, 235, 154, 106, 166, 116, 13, 161, 161, 204, 29, 236, 88, 100, 174, 124, 215, 66, 214, 48, 26, 43, 240, 167, 51, 42, 86, 25, 215, 151, 169, 15, 200, 26, 213, 80, 70, 67, 233, 158, 35, 195, 179, 238, 129, 186, 112, 139, 255, 3, 93, 6, 240, 122, 207, 20, 41, 86, 56, 113, 195, 242, 190, 145, 16, 131, 77, 220, 164, 153, 202, 66, 255, 139, 2, 43, 164, 0, 60, 38, 140, 71, 96, 138, 16, 208, 63, 71, 124, 89, 199, 177, 16, 218, 35, 142, 103, 22, 234, 154, 116, 143, 176, 190, 70, 215, 62, 121, 244, 118, 90, 47, 137, 102, 86, 123, 19, 40, 211, 188, 221, 234, 8, 226, 167, 154, 117, 139, 65, 80, 12, 231, 65, 112, 43, 64, 250, 164, 207, 172, 199, 40, 246, 109, 250, 10, 237, 26, 92, 60, 54, 71, 254, 68, 254, 161, 103, 151, 140, 237, 63, 174, 107, 134, 74, 248, 229, 208, 90, 226, 120, 205, 132, 85, 27, 147, 159, 151, 122, 204, 185, 229, 144, 216, 223, 50, 20, 173, 109, 74, 176, 99, 42, 7, 102, 183, 174, 146, 199, 228, 45, 54, 94, 19, 122, 58, 82, 72, 223, 133, 155, 134, 136, 120, 250, 48, 162, 89, 66, 218, 198, 15, 246, 20, 163, 2, 9, 32, 241, 26, 184, 238, 69, 153, 3, 48, 34, 213, 160, 93, 155, 27, 78, 127, 68, 131, 190, 11, 36, 228, 1, 55, 249, 166, 97, 17, 132, 55, 63, 245, 105, 30, 148, 142, 58, 179, 89, 87, 92, 87, 77, 167, 10, 18, 243, 201, 211, 232, 85, 143, 99, 35, 179, 141, 251, 121, 251, 175, 173, 23, 126, 61, 80, 77, 59, 41, 10, 41, 64, 23, 159, 4, 230, 124, 143, 80, 217, 57, 53, 151, 38, 51, 97, 128, 132, 34, 157, 27, 173, 43, 251, 1, 178, 198, 94, 7, 0, 42, 101, 216, 71, 83, 145, 135, 208, 156, 192, 28, 217, 21, 214, 153, 241, 167, 26, 17, 171, 46, 112, 165, 75, 37, 222, 107, 212, 183, 191, 103, 88, 63, 2, 89, 27, 53, 147, 47, 63, 131, 220, 30, 156, 111, 128, 14, 21, 99, 57, 24, 107, 125, 142, 37, 181, 163, 89, 20, 145, 229, 134, 106, 8, 231, 222, 239, 115, 85, 107, 214, 150, 252, 155, 243, 66, 61, 142, 73, 227, 92, 143, 126, 122, 20, 15, 77, 2, 102, 142, 61, 235, 101, 91, 14, 40, 253, 14, 117, 23, 156, 27, 5, 38, 109, 211, 77, 100, 150, 76, 168, 201, 114, 145, 60, 67, 130, 65, 16, 88, 65, 134, 220, 225, 12, 174, 143, 66, 157, 204, 29, 80, 8, 156, 126, 12, 199, 245, 37, 116, 34, 192, 193, 178, 44, 135, 11, 99, 49, 86, 47, 1, 158, 209, 132, 146, 76, 88, 181, 168, 202, 240, 206, 66, 48, 33, 52, 41, 45, 116, 146, 62, 73, 239, 107, 255, 65, 182, 205, 164, 132, 239, 89, 31, 173, 8, 170, 229, 55, 247, 40, 73, 5, 114, 209, 34, 203, 232, 108, 37, 158, 200, 133, 124, 146, 80, 78, 95, 25, 130, 243, 32, 98, 163, 21, 230, 75, 105, 5, 255, 147, 51, 252, 147, 179, 25, 186, 227, 219, 221, 193, 96, 221, 254, 41, 181, 17, 247, 72, 195, 105, 175, 153, 41, 164, 183, 116, 92, 255, 57, 161, 146, 78, 38, 224, 96, 9, 198, 11, 138, 239, 228, 220, 42, 215, 186, 218, 228, 123, 251, 83, 2, 241, 123, 66, 56, 179, 33, 58, 96, 90, 79, 181, 62, 241, 154, 132, 75, 139, 25, 59, 138, 64, 94, 117, 200, 118, 40, 92, 40, 42, 81, 97, 75, 231, 192, 183, 252, 252, 140, 108, 108, 169, 28, 196, 5, 25, 110, 185, 79, 162, 94, 94, 94, 42, 55, 235, 82, 121, 165, 146, 37, 196, 148, 212, 9, 155, 198, 74, 36, 53, 139, 80, 55, 192, 244, 151, 59, 187, 56, 77, 202, 150, 123, 207, 102, 209, 48, 105, 184, 86, 63, 225, 116, 116, 205, 186, 87, 177, 135, 147, 249, 213, 133, 96, 109, 47, 72, 71, 38, 22, 173, 53, 58, 213, 107, 72, 41, 37, 209, 72, 37, 36, 209, 136, 246, 212, 153, 162, 195, 197, 63, 95, 118, 152, 78, 180, 17, 16, 219, 163, 127, 188, 138, 155, 113, 144, 137, 198, 248, 5, 100, 201, 48, 131, 222, 110, 188, 196, 132, 65, 180, 152, 57, 211, 45, 65, 135, 173, 234, 202, 180, 47, 222, 202, 108, 227, 204, 127, 201, 117, 71, 42, 136, 35, 91, 213, 212, 24, 71, 164, 11, 34, 239, 17, 45, 40, 49, 89, 195, 226, 5, 26, 29, 84, 156, 216, 102, 183, 120, 245, 237, 223, 47, 113, 209, 164, 161, 209, 237, 32, 5, 58, 171, 74, 80, 73, 90, 89, 11, 144, 106, 121, 107, 229, 46, 9, 144, 110, 239, 55, 202, 249, 179, 138, 143, 201, 3, 245, 204, 97, 58, 131, 191, 6, 207, 251, 108, 44, 182, 110, 250, 51, 71, 113, 159, 63, 83, 195, 179, 204, 100, 82, 44, 42, 113, 68, 0, 41, 201, 74, 33, 57, 12, 59, 73, 9, 133, 163, 236, 61, 131, 11, 77, 60, 147, 10, 164, 122, 171, 38, 20, 66, 207, 123, 185, 64, 139, 44, 161, 206, 193, 72, 229, 247, 14, 68, 249, 33, 181, 64, 124, 128, 150, 209, 235, 68, 123, 166, 238, 49, 180, 75, 128, 45, 209, 49, 141, 56, 10, 146, 73, 250, 220, 164, 86, 147, 34, 48, 236, 187, 89, 53, 169, 184, 17, 79, 155, 17, 4, 114, 132, 21, 201, 247, 186, 34, 170, 87, 89, 129, 153, 234, 51, 63, 232, 81, 84, 186, 175, 39, 61, 199, 108, 33, 157, 176, 206, 126, 213, 20, 63, 8, 172, 228, 35, 23, 169, 17, 15, 217, 7, 177, 175, 38, 11, 60, 234, 46, 244, 91, 186, 96, 52, 216, 163, 177, 132, 102, 6, 102, 52, 59, 137, 50, 112, 255, 19, 93, 209, 48, 255, 160, 48, 157, 156, 53, 29, 211, 170, 161, 232, 96, 156, 233, 206, 202, 235, 83, 238, 251, 230, 226, 169, 171, 5, 206, 108, 163, 156, 83, 211, 59, 149, 142, 98, 88, 162, 242, 92, 122, 29, 139, 223, 26, 20, 20, 229, 63, 236, 74, 33, 218, 142, 245, 249, 79, 234, 20, 91, 224, 100, 196, 22, 25, 98, 47, 180, 109, 232, 156, 165, 13, 5, 183, 79, 123, 208, 157, 195, 184, 113, 208, 123, 1, 98, 207, 69, 95, 203, 115, 225, 28, 41, 73, 185, 196, 164, 95, 12, 37, 35, 86, 84, 17, 1, 121, 10, 125, 78, 135, 127, 210, 36, 182, 148, 104, 16, 65, 13, 26, 15, 137, 128, 163, 37, 168, 49, 32, 232, 146, 173, 200, 80, 58, 252, 168, 24, 115, 141, 150, 38, 185, 120, 53, 241, 137, 112, 104, 116, 125, 115, 65, 157, 154, 25, 6, 155, 99, 248, 7, 80, 131, 8, 186, 132, 27, 154, 202, 32, 78, 150, 8, 137, 228, 213, 44, 149, 5, 234, 100, 41, 139, 172, 101, 60, 158, 227, 233, 146, 166, 8, 35, 34, 173, 184, 145, 80, 205, 181, 164, 86, 190, 128, 198, 130, 18, 210, 64, 29, 37, 172, 148, 28, 112, 48, 177, 119, 3, 205, 241, 13, 80, 56, 229, 53, 205, 20, 249, 128, 46, 241, 46, 197, 23, 253, 211, 39, 233, 61, 3, 10, 219, 173, 188, 121, 177, 132, 200, 141, 110, 125, 253, 171, 192, 167, 17, 42, 98, 106, 29, 167, 134, 108, 223, 0, 129, 58, 8, 16, 2, 98, 149, 243, 47, 152, 96, 162, 194, 175, 118, 168, 40, 11, 157, 150, 221, 165, 75, 201, 56, 167, 44, 110, 230, 251, 42, 140, 126, 29, 179, 239, 67, 93, 165, 216, 226, 205, 14, 40, 199, 144, 90, 61, 154, 5, 115, 24, 93, 165, 206, 26, 220, 58, 204, 166, 181, 137, 44, 64, 142, 18, 187, 38, 236, 197, 113, 252, 67, 165, 28, 188, 147, 76, 226, 227, 156, 131, 194, 87, 9, 5, 82, 241, 119, 93, 237, 223, 83, 123, 251, 232, 209, 62, 60, 217, 5, 121, 200, 235, 90, 70, 190, 202, 95, 65, 228, 176, 131, 89, 25, 51, 213, 43, 247, 224, 126, 6, 127, 141, 65, 211, 67, 173, 36, 38, 191, 87, 78, 195, 26, 229, 155, 197, 224, 220, 73, 217, 25, 48, 30, 129, 160, 153, 204, 56, 195, 227, 15, 224, 64, 185, 209, 195, 249, 96, 238, 183, 141, 159, 228, 207, 108, 109, 127, 123, 62, 242, 53, 205, 77, 87, 161, 57, 158, 165, 105, 35, 254, 244, 122, 234, 145, 70, 146, 48, 135, 22, 213, 128, 32, 225, 138, 181, 7, 237, 247, 232, 226, 55, 42, 149, 255, 186, 221, 38, 156, 32, 28, 135, 193, 247, 126, 164, 127, 244, 248, 153, 38, 167, 28, 126, 151, 61, 192, 10, 29, 105, 153, 247, 147, 232, 104, 184, 158, 94, 45, 210, 254, 228, 168, 205, 224, 50, 116, 192, 81, 40, 115, 246, 168, 148, 116, 210, 220, 216, 185, 41, 163, 107, 207, 65, 39, 21, 18, 58, 130, 31, 76, 89, 202, 152, 112, 252, 173, 140, 77, 45, 109, 134, 22, 115, 208, 74, 13, 232, 166, 182, 107, 45, 0, 93, 37, 18, 40, 125, 250, 87, 76, 23, 199, 88, 55, 69, 105, 94, 13, 143, 140, 195, 39, 152, 216, 42, 197, 205, 185, 182, 3, 109, 143, 206, 15, 98, 227, 154, 131, 204, 77, 33, 173, 139, 56, 58, 36, 139, 155, 80, 6, 237, 16, 167, 118, 179, 21, 32, 145, 210, 45, 135, 26, 190, 94, 49, 86, 252, 157, 216, 217, 187, 164, 171, 229, 209, 53, 242, 123, 213, 94, 104, 17, 199, 213, 176, 109, 131, 87, 79, 17, 196, 216, 26, 167, 226, 6, 125, 143, 173, 58, 143, 8, 0, 111, 110, 142, 137, 206, 104, 88, 184, 162, 178, 219, 189, 135, 112, 218, 170, 228, 190, 2, 49, 60, 189, 54, 195, 172, 161, 41, 113, 120, 171, 166, 209, 91, 45, 67, 188, 135, 38, 15, 155, 34, 33, 65, 238, 108, 18, 254, 247, 165, 162, 161, 233, 29, 147, 209, 130, 194, 103, 78, 229, 160, 48, 17, 71, 148, 176, 13, 140, 183, 196, 182, 76, 62, 210, 95, 169, 176, 15, 111, 252, 25, 130, 222, 230, 179, 113, 112, 254, 206, 10, 222, 19, 26, 254, 102, 82, 248, 208, 131, 78, 86, 63, 51, 73, 199, 30, 148, 150, 55, 63, 74, 57, 206, 133, 213, 50, 189, 158, 11, 130, 160, 45, 169, 110, 250, 58, 48, 243, 161, 100, 125, 59, 52, 8, 213, 56, 78, 162, 231, 115, 224, 105, 179, 35, 216, 255, 25, 46, 216, 250, 83, 155, 239, 30, 86, 26, 93, 2, 154, 4, 252, 188, 252, 57, 81, 187, 129, 126, 150, 144, 161, 61, 46, 133, 32, 216, 107, 228, 223, 111, 246, 172, 86, 49, 0, 186, 161, 202, 94, 218, 209, 37, 207, 149, 203, 252, 227, 241, 26, 245, 81, 162, 246, 72, 94, 39, 19, 255, 110, 140, 168, 88, 156, 75, 182, 21, 166, 224, 77, 6, 213, 10, 60, 249, 118, 41, 143, 36, 23, 94, 55, 202, 210, 68, 117, 56, 215, 82, 234, 43, 123, 186, 76, 135, 101, 37, 115, 119, 254, 246, 199, 154, 63, 31, 189, 216, 83, 214, 93, 207, 107, 174, 243, 51, 204, 25, 99, 117, 31, 147, 4, 65, 159, 169, 160, 218, 224, 240, 93, 129, 72, 179, 42, 129, 84, 245, 47, 121, 212, 187, 48, 252, 234, 140, 135, 224, 253, 39, 100, 158, 71, 105, 201, 134, 1, 107, 52, 124, 181, 41, 150, 170, 139, 68, 213, 49, 226, 95, 118, 7, 100, 32, 252, 148, 11, 219, 124, 156, 168, 164, 113, 253, 114, 150, 51, 0, 20, 190, 170, 157, 59, 252, 137, 248, 59, 251, 9, 154, 200, 182, 164, 88, 118, 157, 117, 48, 180, 216, 16, 240, 230, 16, 177, 67, 50, 197, 240, 129, 227, 100, 246, 144, 16, 130, 37, 128, 59, 236, 131, 98, 19, 138, 72, 90, 63, 172, 135, 252, 124, 38, 244, 198, 194, 132, 184, 184, 22, 54, 64, 93, 72, 216, 104, 163, 235, 64, 116, 5, 69, 180, 97, 45, 39, 144, 241, 40, 28, 216, 201, 156, 134, 211, 129, 153, 67, 232, 76, 127, 141, 18, 55, 171, 32, 147, 160, 23, 65, 187, 6, 31, 139, 159, 191, 118, 127, 67, 7, 45, 74, 91, 55, 38, 54, 36, 62, 255, 199, 86, 219, 43, 116, 146, 29, 82, 182, 142, 97, 56, 191, 135, 206, 142, 78, 249, 47, 96, 170, 174, 102, 157, 182, 125, 38, 199, 138, 232, 185, 93, 255, 227, 88, 225, 90, 200, 136, 83, 151, 81, 1, 34, 115, 89, 36, 192, 76, 177, 167, 25, 176, 111, 233, 236, 120, 33, 190, 161, 153, 94, 153, 98, 65, 153, 186, 164, 121, 47, 38, 118, 219, 131, 186, 42, 245, 40, 202, 237, 154, 66, 245, 4, 197, 23, 9, 107, 84, 141, 66, 117, 95, 40, 71, 87, 153, 93, 168, 179, 85, 11, 27, 99, 183, 84, 72, 208, 248, 8, 106, 146, 157, 28, 151, 50, 208, 93, 88, 54, 232, 204, 170, 88, 12, 5, 157, 216, 216, 91, 23, 232, 53, 212, 68, 141, 218, 169, 181, 241, 127, 115, 195, 16, 202, 249, 210, 128, 35, 50, 26, 213, 0, 175, 172, 170, 242, 38, 132, 242, 209, 199, 134, 20, 12, 224, 37, 187, 30, 138, 220, 132, 175, 62, 66, 77, 118, 161, 123, 29, 108, 58, 114, 40, 184, 50, 31, 215, 129, 181, 216, 16, 78, 118, 103, 239, 155, 10, 51, 176, 185, 44, 175, 89, 134, 88, 85, 132, 190, 149, 63, 240, 163, 147, 119, 249, 84, 102, 189, 70, 112, 200, 165, 252, 223, 32, 103, 185, 91, 60, 141, 214, 161, 57, 63, 194, 221, 239, 221, 11, 165, 197, 49, 100, 246, 31, 251, 108, 244, 176, 212, 112, 162, 11, 180, 166, 201, 33, 250, 24, 255, 55, 106, 225, 144, 62, 188, 29, 21, 32, 3, 164, 208, 210, 199, 212, 192, 4, 187, 223, 204, 177, 99, 34, 49, 178, 19, 205, 46, 117, 79, 63, 163, 253, 143, 93, 48, 141, 174, 159, 8, 78, 60, 7, 17, 35, 255, 43, 91, 40, 16, 216, 226, 169, 226, 168, 13, 94, 168, 98, 64, 101, 156, 89, 22, 104, 44, 188, 138, 116, 84, 75, 18, 143, 124, 179, 148, 37, 189, 232, 237, 212, 52, 219, 4, 2, 101, 169, 158, 187, 6, 52, 254, 49, 152, 74, 168, 72, 179, 211, 114, 107, 209, 135, 190, 48, 194, 107, 206, 2, 31, 59, 123, 94, 213, 117, 79, 125, 105, 132, 40, 31, 90, 42, 110, 27, 48, 198, 62, 173, 116, 191, 214, 125, 52, 201, 131, 54, 62, 46, 86, 103, 157, 4, 18, 61, 184, 35, 238, 223, 150, 144, 212, 169, 1, 89, 132, 117, 143, 74, 40, 16, 193, 213, 31, 22, 120, 212, 55, 35, 44, 123, 174, 219, 82, 79, 212, 232, 58, 83, 116, 121, 76, 204, 197, 140, 155, 238, 251, 73, 158, 141, 158, 128, 119, 126, 142, 187, 220, 76, 127, 248, 244, 159, 179, 20, 123, 4, 27, 129, 226, 228, 214, 22, 66, 179, 44, 86, 119, 116, 213, 189, 90, 243, 16, 33, 188, 210, 163, 241, 7, 25, 139, 226, 246, 196, 169, 202, 236, 47, 95, 139, 160, 67, 137, 7, 131, 8, 50, 124, 64, 94, 107, 35, 61, 161, 47, 211, 196, 151, 89, 124, 208, 129, 43, 249, 61, 73, 71, 64, 201, 204, 177, 88, 85, 182, 122, 131, 0, 158, 56, 176, 218, 90, 233, 1, 41, 219, 114, 208, 164, 159, 127, 173, 218, 108, 115, 207, 253, 206, 246, 235, 48, 158, 227, 156, 182, 108, 255, 227, 191, 20, 248, 195, 222, 109, 250, 222, 150, 39, 72, 249, 64, 48, 15, 244, 32, 88, 193, 118, 36, 88, 230, 160, 230, 179, 225, 0, 134, 153, 235, 12, 19, 208, 136, 165, 192, 63, 26, 166, 7, 116, 104, 217, 77, 234, 14, 249, 130, 168, 55, 37, 118, 235, 217, 237, 136, 114, 95, 143, 32, 131, 145, 168, 20, 103, 112, 203, 50, 204, 168, 11, 160, 135, 237, 230, 129, 242, 3, 220, 156, 55, 8, 44, 8, 89, 216, 162, 209, 144, 123, 96, 184, 89, 19, 74, 113, 169, 23, 140, 92, 208, 131, 85, 190, 111, 234, 113, 67, 137, 194, 65, 226, 165, 89, 47, 118, 160, 253, 216, 100, 109, 249, 131, 80, 46, 245, 151, 249, 34, 243, 140, 19, 75, 178, 119, 189, 83, 232, 204, 17, 22, 173, 58, 250, 120, 47, 231, 235, 50, 48, 150, 3, 142, 122, 62, 6, 180, 227, 25, 230, 19, 128, 150, 70]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eRvwZ5Nf7g6"
      },
      "source": [
        "k = 8       # Number of information bits per message, i.e., M=2**k\n",
        "n = 4       # Number of real channel uses per message\n",
        "seed = 2    # Seed RNG reproduce identical results"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28KCbvYif7hD"
      },
      "source": [
        "#### The Autoencoder Class\n",
        "In order to quickly experiment with different architecture and parameter choices, it is useful to create a Python class that has functions for training and inference. Each autoencoder instance has its own Tensorflow session and graph. Thus, you can have multiple instances running at the same time without interference between them."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######### CREATING A PATH LOSS MODEL OUTSIDE CREATE GRAPH(MAIN LOOP)\n",
        "#IEEE 802.11p is an approved amendment to the IEEE 802.11 standard to add wireless access in vehicular environments (WAVE), a vehicular communication system. \n",
        "#IEEE 802.11p standard typically uses channels of 10 MHz bandwidth in the 5.9 GHz band (5.8505.925 GHz).\n",
        "s = np.floor(np.random.uniform(0,M, batch_size))\n",
        "s = s.astype('int64')\n",
        "f = 5.9*10**9;#in Hz corresponding to IEEE 802.11p\n",
        "lamda = 0.05;  #in metres\n",
        "Pt = 1; #BS transmitted power in watts\n",
        "Lo = 8;   #Total system losses in dB\n",
        "Nf = 5;    #Mobile receiver noise figure in dB\n",
        "T = 290;   #temperature in degree kelvin\n",
        "BW = 10*10**6; #in Hz\n",
        "Gb = 8;  #in dB\n",
        "Gm = 0;   #in dB\n",
        "Hb = 1;  #in metres\n",
        "Hm = 1.1;   #in metres\n",
        "B = 1.38*10**-23; #Boltzmann's constant\n",
        "Te = T*(3.162-1)\n",
        "Pn = B*(Te+T)*BW\n",
        "Free_Lp = {}\n",
        "Pr = {}\n",
        "\n",
        "SNR_var = {}\n",
        "#Calculations&Results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "\n",
        "  Free_Lp[i] = 20*math.log10(Hm*Hb/s[i]**2)\n",
        "  Pr[i] = Free_Lp[i]-Lo+Gm+Gb+Pt  #in dBW\n",
        "  SNR_var[i] = Pr[i]-10*math.log10(Pn) #Provides SNR (stddev) values for all distances spanning s\n",
        "\n",
        "ad_noise_std = []  #The values of the dictionary, finally providing an array of stdevs\n",
        "for i in SNR_var.values():\n",
        "  ad_noise_std.append(i)\n",
        "\n",
        "\n",
        "ad_noise_std = np.tile(ad_noise_std, (2,2,1))\n",
        "ad_noise_std = np.transpose(ad_noise_std)\n",
        "print(np.shape(ad_noise_std)) \n",
        "print((ad_noise_std)) \n",
        "#final_ad_noise_std = {}\n",
        "\n",
        "#for i in range(1000):\n",
        "#  final_ad_noise_std[i] = np.random.normal(0.0, ad_noise_std[i], (2, 2))\n",
        "              #final_ad_noise_std[i] = tf.random_normal([batch_size,2,2], mean=0.0, stddev=ad_noise_std[i])\n",
        "\n",
        "#final_ad_noise_std = final_ad_noise_std.values()\n",
        "            #noise = []\n",
        "            #for i in final_ad_noise_std.values():\n",
        "              #noise.append(i)\n",
        "\n",
        "            #noise = np.asarray(noise, np.float32)\n",
        "            #noise = tf.convert_to_tensor(noise, np.float32)  "
      ],
      "metadata": {
        "id": "mpnHSzmqYXny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2591501a-07da-4502-c6e7-fb72a9890120"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 2, 2)\n",
            "[[[53.42552615 53.42552615]\n",
            "  [53.42552615 53.42552615]]\n",
            "\n",
            " [[48.68232646 48.68232646]\n",
            "  [48.68232646 48.68232646]]\n",
            "\n",
            " [[73.55091364 73.55091364]\n",
            "  [73.55091364 73.55091364]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[37.42731416 37.42731416]\n",
            "  [37.42731416 37.42731416]]\n",
            "\n",
            " [[35.5247825  35.5247825 ]\n",
            "  [35.5247825  35.5247825 ]]\n",
            "\n",
            " [[51.51461488 51.51461488]\n",
            "  [51.51461488 51.51461488]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: RuntimeWarning: divide by zero encountered in true_divide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs9Rtd01f7hG"
      },
      "source": [
        "\n",
        "class AE(object):\n",
        "    def __init__(self, k, n, seed=None, filename=None):\n",
        "        assert (n%2 == 0), \"Channel only allows complex symbols -> n must be a multiple of 2\"\n",
        "        self.k = k \n",
        "        self.n = n\n",
        "        self.n_complex = int(self.n/2)\n",
        "        self.bits_per_symbol = self.k/self.n_complex\n",
        "        self.M = 2**self.k\n",
        "        self.seed = seed if (seed is not None) else int(time.time())           \n",
        "        self.graph = None\n",
        "        self.sess = None   \n",
        "        self.vars = None\n",
        "        self.saver = None   \n",
        "        self.constellations = None\n",
        "        self.blers = None\n",
        "        self.create_graph()\n",
        "        self.create_session()\n",
        "        if filename is not None:    \n",
        "            self.load(filename)       \n",
        "        return\n",
        "    \n",
        "    def generate_distances(self,int_batch_size,bins):\n",
        "        # int_batch_size: 1000\n",
        "        # bins = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "\n",
        "        t = np.linspace(0,1,int_batch_size)\n",
        "        triangle1 = signal.sawtooth(2 * np.pi * 5 * t, 0.5)\n",
        "        triangle1 = 10*triangle1\n",
        "        triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "        triangle2 = signal.sawtooth(5 * np.pi * 5 * t, 0.5)\n",
        "        triangle2 = 10*triangle2\n",
        "        triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "        triangle3 = triangle1 + triangle2\n",
        "        triangle3 = triangle3/15*(7)\n",
        "        #print(s)\n",
        "\n",
        "        bins = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "        tr = np.digitize(triangle3, bins, right=True)\n",
        "        replacements = {0:7, 1:6, 2:5, 3:4, 4:3, 5:2, 6:1, 7:0}\n",
        "        replacer = replacements.get\n",
        "        tr = ([replacer(n, n) for n in tr])\n",
        "     \n",
        "        return tr\n",
        "\n",
        "\n",
        "    \n",
        "    def create_graph(self):\n",
        "        '''This function creates the computation graph of the autoencoder'''\n",
        "        self.graph = tf.Graph()        \n",
        "        with self.graph.as_default():    \n",
        "            tf.set_random_seed(self.seed)\n",
        "            batch_size = tf.placeholder(tf.int32, shape=())\n",
        "            \n",
        "            \n",
        "            # Transmitter\n",
        "            #s = tf.random_uniform(shape=[batch_size], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            #s = tf.random_uniform(shape=[1000], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            #s = tf.cast(tf.floor(tf.linspace(0.0, M, batch_size, name=\"linspace\")), tf.int64)\n",
        "            #print(\"Initial shape of S:\",s)\n",
        "            #s = tf.random_uniform(shape=[1000], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            \n",
        "            #plt.plot(t, triangle3, 'o')\n",
        "            #plt.plot(t, triangle3)\n",
        "            #print(s)\n",
        "            #plt.plot(t, s)\n",
        "            #s = tf.convert_to_tensor(s, dtype=tf.int64)\n",
        "            x = self.encoder(s)     \n",
        "            print(\"Initial shape of x:\",x)\n",
        "            \n",
        "            # Channel\n",
        "            noise_std = tf.placeholder(tf.float32, shape=[1000,2,2])\n",
        "            #ad_noise_std = (tf.linspace(0.0, noise_std, tf.shape(x), name=\"linspace\"))\n",
        "            #ad_noise_std = tf.random_normal(tf.shape(x), mean=0.0, stddev=7)\n",
        "            #print(\"Initial shape of ad_noise:\",ad_noise_std)\n",
        "            #noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=ad_SNR)\n",
        "            #x = self.encoder(s)     \n",
        "            \n",
        "            # Channel\n",
        "            #noise_std = tf.placeholder(tf.float32, shape=())\n",
        "            noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=noise_std) \n",
        "\n",
        "            fade = tf.random.normal(shape=tf.shape(x))\n",
        "            sparr1,sparr2 = tf.split(fade,num_or_size_splits=2, axis=2)\n",
        "            complex_fade = tf.complex(sparr1, sparr2)\n",
        "            fade = tf.abs(complex_fade)\n",
        "            #fade = tf.math.sqrt(1/2)*fade\n",
        "            print(\"Initial shape of fade:\",fade)\n",
        "            #print(\"Initial shape of noise:\",noise)\n",
        "            #fade = 1\n",
        "            y = tf.multiply(x,fade) + noise\n",
        "            print(\"Initial shape of y:\",y)\n",
        "           \n",
        "            # Receiver\n",
        "            s_hat = self.decoder(y)\n",
        "            correct_s_hat = tf.argmax(tf.nn.softmax(s_hat), axis=1)\n",
        "\n",
        "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat, weights= (s+1)**(4))\n",
        "\n",
        "\n",
        "            correct_predictions = tf.equal(tf.argmax(tf.nn.softmax(s_hat), axis=1), s)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "            bler = 1-accuracy\n",
        "\n",
        "            # Optimizer\n",
        "            lr = tf.placeholder(tf.float32, shape=()) # We can feed in any desired learning rate for each step     \n",
        "            train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
        "            #train_op_0 = tf.train.AdamOptimizer(lr).minimize(cross_entropy_0)\n",
        "            #lr = tf.train.exponential_decay(1e-10, global_step=cross_entropy, decay_steps=100, decay_rate=1.30)\n",
        "\n",
        "        \n",
        "            # References to graph variables we need to access later \n",
        "            self.vars = {\n",
        "                'accuracy': accuracy,\n",
        "                'batch_size': batch_size,\n",
        "                'bler': bler,\n",
        "                'cross_entropy': cross_entropy,\n",
        "                'init': tf.global_variables_initializer(),\n",
        "                'lr': lr,\n",
        "                'noise_std': noise_std,\n",
        "                'train_op': train_op,\n",
        "                's': s,\n",
        "                's_hat': s_hat,\n",
        "                'correct_s_hat': correct_s_hat,\n",
        "                'x': x,\n",
        "            }            \n",
        "            self.saver = tf.train.Saver()\n",
        "        return\n",
        "    \n",
        "    def create_session(self):\n",
        "        '''Create a session for the autoencoder instance with the compuational graph'''\n",
        "        self.sess = tf.Session(graph=self.graph)        \n",
        "        self.sess.run(self.vars['init'])\n",
        "        return\n",
        "    \n",
        "    def encoder(self, input):\n",
        "        '''The transmitter'''\n",
        "        self.weight_var_rec = self.weight_variable((self.M,self.M)) # shape = (8,8)\n",
        "        self.embedding_lookup_rec = tf.nn.embedding_lookup(self.weight_var_rec, input)\n",
        "        print(self.embedding_lookup_rec)\n",
        "        x = tf.nn.elu(self.embedding_lookup_rec)\n",
        "        #x = tf.layers.dense(self.embedding_lookup_rec, self.M, activation=tf.nn.relu)\n",
        "        x = tf.layers.dense(x, self.M, activation=None)\n",
        "        x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        x = tf.reshape(x, shape=[-1,self.n_complex,2])\n",
        "        print(x);\n",
        "        #Average power normalization\n",
        "        x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x))) \n",
        "        return x\n",
        "    \n",
        "    def decoder(self, input):\n",
        "        '''The Receiver'''\n",
        "        #input = self.flip_decoder(input)\n",
        "        y = tf.reshape(input, shape=[-1,self.n])\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        print(y);\n",
        "        return y\n",
        "    \n",
        "    def EbNo2Sigma(self, ebnodb):\n",
        "        '''Convert Eb/No in dB to noise standard deviation'''\n",
        "        ebno = 10**(ebnodb/10)\n",
        "        return 1/np.sqrt(2*self.bits_per_symbol*ebno)\n",
        "    \n",
        "    def gen_feed_dict(self, batch_size, ebnodb, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['lr']: lr,\n",
        "        }\n",
        "\n",
        "    def gen_e2e_feed_dict(self, batch_size, ebnodb, s_input):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['s']: s_input,\n",
        "        }   \n",
        "    \n",
        "    def load(self, filename):\n",
        "        '''Load an pre_trained model'''\n",
        "        return self.saver.restore(self.sess, filename)\n",
        "        \n",
        "    def plot_constellation(self, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = self.transmit(range(self.M))\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(self.n_complex):\n",
        "            image = plt.figure(figsize=(6,6))\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-maxrange,maxrange)\n",
        "            plt.ylim(-maxrange,maxrange)\n",
        "            for i in range(self.M):       \n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\",marker='x')   \n",
        "            image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            image.suptitle('%d. complex symbol' % (k+1))\n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "        return x, image\n",
        "    \n",
        "    def save(self, filename):\n",
        "        '''Save the current model'''\n",
        "        return self.saver.save(self.sess, filename)  \n",
        "    \n",
        "    def test_step(self, batch_size, ebnodb):\n",
        "        '''Compute the BLER over a single batch and Eb/No'''\n",
        "        bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n",
        "        return bler\n",
        "    \n",
        "    def transmit(self, s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['x'], feed_dict={self.vars['s']: s})\n",
        "\n",
        "    def end2end(self, batch_size, ebnodb, input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['correct_s_hat'], feed_dict=self.gen_e2e_feed_dict(batch_size, ebnodb, input_s)) \n",
        "        #print(self.sess.run(self.vars['correct_s_hat'], feed_dict={self.vars['s']: input_s}))     \n",
        "\n",
        "    #print(self.sess.run(self.vars['s_hat'], feed_dict={self.vars['s']: s}))\n",
        "\n",
        "    def train(self, training_params, validation_params):  \n",
        "        #s_input = self.generate_distances(100,np.array([0, 1, 2, 3, 4, 5, 6]))\n",
        "        \n",
        "        '''Training and validation loop'''\n",
        "        for index, params in enumerate(training_params):            \n",
        "            batch_size, lr, ebnodb, iterations = params            \n",
        "            print('\\nBatch Size: ' + str(batch_size) +\n",
        "                  ', Learning Rate: ' + str(lr) +\n",
        "                  ', EbNodB: ' + str(ebnodb) +\n",
        "                  ', Iterations: ' + str(iterations))\n",
        "            \n",
        "            val_size, val_ebnodb, val_steps = validation_params[index]\n",
        "            \n",
        "            for i in range(iterations):\n",
        "                self.train_step(batch_size, ebnodb, lr)    \n",
        "                if (i%val_steps==0):\n",
        "                    #bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_new_feed_dict(val_size, val_ebnodb, lr, s_input))\n",
        "                    bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size, val_ebnodb, lr))\n",
        "                    print(bler)                           \n",
        "        return       \n",
        "    \n",
        "    def train_step(self, batch_size, ebnodb, lr):\n",
        "        '''A single training step'''\n",
        "        #self.sess.run(self.vars['train_op'], feed_dict=self.gen_new_feed_dict(batch_size, ebnodb, lr, s_input))\n",
        "        self.sess.run(self.vars['train_op'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr))\n",
        "        return \n",
        "    \n",
        "    def weight_variable(self, shape):\n",
        "        '''Xavier-initialized weights optimized for ReLU Activations'''\n",
        "        (fan_in, fan_out) = shape\n",
        "        low = np.sqrt(6.0/(fan_in + fan_out)) \n",
        "        high = -np.sqrt(6.0/(fan_in + fan_out))\n",
        "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n",
        "    \n",
        "    def bler_sim(self, ebnodbs, batch_size, iterations):\n",
        "        '''Monte Carlo simulations of BLER for a range of Eb/No\n",
        "           Sometimes we to compute statistics for batch sizes that do not fit into the GPUs memory.\n",
        "           You can average over multiple batches with small size instead.           \n",
        "        '''\n",
        "        BLER = np.zeros_like(ebnodbs)\n",
        "        for i in range(iterations):\n",
        "            bler = np.array([self.sess.run(self.vars['bler'],\n",
        "                            feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0)) for ebnodb in ebnodbs])\n",
        "            BLER = BLER + bler/iterations\n",
        "        return BLER\n",
        "    \n",
        "    def plot_bler(self, EbNodB, BLER):\n",
        "        '''Plot a BLER curve'''\n",
        "        image = plt.figure(figsize=(10,8))\n",
        "        plt.plot(EbNodB, BLER, '-r', linewidth=2.0)\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('EbNo (dB)', fontsize=18)\n",
        "        plt.ylabel('Block-error rate', fontsize=18)\n",
        "        plt.grid(True)\n",
        "        plt.ylim([1e-5,1])\n",
        "        return image"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bANUdLIsf7hM"
      },
      "source": [
        "## Training parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeHghWVRf7hO",
        "outputId": "8c660995-dfde-476d-bfeb-8df28f462ae1"
      },
      "source": [
        "train_EbNodB = ad_noise_std\n",
        "val_EbNodB = train_EbNodB\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "\n",
        "#epoch = [10000]\n",
        "\n",
        "#for i in epoch:\n",
        "training_params = [\n",
        "    #batch_size, lr, ebnodb, iterations\n",
        "    [1000 , lr, train_EbNodB, 10000]\n",
        "    ]\n",
        "\n",
        "validation_params = [\n",
        "    #batch_size, ebnodb, val_steps \n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000]\n",
        "    ]\n",
        "\n",
        "model_file_uw = 'models/ae_k_{}_n_{}'.format(k,n)\n",
        "ae = AE(k,n,seed)\n",
        "ae.train(training_params, validation_params)\n",
        "ae.save(model_file_uw);\n",
        "ae = AE(k,n,seed, filename=model_file_uw)\n",
        "  #ae.plot_constellation();\n",
        "  #ae.end2end(tr)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of x: Tensor(\"truediv:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of fade: Tensor(\"Abs:0\", shape=(1000, 2, 1), dtype=float32)\n",
            "Initial shape of y: Tensor(\"add:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n",
            "\n",
            "Batch Size: 1000, Learning Rate: 0.01, EbNodB: [[[53.42552615 53.42552615]\n",
            "  [53.42552615 53.42552615]]\n",
            "\n",
            " [[48.68232646 48.68232646]\n",
            "  [48.68232646 48.68232646]]\n",
            "\n",
            " [[73.55091364 73.55091364]\n",
            "  [73.55091364 73.55091364]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[37.42731416 37.42731416]\n",
            "  [37.42731416 37.42731416]]\n",
            "\n",
            " [[35.5247825  35.5247825 ]\n",
            "  [35.5247825  35.5247825 ]]\n",
            "\n",
            " [[51.51461488 51.51461488]\n",
            "  [51.51461488 51.51461488]]], Iterations: 10000\n",
            "0.945\n",
            "0.06699997\n",
            "0.032999992\n",
            "0.049000025\n",
            "0.027999997\n",
            "0.026000023\n",
            "0.04400003\n",
            "0.01700002\n",
            "0.032999992\n",
            "0.027999997\n",
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of x: Tensor(\"truediv:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of fade: Tensor(\"Abs:0\", shape=(1000, 2, 1), dtype=float32)\n",
            "Initial shape of y: Tensor(\"add:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl3hENsvf7hW"
      },
      "source": [
        "## Create and train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0GA7ihjf7hZ"
      },
      "source": [
        "#model_file = 'models/ae_k_{}_n_{}'.format(k,n)\n",
        "#ae = AE(k,n,seed)\n",
        "#ae.train(training_params, validation_params)\n",
        "#ae.save(model_file); # Save the trained autoencoder if you want to reuse it later\n",
        "#sess = tf.Session()\n",
        "#print(sess.run((rmse_uw_0)))"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzqpcwGaf7hm"
      },
      "source": [
        "## Evaluate trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y33vV4xKf7hn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c13aa20-2c69-4cd3-f3b3-0130b3b91d08"
      },
      "source": [
        "ae_uw = AE(k,n,seed, filename=model_file_uw) #Load a pretrained model that you have saved if needed"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of x: Tensor(\"truediv:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of fade: Tensor(\"Abs:0\", shape=(1000, 2, 1), dtype=float32)\n",
            "Initial shape of y: Tensor(\"add:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fasS-Rz1lapW"
      },
      "source": [
        "\n",
        "class AE_Weighted(object):\n",
        "    def __init__(self, k, n, seed=None, filename=None):\n",
        "        assert (n%2 == 0), \"Channel only allows complex symbols -> n must be a multiple of 2\"\n",
        "        self.k = k \n",
        "        self.n = n\n",
        "        self.n_complex = int(self.n/2)\n",
        "        self.bits_per_symbol = self.k/self.n_complex\n",
        "        self.M = 2**self.k\n",
        "        self.seed = seed if (seed is not None) else int(time.time())           \n",
        "        self.graph = None\n",
        "        self.sess = None   \n",
        "        self.vars = None\n",
        "        self.saver = None   \n",
        "        self.constellations = None\n",
        "        self.blers = None\n",
        "        self.create_graph()\n",
        "        self.create_session()\n",
        "        if filename is not None:    \n",
        "            self.load(filename)       \n",
        "        return\n",
        "    \n",
        "    def generate_distances(self,int_batch_size,bins):\n",
        "        # int_batch_size: 1000\n",
        "        # bins = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "\n",
        "        t = np.linspace(0,1,int_batch_size)\n",
        "        triangle1 = signal.sawtooth(2 * np.pi * 5 * t, 0.5)\n",
        "        triangle1 = 10*triangle1\n",
        "        triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "        triangle2 = signal.sawtooth(5 * np.pi * 5 * t, 0.5)\n",
        "        triangle2 = 10*triangle2\n",
        "        triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "        triangle3 = triangle1 + triangle2\n",
        "        triangle3 = triangle3/15*(7)\n",
        "        #print(s)\n",
        "\n",
        "        bins = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "        tr = np.digitize(triangle3, bins, right=True)\n",
        "        replacements = {0:7, 1:6, 2:5, 3:4, 4:3, 5:2, 6:1, 7:0}\n",
        "        replacer = replacements.get\n",
        "        tr = ([replacer(n, n) for n in tr])\n",
        "     \n",
        "        return tr\n",
        "\n",
        "\n",
        "    \n",
        "    def create_graph(self):\n",
        "        '''This function creates the computation graph of the autoencoder'''\n",
        "        self.graph = tf.Graph()        \n",
        "        with self.graph.as_default():    \n",
        "            tf.set_random_seed(self.seed)\n",
        "            batch_size = tf.placeholder(tf.int32, shape=())\n",
        "            \n",
        "            noise_std = tf.placeholder(tf.float32, shape=[1000,2,2])\n",
        "            #ad_noise_std = (tf.linspace(0.0, noise_std, tf.shape(x), name=\"linspace\"))\n",
        "            #ad_noise_std = tf.random_normal(tf.shape(x), mean=0.0, stddev=7)\n",
        "            #print(\"Initial shape of ad_noise:\",ad_noise_std)\n",
        "            #noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=ad_SNR)\n",
        "            x = self.encoder(s)     \n",
        "            \n",
        "            # Channel\n",
        "            #noise_std = tf.placeholder(tf.float32, shape=())\n",
        "            noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=noise_std) \n",
        "     \n",
        "\n",
        "            fade = tf.random.normal(shape=tf.shape(x))\n",
        "            sparr1,sparr2 = tf.split(fade,num_or_size_splits=2, axis=2)\n",
        "            complex_fade = tf.complex(sparr1, sparr2)\n",
        "            fade = tf.abs(complex_fade)\n",
        "            #fade = tf.math.sqrt(1/2)*fade\n",
        "\n",
        "            #fade = 1\n",
        "            y = tf.multiply(x,fade) + noise\n",
        "            \n",
        "           \n",
        "            # Receiver\n",
        "            s_hat = self.decoder(y)\n",
        "            correct_s_hat = tf.argmax(tf.nn.softmax(s_hat), axis=1)\n",
        "\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat, weights= (s+1)**(4))\n",
        "\n",
        "\n",
        "            correct_predictions = tf.equal(tf.argmax(tf.nn.softmax(s_hat), axis=1), s)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "            bler = 1-accuracy\n",
        "\n",
        "            # Optimizer\n",
        "            lr = tf.placeholder(tf.float32, shape=()) # We can feed in any desired learning rate for each step     \n",
        "            train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
        "            #train_op_0 = tf.train.AdamOptimizer(lr).minimize(cross_entropy_0)\n",
        "            #lr = tf.train.exponential_decay(1e-10, global_step=cross_entropy, decay_steps=100, decay_rate=1.30)\n",
        "\n",
        "        \n",
        "            # References to graph variables we need to access later \n",
        "            self.vars = {\n",
        "                'accuracy': accuracy,\n",
        "                'batch_size': batch_size,\n",
        "                'bler': bler,\n",
        "                'cross_entropy': cross_entropy,\n",
        "                'init': tf.global_variables_initializer(),\n",
        "                'lr': lr,\n",
        "                'noise_std': noise_std,\n",
        "                'train_op': train_op,\n",
        "                's': s,\n",
        "                's_hat': s_hat,\n",
        "                'correct_s_hat': correct_s_hat,\n",
        "                'x': x,\n",
        "            }            \n",
        "            self.saver = tf.train.Saver()\n",
        "        return\n",
        "    \n",
        "    def create_session(self):\n",
        "        '''Create a session for the autoencoder instance with the compuational graph'''\n",
        "        self.sess = tf.Session(graph=self.graph)        \n",
        "        self.sess.run(self.vars['init'])\n",
        "        return\n",
        "    \n",
        "    def encoder(self, input):\n",
        "        '''The transmitter'''\n",
        "        self.weight_var_rec = self.weight_variable((self.M,self.M)) # shape = (8,8)\n",
        "        self.embedding_lookup_rec = tf.nn.embedding_lookup(self.weight_var_rec, input)\n",
        "        print(self.embedding_lookup_rec)\n",
        "        x = tf.nn.elu(self.embedding_lookup_rec)\n",
        "        #x = tf.layers.dense(self.embedding_lookup_rec, self.M, activation=tf.nn.relu)\n",
        "        x = tf.layers.dense(x, self.M, activation=None)\n",
        "        x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        x = tf.reshape(x, shape=[-1,self.n_complex,2])\n",
        "        print(x);\n",
        "        #Average power normalization\n",
        "        x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x))) \n",
        "        return x\n",
        "    \n",
        "    def decoder(self, input):\n",
        "        '''The Receiver'''\n",
        "        #input = self.flip_decoder(input)\n",
        "        y = tf.reshape(input, shape=[-1,self.n])\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        print(y);\n",
        "        return y\n",
        "    \n",
        "    def EbNo2Sigma(self, ebnodb):\n",
        "        '''Convert Eb/No in dB to noise standard deviation'''\n",
        "        ebno = 10**(ebnodb/10)\n",
        "        return 1/np.sqrt(2*self.bits_per_symbol*ebno)\n",
        "    \n",
        "    def gen_feed_dict(self, batch_size, ebnodb, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['lr']: lr,\n",
        "        }\n",
        "\n",
        "    def gen_e2e_feed_dict(self, batch_size, ebnodb, s_input):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['s']: s_input,\n",
        "        }   \n",
        "    \n",
        "    def load(self, filename):\n",
        "        '''Load an pre_trained model'''\n",
        "        return self.saver.restore(self.sess, filename)\n",
        "        \n",
        "    def plot_constellation(self, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = self.transmit(range(self.M))\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(self.n_complex):\n",
        "            image = plt.figure(figsize=(6,6))\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-maxrange,maxrange)\n",
        "            plt.ylim(-maxrange,maxrange)\n",
        "            for i in range(self.M):       \n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\",marker='x')   \n",
        "            image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            image.suptitle('%d. complex symbol' % (k+1))\n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "        return x, image\n",
        "    \n",
        "    def save(self, filename):\n",
        "        '''Save the current model'''\n",
        "        return self.saver.save(self.sess, filename)  \n",
        "    \n",
        "    def test_step(self, batch_size, ebnodb):\n",
        "        '''Compute the BLER over a single batch and Eb/No'''\n",
        "        bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n",
        "        return bler\n",
        "    \n",
        "    def transmit(self, s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['x'], feed_dict={self.vars['s']: s})\n",
        "\n",
        "    def end2end(self, batch_size, ebnodb, input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['correct_s_hat'], feed_dict=self.gen_e2e_feed_dict(batch_size, ebnodb, input_s)) \n",
        "        #print(self.sess.run(self.vars['correct_s_hat'], feed_dict={self.vars['s']: input_s}))     \n",
        "\n",
        "    #print(self.sess.run(self.vars['s_hat'], feed_dict={self.vars['s']: s}))\n",
        "\n",
        "    def train(self, training_params, validation_params):  \n",
        "        #s_input = self.generate_distances(100,np.array([0, 1, 2, 3, 4, 5, 6]))\n",
        "        \n",
        "        '''Training and validation loop'''\n",
        "        for index, params in enumerate(training_params):            \n",
        "            batch_size, lr, ebnodb, iterations = params            \n",
        "            print('\\nBatch Size: ' + str(batch_size) +\n",
        "                  ', Learning Rate: ' + str(lr) +\n",
        "                  ', EbNodB: ' + str(ebnodb) +\n",
        "                  ', Iterations: ' + str(iterations))\n",
        "            \n",
        "            val_size, val_ebnodb, val_steps = validation_params[index]\n",
        "            \n",
        "            for i in range(iterations):\n",
        "                self.train_step(batch_size, ebnodb, lr)    \n",
        "                if (i%val_steps==0):\n",
        "                    #bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_new_feed_dict(val_size, val_ebnodb, lr, s_input))\n",
        "                    bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size, val_ebnodb, lr))\n",
        "                    print(bler)                           \n",
        "        return       \n",
        "    \n",
        "    def train_step(self, batch_size, ebnodb, lr):\n",
        "        '''A single training step'''\n",
        "        #self.sess.run(self.vars['train_op'], feed_dict=self.gen_new_feed_dict(batch_size, ebnodb, lr, s_input))\n",
        "        self.sess.run(self.vars['train_op'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr))\n",
        "        return \n",
        "    \n",
        "    def weight_variable(self, shape):\n",
        "        '''Xavier-initialized weights optimized for ReLU Activations'''\n",
        "        (fan_in, fan_out) = shape\n",
        "        low = np.sqrt(6.0/(fan_in + fan_out)) \n",
        "        high = -np.sqrt(6.0/(fan_in + fan_out))\n",
        "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n",
        "    \n",
        "    def bler_sim(self, ebnodbs, batch_size, iterations):\n",
        "        '''Monte Carlo simulations of BLER for a range of Eb/No\n",
        "           Sometimes we to compute statistics for batch sizes that do not fit into the GPUs memory.\n",
        "           You can average over multiple batches with small size instead.           \n",
        "        '''\n",
        "        BLER = np.zeros_like(ebnodbs)\n",
        "        for i in range(iterations):\n",
        "            bler = np.array([self.sess.run(self.vars['bler'],\n",
        "                            feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0)) for ebnodb in ebnodbs])\n",
        "            BLER = BLER + bler/iterations\n",
        "        return BLER\n",
        "    \n",
        "    def plot_bler(self, EbNodB, BLER):\n",
        "        '''Plot a BLER curve'''\n",
        "        image = plt.figure(figsize=(10,8))\n",
        "        plt.plot(EbNodB, BLER, '-r', linewidth=2.0)\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('EbNo (dB)', fontsize=18)\n",
        "        plt.ylabel('Block-error rate', fontsize=18)\n",
        "        plt.grid(True)\n",
        "        plt.ylim([1e-5,1])\n",
        "        return image"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjh_J3MIlyrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d6a6dd0-a420-408a-8959-44ffc728ad3d"
      },
      "source": [
        "train_EbNodB = ad_noise_std\n",
        "val_EbNodB = train_EbNodB\n",
        "\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "training_params = [\n",
        "    #batch_size, lr, ebnodb, iterations\n",
        "    [1000 , lr, train_EbNodB, 10000]\n",
        "    ]\n",
        "\n",
        "validation_params = [\n",
        "    #batch_size, ebnodb, val_steps \n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000]\n",
        "    ]\n",
        "\n",
        "model_file = 'models/ae_k_{}_n_{}'.format(k,n)\n",
        "ae_Weighted = AE_Weighted(k,n,seed)\n",
        "ae_Weighted.train(training_params, validation_params)\n",
        "ae_Weighted.save(model_file);\n",
        "ae_Weighted = AE_Weighted(k,n,seed, filename=model_file)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n",
            "\n",
            "Batch Size: 1000, Learning Rate: 0.01, EbNodB: [[[53.42552615 53.42552615]\n",
            "  [53.42552615 53.42552615]]\n",
            "\n",
            " [[48.68232646 48.68232646]\n",
            "  [48.68232646 48.68232646]]\n",
            "\n",
            " [[73.55091364 73.55091364]\n",
            "  [73.55091364 73.55091364]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[37.42731416 37.42731416]\n",
            "  [37.42731416 37.42731416]]\n",
            "\n",
            " [[35.5247825  35.5247825 ]\n",
            "  [35.5247825  35.5247825 ]]\n",
            "\n",
            " [[51.51461488 51.51461488]\n",
            "  [51.51461488 51.51461488]]], Iterations: 10000\n",
            "0.99\n",
            "0.357\n",
            "0.35399997\n",
            "0.29799998\n",
            "0.306\n",
            "0.301\n",
            "0.31699997\n",
            "0.29299998\n",
            "0.255\n",
            "0.254\n",
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsoF-I7Rf7hy"
      },
      "source": [
        "### Plot of learned constellations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjGW8S_df7h0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "67d49f6a-e725-4064-bbbe-4cc71cfc8f85"
      },
      "source": [
        "import itertools\n",
        "def plot_constellation_2(ae, arr, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = ae.transmit(arr)\n",
        "        #marker = itertools.cycle(('1', '2', '3', '4', '^', '6', '7', '8'))\n",
        "        weights = [1,4,9,16,25,36,49,64]\n",
        "        #weights = [1,1,1,1,1,1,1,1]\n",
        "        #weights = [1,8,27,64,125,216,343,512]\n",
        "        #weights = [1,2,3,4,5,6,7,8]\n",
        "        #weights = [1,16,81,256,625, 1296, 2401, 4096]\n",
        "        #weights = [1,256,6561,65536, 390625, 1.6, 5.7, 16.7]\n",
        "        #x = ae.transmit(np.ones(ae.M)*n)\n",
        "        #print(ae.M)\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(ae.n_complex):\n",
        "            \n",
        "#            image = plt.figure(figsize=(6,6))\n",
        "            image = plt.plot()\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-2,2)\n",
        "            plt.ylim(-2,2)\n",
        "            #plt.show() \n",
        "            #plt.ion()\n",
        "            xshape = np.shape(x)\n",
        "            for i in range(xshape[0]):      \n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\")\n",
        "                plt.annotate(weights[i], (x[i,k,0],x[i,k,1]))\n",
        "\n",
        "\n",
        "                #plt.show()              \n",
        "                #plt.pause(1)\n",
        "                #plt.hold(True)\n",
        "            #image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            #image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            #image.suptitle('%d. complex symbol' % (k+1))\n",
        "            #image.canvas.draw()\n",
        "            #image.canvas.flush_events()\n",
        "            \n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "            #time.sleep(0.1)\n",
        "        return x, image\n",
        "\n",
        "#plot_constellation_2(ae,range(0,ae.M))\n",
        "plot_constellation_2(ae_Weighted, range(0, ae_Weighted.M))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-124-ad5ab0cfffa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#plot_constellation_2(ae,range(0,ae.M))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mplot_constellation_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mae_Weighted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mae_Weighted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-124-ad5ab0cfffa9>\u001b[0m in \u001b[0;36mplot_constellation_2\u001b[0;34m(ae, arr, maxrange)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_constellation_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxrange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m'''Generate a plot of the current constellation'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;31m#marker = itertools.cycle(('1', '2', '3', '4', '^', '6', '7', '8'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m49\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-122-99fda1a1280a>\u001b[0m in \u001b[0;36mtransmit\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;34m'''Returns the transmitted sigals corresponding to message indices'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mend2end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mebnodb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMbROTHNMa79"
      },
      "source": [
        "**THE RMSE Calculations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7-W-Be2auJV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "5b188212-48e0-4a19-b8b7-1a48f5515b8a"
      },
      "source": [
        "def rmse(predictions,targets):\n",
        "  return (np.sqrt((np.subtract(predictions,targets) ** 2).mean()))   \n",
        "\n",
        "tr_hat = ae.end2end(len(tr), 7, tr)\n",
        "#print(np.shape(tr))\n",
        "#print(np.shape(tr_hat))\n",
        "print(tr_hat)\n",
        "\n",
        "\n",
        "tr_hat_w = ae_Weighted.end2end(len(tr), 7, tr)\n",
        "\n",
        "rmse_uw = {}\n",
        "for j in range(M):\n",
        "  rmse_uw[j] = rmse(([tr_hat[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "\n",
        "rmse_w = {}\n",
        "for j in range(M):\n",
        "  rmse_w[j] = rmse(([tr_hat_w[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "\n",
        "\n",
        "\n",
        "#message_factor = [15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,1]\n",
        "message_factor = np.flip(np.arange(M))\n",
        "message_factor[M-1] = 1\n",
        "rmse_uwA = []\n",
        "for i in rmse_uw.values():\n",
        "  rmse_uwA.append(i)\n",
        "\n",
        "rmse_wA = []\n",
        "for i in rmse_w.values():\n",
        "  rmse_wA.append(i)\n",
        "\n",
        "    \n",
        "#rmse_uw = [rmse_uw_0, rmse_uw_1, rmse_uw_2, rmse_uw_3, rmse_uw_4, rmse_uw_5, rmse_uw_6, rmse_uw_7, rmse_uw_8, rmse_uw_9, rmse_uw_10, rmse_uw_11, rmse_uw_12, rmse_uw_13, rmse_uw_14, rmse_uw_15]\n",
        "rmse_uw = (np.divide(rmse_uwA,message_factor))\n",
        "#rmse_w = [rmse_w_0, rmse_w_1, rmse_w_2, rmse_w_3, rmse_w_4, rmse_w_5, rmse_w_6, rmse_w_7, rmse_w_8, rmse_w_9, rmse_w_10, rmse_w_11, rmse_w_12, rmse_w_13, rmse_w_14, rmse_w_15]\n",
        "rmse_w = (np.divide(rmse_wA,message_factor))\n",
        "#message = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
        "message = np.arange(M)\n",
        "\n",
        "print(rmse_uw)\n",
        "plt.plot(message, rmse_uw, '-o');\n",
        "plt.plot(message, rmse_w, '-*');\n",
        "plt.xlabel('Message Bit')\n",
        "plt.ylabel('RMSE deviation log2')\n",
        "plt.legend(['Unweighted', 'Weighted(^2)'])\n",
        "\n",
        "\n",
        "print(np.sum(rmse_uw))\n",
        "print(np.sum(rmse_w))\n",
        "\n",
        "print(np.sum(rmse_uwA))\n",
        "print(np.sum(rmse_wA))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-c0593a056488>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtr_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend2end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(np.shape(tr))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#print(np.shape(tr_hat))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-118-12c226836400>\u001b[0m in \u001b[0;36mend2end\u001b[0;34m(self, batch_size, ebnodb, input_s)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mend2end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mebnodb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;34m'''Returns the transmitted sigals corresponding to message indices'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'correct_s_hat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_e2e_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mebnodb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;31m#print(self.sess.run(self.vars['correct_s_hat'], feed_dict={self.vars['s']: input_s}))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-118-12c226836400>\u001b[0m in \u001b[0;36mgen_e2e_feed_dict\u001b[0;34m(self, batch_size, ebnodb, s_input)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'noise_std'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEbNo2Sigma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mebnodb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         }   \n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxostCb-vY1i"
      },
      "source": [
        "ae.plot_constellation();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtoc7OKCUgko"
      },
      "source": [
        "# 8-PSK Modulation\n",
        "#8PSK constellation \n",
        "#Demodulation matrx\n",
        "#Qfunction \n",
        "\n",
        "import numpy as np\n",
        "from scipy import special\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import subprocess\n",
        "import shlex\n",
        "\n",
        "\n",
        "#Generating constellation points\n",
        "s = np.zeros((8,2))\n",
        "s_comp = np.zeros((8,1))+1j*np.zeros((8,1))\n",
        "for i in range(8):\n",
        "\ts[i,:] = np.array(([np.cos(i*2*np.pi/8),np.sin(i*2*np.pi/8)])) #vector\n",
        "\ts_comp[i] = s[i,0]+1j*s[i,1] #equivalent complex number\n",
        "\n",
        "#Generating demodulation matrix\n",
        "A = np.zeros((8,2,2))\n",
        "A[0,:,:] = np.array(([np.sqrt(2)-1,1],[np.sqrt(2)-1,-1]))\n",
        "A[1,:,:] = np.array(([np.sqrt(2)+1,-1],[-(np.sqrt(2)-1),1]))\n",
        "A[2,:,:] = np.array(([-(np.sqrt(2)+1),1],[np.sqrt(2)+1,1]))\n",
        "A[3,:,:] = np.array(([np.sqrt(2)-1,1],[-(np.sqrt(2)+1),-1]))\n",
        "A[4,:,:] = np.array(([-(np.sqrt(2)-1),-1],[-(np.sqrt(2)-1),1]))\n",
        "A[5,:,:] = np.array(([-(np.sqrt(2)+1),1],[np.sqrt(2)-1,-1]))\n",
        "A[6,:,:] = np.array(([np.sqrt(2)+1,-1],[-(np.sqrt(2)+1),-1]))\n",
        "A[7,:,:] = np.array(([-(np.sqrt(2)-1),-1],[np.sqrt(2)+1,1]))\n",
        "\n",
        "#Gray code\n",
        "gray = np.zeros((8,3))\n",
        "gray[0,:] = np.array(([0,0,0]))\n",
        "gray[1,:] = np.array(([0,0,1]))\n",
        "gray[2,:] = np.array(([0,1,1]))\n",
        "gray[3,:] = np.array(([0,1,0]))\n",
        "gray[4,:] = np.array(([1,1,0]))\n",
        "gray[5,:] = np.array(([1,1,1]))\n",
        "gray[6,:] = np.array(([1,0,1]))\n",
        "gray[7,:] = np.array(([1,0,0]))\n",
        "\n",
        "\n",
        "#Q-function\n",
        "def qfunc(x):\n",
        "\treturn 0.5*special.erfc(x/np.sqrt(2))"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LDujO11Ulo5"
      },
      "source": [
        "def decode(vec_comp):\n",
        "\tvec = np.zeros((2,1))\n",
        "\tvec[0] = np.real(vec_comp)\n",
        "\tvec[1] = np.imag(vec_comp)\n",
        "\tfor i in range(8):\n",
        "\t\ty = A[i,:,:]@vec\n",
        "\t\tif (y [0] >= 0) and (y[1] >= 0):\n",
        "\t\t\treturn s_comp[i]\n",
        "\n",
        "#Extracting bits from demodulated symbols\n",
        "def detect(vec_comp):\n",
        "\tvec = np.zeros((2,1))\n",
        "\tvec[0] = np.real(vec_comp)\n",
        "\tvec[1] = np.imag(vec_comp)\n",
        "\tfor i in range(8):\n",
        "\t\tif s[i,0]==vec[0] and s[i,1] == vec[1]:\n",
        "\t\t\treturn gray[i,:]\n",
        "\n",
        "#Demodulating symbol stream from received noisy  symbols\n",
        "def rx_symb(mat):\n",
        "\tlen = mat.shape[1]\n",
        "\trx_symb_stream = []\n",
        "\tfor i in range(len):\n",
        "\t\trx_symb_stream.append(decode(mat[:,i]))\n",
        "\treturn rx_symb_stream\n",
        "\n",
        "#Getting received bit stream from demodulated symbols\n",
        "def rx_bit(mat):\n",
        "\tlen = mat.shape[1]\n",
        "\trx_bit_stream = []\n",
        "\tfor i in range(len):\n",
        "\t\trx_bit_stream.append(detect(mat[:,i]))\n",
        "\treturn rx_bit_stream"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPu3HX_SUov_"
      },
      "source": [
        "#Generates a bitstream\n",
        "def bitstream(n):\n",
        "\treturn np.random.randint(0,2,n)\n",
        "\n",
        "#Converts bits to 8-PSK symbols using gray code\n",
        "def mapping(b0,b1,b2):\n",
        "\tif (b0 == 0 and b1 == 0 and b2 == 0):\n",
        "\t\treturn s[0,:]\n",
        "\telif (b0 == 0 and b1 == 0 and b2 == 1):\n",
        "\t\treturn s[1,:]\n",
        "\telif (b0 == 0 and b1 == 1 and b2 == 1):\n",
        "\t\treturn s[2,:]\n",
        "\telif (b0 == 0 and b1 == 1 and b2 == 0):\n",
        "\t\treturn s[3,:]\n",
        "\telif( b0 == 1 and b1 == 1 and b2 == 0):\n",
        "\t\treturn s[4,:]\n",
        "\telif(b0==1 and b1 == 1 and b2 == 1):\n",
        "\t\treturn s[5,:]\n",
        "\telif(b0==1 and b1 == 0 and b2 == 1):\n",
        "\t\treturn s[6,:]\n",
        "\telif(b0==1 and b1 == 0 and b2 == 0):\n",
        "\t\treturn s[7,:]\n",
        "\n",
        "\n",
        "#Converts bitstream to 8-PSK symbol stream\n",
        "def symb(bits):\n",
        "\tsymbol =[]\n",
        "\ti = 0\n",
        "\twhile(1):\n",
        "\t\ttry:\n",
        "\t\t\tsymbol.append(mapping(bits[i],bits[i+1],bits[i+2]))\n",
        "\t\t\ti = i+3\n",
        "\t\texcept IndexError:\n",
        "\t\t\treturn symbol\n",
        "\n",
        "#Converts bitstream to 8-PSK complex symbol stream\n",
        "def CompSymb(bits):\n",
        "\tsymbols_lst = symb(bits)\n",
        "\tsymbols = np.array(symbols_lst).T #Symbol vectors\n",
        "\tsymbols_comp = symbols[0,:]+1j*symbols[1,:] #Equivalent complex symbols\n",
        "\treturn symbols_comp"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPNtUtWBUryp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "ab649561-b714-4e51-c737-4e1b5850f93d"
      },
      "source": [
        "\n",
        "#SNR range\n",
        "snrlen=15\n",
        "\n",
        "#SNR in dB and actual per bit \n",
        "#(Check Proakis for factor of 6)\n",
        "snr_db = np.linspace(0,snrlen,snrlen)\n",
        "snr = 6*10**(0.1*snr_db)\n",
        "\n",
        "#Bitstream size\n",
        "bitsimlen = 99999\n",
        "\n",
        "#Symbol stream size\n",
        "simlen = bitsimlen //3\n",
        "\n",
        "#Generating bitstream\n",
        "bits = bitstream(bitsimlen)\n",
        "\n",
        "#Converting bits to Gray coded 8-PSK symbols\n",
        "#Intermediate steps  required for converting list to\n",
        "#numpy matrix\n",
        "symbols_lst = symb(bits)\n",
        "symbols = np.array(symbols_lst).T #Symbol vectors\n",
        "symbols_comp = symbols[0,:]+1j*symbols[1,:] #Equivalent complex symbols\n",
        "\n",
        "ser =[]\n",
        "ser_anal=[]\n",
        "ber = []\n",
        "\n",
        "#SNRloop\n",
        "for k in range(0,snrlen):\n",
        "\treceived = []\n",
        "\tt=0\n",
        "\t#Complex noise\n",
        "\tnoise_comp = np.random.normal(0,1,simlen)+1j*np.random.normal(0,1,simlen)\n",
        "\t#Generating complex received symbols\n",
        "  #fade_comp = np.random.normal(0,1,simlen)+1j*np.random.normal(0,1,simlen)\n",
        "  #fade_comp = np.abs(fade_comp)\n",
        "  #fade_comp = np.math.sqrt(1/2)*fade_comp\n",
        "\ty_comp = np.sqrt(snr[k])*symbols_comp +noise_comp\n",
        "\tbrx = []\n",
        "\tfor i in range(simlen):\n",
        "\t\tsrx_comp = decode(y_comp[i]) #Received Symbol\n",
        "\t\tbrx.append(detect(srx_comp))  #Received Bits\n",
        "\t\tif symbols_comp[i]==srx_comp:\n",
        "\t\t\tt+=1; #Counting symbol errors\n",
        "\t#Evaluating SER\n",
        "\tser.append(1-(t/33334.0))\n",
        "\tser_anal.append(2*qfunc((np.sqrt(snr[k]))*np.sin(np.pi/8)))\n",
        "\t#Received bitstream\n",
        "\tbrx=np.array(brx).flatten()\n",
        "\t#Evaluating BER\n",
        "\tbit_diff = bits-brx\n",
        "\tber.append(1-len(np.where(bit_diff == 0)[0])/bitsimlen)\n",
        "\n",
        "\n",
        "\n",
        "#Plots\n",
        "plt.semilogy(snr_db,ser_anal,label='SER Analysis')\n",
        "plt.semilogy(snr_db,ser,'o',label='SER Sim')\n",
        "plt.semilogy(snr_db,ber,label='BER Sim')\n",
        "plt.xlabel('SNR$\\\\left(\\\\frac{E_b}{N_0}\\\\right)$')\n",
        "plt.ylabel('$P_e$')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFZCQNe9f7iD"
      },
      "source": [
        "### BLER Simulations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wukzCBJff7iE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "da3aa236-c89f-4eca-d282-0a9a8dc07fd6"
      },
      "source": [
        " ebnodbs = np.linspace(0,14,15)\n",
        "BLER_8PSK = [0.3478959, 0.2926128, 0.2378847, 0.1854187, 0.1372344, 0.0953536, 0.0614003, 0.0360195, 0.0185215, 0.0082433, 0.0030178, 0.0008626, 0.0001903, 0.0000289, 0.0000027, ]\n",
        "blers = ae.bler_sim(ebnodbs, 1000000, 1);\n",
        "ae.plot_bler(ebnodbs, blers);\n",
        "blers_w = ae_Weighted.bler_sim(ebnodbs, 1000000, 1);\n",
        "#ae_Weighted.plot_bler(ebnodbs, blers_w);\n",
        "plt.plot(ebnodbs, blers_w)\n",
        "plt.semilogy(snr_db,ser,'o')\n",
        "plt.plot(ebnodbs,BLER_8PSK);\n",
        "plt.legend(['Autoencoder (Rayleigh+AWGN)', 'Weighted Autoencoder(Rayleigh+AWGN)', 'SER Sim(AWGN)', '8PSK(AWGN)'], prop={'size': 16}, loc='lower left');"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-b2b62361f86b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mebnodbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mBLER_8PSK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.3478959\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2926128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2378847\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1854187\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1372344\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0953536\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0614003\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0360195\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0185215\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0082433\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0030178\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0008626\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0001903\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0000289\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0000027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mblers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbler_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mebnodbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_bler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mebnodbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mblers_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mae_Weighted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbler_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mebnodbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-118-12c226836400>\u001b[0m in \u001b[0;36mbler_sim\u001b[0;34m(self, ebnodbs, batch_size, iterations)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             bler = np.array([self.sess.run(self.vars['bler'],\n\u001b[0;32m--> 279\u001b[0;31m                             feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0)) for ebnodb in ebnodbs])\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0mBLER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBLER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbler\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mBLER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-118-12c226836400>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             bler = np.array([self.sess.run(self.vars['bler'],\n\u001b[0;32m--> 279\u001b[0;31m                             feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0)) for ebnodb in ebnodbs])\n\u001b[0m\u001b[1;32m    280\u001b[0m             \u001b[0mBLER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBLER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbler\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mBLER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1157\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape () for Tensor 'Placeholder_1:0', which has shape '(1000, 2, 2)'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMl-Rljrf7iR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}