{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Adaptive_SNR_Testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neelabhro/Deep-Learning-based-Wireless-Communications/blob/main/Adaptive_SNR_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIoYASfEf7go"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKl9e4A0H1lk",
        "outputId": "9a574f49-48d5-4068-9db7-8aef2e4f9d4c"
      },
      "source": [
        "# magic command to use TF 1.X in colaboraty when importing tensorflow\n",
        "%tensorflow_version 1.x \n",
        "import tensorflow as tf                       # imports the tensorflow library to the python kernel\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)    # sets the amount of debug information from TF (INFO, WARNING, ERROR)\n",
        "import time\n",
        "import random\n",
        "print(\"Using tensorflow version:\", tf.__version__)\n",
        "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using tensorflow version: 1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiCDuiGqf7gr"
      },
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "import math\n",
        "pi = tf.constant(math.pi)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrLsO1Nxf7g3"
      },
      "source": [
        "#### System parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spN0MeqFD7x-",
        "outputId": "1fb8bde2-d86c-446e-d98f-ff2575613ad3"
      },
      "source": [
        "batch_size = 1000\n",
        "########## BIT FLIPPING ON\n",
        "#print(tr)\n",
        "#plt.plot(t, tr)\n",
        "T1 = random.randint(1, 10)\n",
        "T2 = random.randint(1, 10)\n",
        "M = 256\n",
        "t = np.linspace(0, 1, batch_size)\n",
        "triangle1 = signal.sawtooth(T1 * np.pi * 5 * t, 0.5)\n",
        "triangle1 = M/2*(triangle1)\n",
        "triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "triangle2 = signal.sawtooth(T2 * np.pi * 5 * t, 0.5)\n",
        "triangle2 = M/2*(triangle2)\n",
        "triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "triangle3 = triangle1 + triangle2\n",
        "#triangle3 = triangle3/15*(7)\n",
        "tr = triangle3\n",
        "#bins = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])\n",
        "bins = np.arange(M-1)\n",
        "tr = np.digitize(triangle3, bins, right=True)\n",
        "#plt.plot(t, triangle3)\n",
        "\n",
        "#tr = np.flip(tr)\n",
        "#print(tr)\n",
        "tr = np.floor(np.random.uniform(0,M, 10000))\n",
        "print(tr)\n",
        "#replacements = {0:7, 1:6, 2:5, 3:4, 4:3, 5:2, 6:1, 7:0}\n",
        "#replacements = {0:15, 1:14, 2:13, 3:12, 4:11, 5:10, 6:9, 7:8, 8:7, 9:6, 10:5, 11:4, 12:3, 13:2, 14:1, 15:0}\n",
        "rp1 = np.arange(M)\n",
        "rp2 = np.flip(np.arange(M))\n",
        "replacements = dict(zip(rp1,rp2))\n",
        "#print(rp2)\n",
        "replacer = replacements.get\n",
        "tr = ([replacer(n, n) for n in tr])\n",
        "\n",
        "#tr2 = ([replacer(n, n) for n in triangle3])\n",
        "print(tr)\n",
        "#plt.plot(t, tr)\n",
        "#plt.legend(['Input signal', 'Quantized and bit flipped'])\n",
        "\n",
        "#s_ind = np.empty(shape=(M,batch_size))\n",
        "s_ind = {}\n",
        "for j in range(M):\n",
        "  s_ind[j] = [i for i, x in enumerate(tr) if x == j]\n",
        "\n",
        "#print([tr[x] for x in s_ind_0])"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[165.  11. 213. ...  83. 148. 131.]\n",
            "[90, 244, 42, 125, 201, 58, 37, 223, 157, 155, 39, 109, 90, 67, 178, 88, 216, 119, 225, 29, 96, 188, 251, 54, 176, 20, 142, 171, 135, 194, 165, 16, 53, 83, 218, 71, 79, 20, 94, 183, 14, 138, 175, 33, 41, 230, 250, 169, 4, 195, 115, 172, 70, 172, 88, 201, 228, 175, 142, 72, 118, 49, 238, 246, 96, 65, 225, 255, 167, 191, 113, 152, 114, 88, 192, 185, 239, 188, 93, 49, 13, 4, 27, 118, 48, 216, 89, 92, 23, 134, 150, 98, 253, 105, 254, 40, 214, 196, 248, 124, 145, 44, 203, 121, 47, 209, 254, 63, 216, 109, 135, 154, 138, 33, 182, 193, 100, 14, 249, 61, 58, 231, 247, 186, 120, 183, 212, 140, 170, 64, 64, 3, 66, 3, 10, 247, 210, 176, 170, 180, 4, 76, 185, 222, 108, 161, 84, 97, 30, 112, 56, 14, 126, 200, 146, 189, 152, 137, 89, 56, 221, 34, 149, 194, 84, 63, 116, 234, 150, 65, 27, 139, 254, 62, 108, 219, 242, 40, 187, 230, 202, 155, 246, 164, 173, 90, 115, 17, 41, 102, 161, 149, 32, 16, 82, 243, 73, 91, 122, 98, 78, 139, 244, 136, 197, 177, 122, 220, 124, 132, 139, 237, 224, 117, 13, 133, 238, 0, 48, 25, 222, 172, 236, 232, 59, 186, 227, 201, 66, 147, 223, 148, 26, 148, 20, 69, 96, 14, 182, 20, 227, 45, 56, 190, 118, 170, 216, 95, 54, 198, 2, 26, 62, 244, 224, 82, 159, 191, 62, 218, 82, 147, 87, 77, 136, 235, 65, 86, 7, 194, 113, 142, 10, 129, 139, 157, 112, 47, 250, 68, 63, 60, 150, 248, 107, 253, 197, 179, 234, 108, 105, 169, 238, 206, 204, 244, 66, 64, 4, 120, 208, 102, 213, 50, 234, 197, 127, 52, 250, 122, 137, 81, 232, 18, 170, 135, 107, 91, 238, 123, 137, 171, 56, 19, 231, 32, 221, 212, 145, 249, 213, 139, 51, 41, 243, 98, 109, 62, 135, 75, 159, 91, 148, 2, 33, 243, 160, 212, 133, 224, 95, 216, 37, 78, 34, 20, 144, 47, 37, 201, 230, 136, 148, 54, 10, 190, 165, 233, 224, 238, 52, 244, 21, 79, 254, 43, 184, 73, 120, 215, 190, 234, 192, 229, 139, 119, 73, 83, 223, 28, 34, 12, 99, 95, 144, 124, 216, 202, 18, 49, 153, 188, 255, 212, 254, 206, 155, 22, 41, 149, 34, 87, 219, 35, 115, 41, 160, 236, 255, 98, 88, 245, 113, 138, 27, 161, 193, 186, 44, 208, 205, 78, 13, 62, 118, 151, 9, 209, 88, 234, 88, 208, 45, 82, 107, 8, 61, 120, 118, 190, 174, 82, 40, 170, 70, 152, 155, 1, 87, 131, 244, 66, 245, 32, 24, 66, 73, 188, 42, 62, 198, 142, 31, 77, 101, 16, 180, 76, 100, 202, 148, 222, 97, 91, 9, 28, 248, 174, 98, 31, 204, 89, 82, 74, 126, 168, 31, 48, 88, 114, 83, 123, 16, 164, 107, 34, 108, 35, 125, 185, 141, 227, 81, 196, 18, 124, 219, 102, 52, 144, 106, 111, 33, 44, 132, 199, 254, 187, 43, 221, 85, 41, 35, 95, 91, 235, 252, 15, 42, 87, 127, 196, 36, 10, 176, 30, 179, 194, 176, 208, 142, 186, 179, 209, 180, 58, 209, 167, 14, 22, 250, 112, 117, 231, 112, 108, 214, 221, 194, 176, 64, 58, 116, 158, 49, 104, 113, 10, 82, 114, 32, 248, 129, 45, 168, 148, 81, 130, 84, 242, 244, 241, 132, 42, 92, 41, 23, 62, 198, 87, 194, 144, 60, 125, 32, 183, 183, 201, 58, 2, 176, 223, 5, 250, 94, 162, 158, 200, 30, 250, 106, 93, 79, 20, 167, 82, 188, 72, 52, 73, 236, 99, 246, 205, 6, 47, 208, 57, 111, 66, 153, 28, 219, 25, 109, 188, 246, 63, 125, 50, 91, 219, 167, 204, 106, 110, 43, 37, 219, 34, 161, 220, 162, 35, 226, 63, 199, 248, 46, 125, 254, 37, 108, 201, 193, 248, 199, 166, 255, 22, 155, 177, 121, 10, 10, 243, 92, 216, 244, 121, 124, 37, 159, 60, 206, 173, 24, 57, 69, 201, 70, 166, 70, 196, 114, 217, 114, 182, 115, 103, 229, 51, 220, 142, 48, 119, 114, 111, 223, 63, 182, 236, 113, 174, 148, 186, 228, 54, 75, 171, 67, 95, 235, 254, 99, 237, 218, 203, 204, 43, 244, 4, 225, 129, 147, 39, 107, 144, 57, 110, 100, 246, 138, 153, 191, 233, 94, 187, 123, 103, 48, 72, 47, 52, 81, 162, 131, 249, 89, 220, 239, 105, 75, 31, 49, 88, 13, 6, 55, 0, 186, 238, 251, 42, 40, 67, 171, 204, 87, 76, 17, 50, 14, 214, 230, 203, 59, 178, 243, 175, 85, 152, 151, 158, 215, 209, 85, 28, 55, 108, 87, 76, 91, 212, 155, 122, 33, 199, 248, 78, 165, 165, 50, 185, 43, 199, 48, 178, 209, 110, 136, 166, 217, 254, 91, 126, 204, 250, 9, 235, 96, 46, 150, 54, 232, 129, 34, 14, 150, 26, 127, 184, 236, 12, 55, 51, 123, 106, 168, 144, 29, 191, 16, 223, 198, 173, 93, 58, 10, 17, 212, 47, 244, 55, 241, 37, 85, 172, 165, 39, 41, 163, 183, 101, 109, 38, 47, 65, 18, 95, 34, 90, 0, 217, 148, 181, 126, 30, 207, 221, 139, 225, 143, 174, 48, 77, 41, 45, 79, 132, 183, 169, 5, 205, 116, 98, 209, 28, 100, 61, 158, 22, 178, 148, 1, 224, 80, 57, 191, 93, 18, 60, 23, 29, 1, 86, 201, 206, 133, 30, 208, 79, 238, 47, 215, 66, 220, 39, 166, 43, 77, 45, 234, 118, 16, 43, 189, 85, 108, 112, 88, 90, 55, 72, 189, 84, 107, 185, 218, 141, 119, 65, 49, 100, 216, 48, 129, 90, 123, 230, 62, 194, 92, 175, 191, 110, 67, 70, 88, 110, 131, 106, 86, 94, 162, 170, 66, 20, 110, 75, 84, 36, 216, 76, 22, 206, 213, 38, 239, 6, 79, 145, 97, 9, 72, 19, 246, 179, 110, 206, 227, 253, 184, 32, 95, 204, 251, 78, 150, 28, 113, 101, 5, 54, 42, 152, 104, 40, 162, 88, 122, 115, 122, 212, 33, 21, 65, 115, 243, 139, 14, 86, 102, 14, 203, 158, 248, 57, 56, 235, 193, 181, 241, 216, 195, 172, 116, 39, 26, 89, 101, 184, 108, 92, 221, 89, 54, 181, 223, 13, 218, 111, 2, 176, 164, 28, 114, 182, 98, 21, 105, 22, 49, 170, 97, 203, 173, 35, 72, 81, 112, 245, 185, 177, 84, 4, 77, 221, 225, 238, 125, 136, 104, 239, 175, 145, 54, 180, 1, 238, 51, 194, 177, 181, 73, 16, 208, 125, 97, 171, 61, 114, 218, 138, 14, 191, 175, 57, 247, 164, 134, 230, 91, 141, 236, 118, 79, 88, 201, 54, 114, 145, 36, 232, 80, 143, 140, 20, 53, 31, 169, 111, 98, 187, 111, 169, 227, 130, 98, 189, 195, 56, 90, 150, 10, 189, 117, 152, 12, 4, 101, 204, 17, 226, 130, 25, 167, 42, 185, 8, 226, 36, 200, 253, 85, 17, 237, 198, 114, 165, 16, 39, 142, 16, 139, 16, 31, 185, 229, 231, 209, 224, 91, 97, 42, 73, 14, 204, 209, 105, 23, 1, 158, 79, 43, 161, 227, 60, 167, 163, 254, 72, 133, 206, 69, 242, 183, 123, 6, 112, 96, 228, 59, 19, 238, 248, 167, 65, 64, 227, 19, 213, 155, 208, 28, 149, 118, 49, 215, 255, 191, 35, 220, 50, 198, 247, 90, 211, 245, 93, 214, 248, 26, 1, 96, 228, 166, 213, 164, 109, 107, 25, 145, 73, 55, 102, 143, 219, 88, 187, 200, 68, 179, 248, 233, 52, 187, 106, 163, 236, 125, 26, 44, 98, 75, 59, 203, 92, 225, 174, 182, 35, 227, 60, 129, 2, 159, 151, 141, 127, 71, 193, 153, 101, 139, 168, 7, 24, 66, 13, 97, 95, 1, 30, 37, 135, 54, 143, 210, 46, 60, 170, 234, 142, 130, 22, 222, 42, 145, 71, 81, 209, 153, 165, 22, 215, 116, 38, 40, 55, 22, 158, 201, 121, 28, 44, 56, 156, 13, 39, 232, 178, 123, 23, 62, 216, 105, 88, 182, 244, 85, 70, 104, 197, 110, 248, 187, 39, 229, 107, 91, 46, 70, 201, 103, 196, 203, 6, 254, 237, 202, 197, 182, 228, 157, 51, 210, 234, 120, 229, 197, 177, 71, 108, 133, 33, 208, 137, 244, 240, 117, 193, 186, 105, 83, 8, 243, 216, 241, 112, 243, 107, 27, 123, 251, 2, 78, 95, 165, 40, 80, 138, 216, 161, 93, 68, 101, 71, 85, 202, 208, 138, 86, 245, 74, 39, 147, 42, 211, 146, 123, 216, 157, 55, 250, 133, 95, 204, 102, 82, 81, 153, 97, 95, 155, 93, 45, 62, 104, 19, 68, 247, 55, 250, 70, 41, 114, 252, 180, 149, 57, 67, 22, 140, 191, 93, 132, 3, 66, 141, 213, 164, 127, 33, 141, 51, 181, 29, 198, 216, 243, 206, 106, 31, 34, 76, 177, 230, 152, 69, 142, 160, 200, 164, 167, 101, 49, 48, 179, 16, 95, 40, 220, 223, 52, 42, 4, 165, 219, 46, 213, 73, 135, 177, 11, 87, 227, 149, 210, 9, 177, 181, 90, 187, 210, 217, 7, 159, 82, 51, 175, 115, 158, 240, 23, 50, 135, 96, 252, 205, 6, 83, 120, 2, 16, 35, 55, 254, 61, 37, 199, 112, 37, 53, 51, 54, 221, 50, 212, 146, 198, 216, 12, 213, 206, 96, 202, 169, 211, 38, 70, 29, 91, 35, 221, 31, 16, 239, 206, 77, 53, 255, 160, 238, 246, 114, 118, 118, 99, 162, 184, 48, 81, 219, 69, 182, 147, 211, 203, 125, 42, 29, 3, 27, 170, 55, 62, 27, 182, 36, 211, 5, 191, 208, 220, 226, 9, 33, 246, 5, 81, 125, 231, 254, 74, 254, 215, 157, 252, 57, 91, 218, 252, 233, 132, 163, 250, 140, 224, 237, 196, 233, 3, 155, 46, 194, 221, 233, 125, 206, 25, 87, 128, 210, 203, 158, 175, 4, 179, 114, 80, 99, 9, 10, 167, 39, 25, 99, 80, 243, 23, 174, 169, 97, 143, 187, 136, 78, 141, 39, 75, 214, 9, 103, 220, 138, 156, 240, 45, 101, 168, 134, 229, 141, 161, 32, 182, 50, 96, 67, 181, 99, 152, 16, 134, 28, 222, 30, 171, 241, 114, 134, 81, 250, 142, 22, 136, 250, 75, 174, 237, 97, 92, 249, 15, 218, 155, 137, 70, 57, 205, 117, 144, 169, 66, 54, 220, 112, 69, 51, 119, 246, 105, 117, 31, 178, 55, 186, 251, 58, 93, 21, 109, 57, 98, 94, 217, 176, 160, 139, 79, 51, 141, 163, 155, 123, 156, 118, 252, 132, 166, 23, 164, 163, 186, 113, 111, 33, 151, 155, 162, 203, 221, 108, 95, 99, 155, 213, 62, 230, 25, 35, 39, 135, 54, 190, 87, 187, 45, 58, 8, 95, 197, 123, 143, 17, 147, 109, 101, 252, 190, 20, 243, 60, 180, 46, 155, 210, 83, 82, 55, 164, 209, 66, 51, 240, 9, 235, 183, 170, 137, 76, 9, 19, 138, 12, 53, 153, 248, 89, 133, 115, 112, 231, 205, 252, 195, 212, 225, 18, 186, 164, 12, 210, 134, 62, 251, 225, 215, 94, 36, 91, 156, 75, 185, 92, 251, 53, 167, 35, 228, 108, 22, 36, 27, 237, 204, 169, 109, 42, 5, 106, 77, 210, 161, 47, 99, 24, 120, 232, 251, 18, 92, 81, 195, 219, 199, 121, 120, 7, 24, 3, 228, 175, 57, 172, 128, 239, 216, 117, 149, 154, 135, 159, 206, 12, 49, 177, 163, 152, 78, 223, 169, 77, 218, 40, 40, 139, 7, 165, 245, 252, 70, 25, 66, 18, 69, 65, 17, 206, 53, 170, 240, 198, 161, 51, 39, 194, 102, 40, 136, 3, 175, 249, 67, 246, 55, 230, 220, 187, 244, 42, 118, 0, 183, 0, 227, 209, 224, 185, 96, 160, 218, 20, 107, 49, 73, 150, 151, 139, 29, 200, 87, 80, 90, 143, 109, 26, 209, 108, 186, 254, 19, 200, 218, 115, 66, 123, 218, 218, 7, 122, 105, 111, 245, 156, 63, 219, 88, 239, 76, 241, 47, 116, 205, 73, 28, 195, 247, 112, 87, 233, 179, 255, 253, 238, 85, 0, 136, 217, 10, 146, 84, 253, 192, 206, 253, 212, 234, 213, 215, 87, 226, 67, 108, 34, 100, 118, 120, 250, 196, 41, 177, 149, 138, 42, 15, 227, 142, 75, 124, 150, 29, 196, 41, 63, 175, 165, 244, 119, 38, 89, 212, 19, 82, 154, 181, 133, 210, 224, 48, 89, 208, 127, 22, 97, 143, 181, 176, 86, 35, 244, 127, 29, 18, 211, 77, 106, 206, 120, 54, 173, 192, 190, 115, 44, 213, 61, 112, 152, 48, 33, 208, 127, 117, 163, 239, 254, 67, 242, 136, 124, 221, 199, 47, 200, 181, 205, 139, 170, 81, 157, 13, 145, 220, 116, 124, 245, 74, 52, 175, 6, 101, 163, 106, 90, 121, 247, 132, 219, 73, 237, 23, 244, 103, 18, 13, 170, 247, 177, 109, 168, 141, 190, 250, 12, 8, 71, 92, 61, 5, 97, 100, 186, 224, 131, 198, 15, 101, 196, 223, 79, 61, 139, 24, 255, 173, 144, 125, 108, 53, 42, 182, 235, 58, 236, 10, 128, 186, 44, 56, 66, 178, 44, 178, 254, 25, 83, 221, 29, 48, 111, 53, 177, 221, 181, 26, 193, 26, 133, 188, 178, 3, 2, 232, 95, 126, 34, 135, 81, 167, 117, 159, 156, 13, 98, 238, 181, 159, 47, 41, 188, 90, 47, 90, 94, 171, 97, 159, 98, 46, 56, 167, 104, 213, 229, 33, 123, 204, 134, 58, 7, 119, 129, 246, 121, 81, 173, 31, 49, 73, 170, 5, 15, 249, 96, 39, 85, 109, 154, 94, 169, 37, 47, 153, 234, 137, 34, 138, 232, 39, 217, 84, 56, 59, 29, 198, 163, 201, 93, 67, 204, 40, 165, 53, 73, 77, 224, 137, 24, 234, 67, 179, 197, 47, 249, 68, 203, 172, 2, 142, 74, 3, 164, 219, 11, 241, 9, 48, 210, 65, 46, 74, 137, 244, 219, 23, 135, 104, 68, 215, 115, 118, 236, 213, 172, 149, 6, 98, 167, 255, 155, 141, 76, 26, 151, 16, 219, 23, 115, 222, 80, 11, 173, 102, 61, 54, 128, 82, 238, 78, 8, 75, 169, 205, 87, 173, 228, 32, 184, 73, 24, 223, 121, 76, 51, 124, 172, 111, 118, 98, 162, 46, 111, 47, 76, 7, 201, 236, 176, 104, 82, 7, 8, 248, 105, 216, 219, 25, 234, 84, 243, 219, 179, 122, 179, 103, 198, 79, 123, 173, 130, 38, 75, 70, 110, 162, 166, 251, 102, 43, 222, 225, 105, 190, 126, 11, 86, 176, 210, 55, 221, 205, 54, 140, 176, 123, 98, 53, 148, 111, 172, 28, 190, 91, 61, 183, 40, 128, 18, 67, 43, 54, 56, 67, 57, 150, 68, 236, 205, 69, 4, 229, 83, 121, 11, 178, 166, 6, 253, 160, 78, 164, 133, 12, 108, 116, 101, 175, 77, 12, 115, 104, 221, 99, 99, 79, 201, 98, 12, 232, 28, 44, 167, 212, 171, 43, 3, 191, 148, 96, 98, 93, 5, 5, 68, 250, 62, 30, 202, 227, 239, 75, 95, 203, 75, 242, 173, 10, 249, 252, 88, 76, 73, 213, 223, 177, 95, 192, 165, 208, 151, 72, 143, 10, 73, 89, 94, 37, 84, 237, 89, 7, 187, 92, 209, 93, 165, 27, 102, 12, 154, 174, 188, 202, 175, 250, 217, 126, 104, 229, 51, 112, 127, 238, 118, 47, 196, 52, 204, 153, 89, 0, 147, 117, 2, 219, 247, 164, 125, 44, 58, 171, 89, 104, 245, 62, 233, 196, 197, 23, 189, 207, 203, 201, 164, 148, 10, 5, 171, 55, 182, 216, 133, 40, 191, 93, 148, 151, 190, 142, 191, 132, 180, 109, 224, 147, 122, 212, 69, 170, 147, 225, 86, 103, 26, 66, 11, 163, 155, 61, 240, 190, 204, 156, 240, 252, 172, 250, 178, 22, 144, 194, 75, 210, 152, 204, 137, 130, 92, 198, 26, 236, 43, 170, 0, 98, 21, 50, 220, 18, 7, 245, 205, 139, 167, 21, 14, 97, 95, 194, 22, 114, 29, 171, 190, 153, 236, 126, 188, 195, 140, 190, 181, 255, 243, 254, 85, 224, 26, 21, 34, 160, 105, 96, 155, 146, 105, 120, 121, 26, 251, 217, 31, 141, 203, 238, 200, 8, 202, 122, 41, 87, 17, 48, 64, 49, 113, 214, 139, 240, 110, 32, 14, 126, 73, 29, 154, 39, 135, 82, 90, 194, 211, 21, 230, 64, 5, 69, 108, 98, 217, 27, 232, 85, 225, 82, 229, 184, 193, 97, 227, 39, 243, 190, 145, 70, 69, 153, 178, 60, 93, 107, 40, 67, 213, 30, 252, 8, 83, 108, 194, 60, 155, 158, 28, 196, 38, 196, 117, 179, 198, 233, 106, 105, 255, 61, 118, 230, 181, 118, 89, 86, 205, 74, 87, 129, 61, 40, 119, 221, 35, 80, 179, 252, 82, 46, 144, 72, 24, 239, 246, 232, 88, 93, 56, 33, 228, 33, 208, 235, 232, 177, 213, 199, 135, 157, 69, 173, 160, 239, 248, 77, 0, 96, 62, 249, 57, 47, 232, 119, 82, 146, 154, 17, 254, 223, 12, 121, 35, 252, 41, 116, 116, 219, 168, 141, 63, 100, 140, 134, 90, 189, 244, 224, 255, 163, 174, 122, 244, 98, 30, 47, 191, 68, 114, 160, 59, 2, 95, 101, 219, 142, 117, 32, 0, 22, 252, 14, 56, 132, 4, 173, 75, 243, 58, 67, 174, 167, 184, 211, 157, 205, 97, 54, 176, 87, 69, 126, 194, 121, 64, 74, 8, 248, 216, 142, 76, 13, 255, 255, 97, 59, 137, 212, 249, 160, 19, 47, 53, 164, 92, 141, 192, 44, 48, 47, 225, 137, 10, 234, 243, 15, 159, 108, 81, 239, 150, 181, 117, 228, 86, 154, 62, 161, 101, 39, 92, 192, 27, 252, 225, 17, 72, 39, 52, 154, 156, 154, 9, 73, 42, 219, 146, 41, 241, 125, 246, 132, 245, 125, 60, 174, 156, 238, 58, 111, 9, 158, 83, 37, 118, 176, 102, 200, 202, 63, 84, 56, 104, 211, 151, 221, 239, 230, 13, 182, 18, 146, 47, 154, 165, 111, 146, 165, 59, 157, 237, 228, 177, 176, 29, 234, 254, 228, 212, 154, 110, 199, 115, 173, 35, 46, 216, 88, 199, 4, 16, 139, 187, 92, 249, 55, 45, 167, 130, 11, 20, 31, 238, 17, 75, 249, 128, 105, 174, 255, 115, 2, 126, 30, 23, 79, 192, 182, 94, 216, 31, 12, 185, 135, 181, 175, 235, 11, 118, 243, 109, 251, 226, 234, 36, 188, 30, 213, 30, 196, 32, 227, 216, 137, 45, 7, 50, 191, 41, 242, 88, 170, 195, 101, 166, 162, 4, 101, 198, 252, 215, 15, 132, 12, 47, 153, 74, 4, 18, 254, 77, 121, 248, 162, 108, 230, 226, 102, 221, 10, 85, 96, 219, 64, 60, 70, 161, 51, 100, 14, 128, 219, 204, 193, 239, 213, 89, 51, 244, 210, 169, 43, 21, 189, 237, 25, 245, 128, 253, 148, 105, 204, 50, 164, 122, 71, 130, 195, 104, 154, 99, 219, 252, 71, 240, 66, 237, 119, 27, 241, 209, 98, 101, 254, 28, 109, 103, 115, 66, 39, 97, 82, 5, 130, 32, 197, 17, 143, 86, 255, 59, 205, 157, 221, 142, 102, 104, 87, 212, 163, 246, 174, 29, 36, 116, 205, 180, 207, 90, 59, 240, 238, 224, 229, 132, 128, 222, 163, 107, 201, 80, 25, 21, 4, 246, 49, 52, 52, 241, 226, 62, 200, 60, 188, 148, 112, 76, 94, 171, 65, 169, 138, 232, 58, 130, 150, 86, 2, 200, 6, 16, 191, 0, 42, 104, 23, 90, 112, 240, 85, 57, 176, 8, 125, 26, 108, 197, 168, 216, 228, 182, 93, 40, 190, 12, 49, 153, 41, 195, 144, 230, 160, 146, 125, 80, 46, 239, 21, 41, 130, 223, 198, 35, 27, 48, 98, 246, 140, 211, 220, 133, 36, 175, 98, 45, 243, 21, 136, 232, 139, 60, 48, 31, 222, 208, 194, 137, 188, 209, 169, 55, 16, 189, 86, 80, 167, 187, 233, 234, 12, 53, 125, 115, 192, 246, 28, 235, 144, 209, 26, 20, 197, 104, 151, 246, 193, 139, 91, 86, 69, 232, 73, 197, 201, 187, 215, 252, 110, 97, 100, 40, 53, 80, 114, 209, 200, 171, 7, 125, 196, 117, 105, 140, 177, 230, 104, 215, 170, 208, 100, 21, 34, 63, 89, 1, 162, 11, 222, 97, 134, 198, 10, 100, 91, 175, 81, 175, 170, 61, 122, 198, 201, 81, 138, 98, 106, 155, 232, 0, 168, 108, 139, 209, 116, 166, 122, 248, 152, 114, 23, 226, 62, 35, 97, 20, 243, 242, 122, 212, 182, 10, 218, 54, 194, 251, 214, 5, 91, 161, 131, 199, 83, 245, 224, 213, 20, 248, 92, 113, 218, 211, 86, 248, 181, 82, 23, 18, 62, 195, 57, 232, 232, 205, 119, 193, 104, 56, 58, 50, 87, 2, 178, 29, 125, 205, 36, 178, 197, 54, 0, 93, 209, 215, 136, 30, 195, 240, 174, 181, 237, 95, 180, 61, 176, 21, 56, 113, 159, 15, 155, 179, 103, 38, 221, 228, 11, 177, 70, 22, 159, 141, 21, 200, 254, 248, 23, 175, 69, 95, 162, 177, 199, 6, 83, 173, 188, 115, 83, 111, 191, 4, 28, 164, 227, 240, 156, 131, 84, 185, 176, 54, 241, 77, 149, 220, 81, 67, 251, 145, 96, 82, 105, 78, 250, 31, 75, 179, 77, 31, 58, 102, 163, 62, 198, 36, 212, 168, 218, 75, 249, 75, 46, 24, 10, 150, 95, 127, 186, 164, 150, 55, 205, 111, 14, 234, 47, 135, 166, 223, 87, 116, 171, 32, 116, 126, 63, 3, 189, 127, 223, 166, 152, 44, 50, 103, 104, 145, 149, 233, 95, 202, 179, 185, 31, 125, 142, 156, 222, 230, 3, 50, 12, 115, 93, 158, 126, 246, 118, 35, 113, 187, 235, 235, 55, 114, 30, 139, 120, 160, 129, 230, 197, 106, 152, 185, 180, 168, 58, 80, 174, 33, 159, 236, 173, 249, 219, 70, 43, 64, 25, 57, 74, 230, 40, 126, 166, 51, 216, 212, 38, 67, 68, 105, 249, 68, 232, 180, 17, 63, 202, 85, 190, 19, 159, 171, 33, 20, 78, 147, 33, 34, 23, 210, 113, 9, 188, 120, 47, 243, 56, 36, 201, 239, 182, 8, 160, 102, 199, 243, 113, 132, 249, 252, 171, 7, 115, 111, 217, 120, 46, 152, 249, 240, 95, 240, 55, 204, 246, 60, 31, 114, 104, 164, 38, 195, 164, 239, 11, 141, 20, 174, 188, 232, 185, 178, 17, 53, 50, 10, 117, 66, 15, 7, 200, 59, 230, 33, 73, 237, 253, 136, 239, 119, 172, 23, 193, 191, 70, 50, 165, 70, 17, 78, 1, 193, 14, 244, 115, 97, 255, 205, 112, 116, 111, 184, 44, 232, 186, 30, 185, 189, 100, 190, 161, 126, 218, 140, 33, 194, 225, 154, 65, 12, 82, 100, 197, 87, 231, 107, 124, 156, 5, 199, 233, 71, 229, 96, 105, 124, 145, 217, 135, 111, 189, 1, 244, 176, 161, 214, 190, 64, 252, 199, 96, 123, 165, 92, 81, 145, 122, 61, 9, 219, 245, 166, 233, 231, 18, 113, 173, 10, 218, 201, 73, 194, 71, 85, 197, 250, 8, 151, 189, 77, 2, 40, 207, 111, 9, 192, 173, 232, 16, 241, 84, 77, 222, 228, 250, 239, 166, 232, 63, 96, 199, 21, 144, 43, 105, 69, 230, 128, 192, 188, 27, 105, 74, 134, 162, 118, 172, 32, 188, 96, 97, 20, 103, 125, 29, 147, 44, 31, 80, 171, 72, 35, 1, 109, 183, 49, 190, 72, 49, 186, 108, 40, 67, 163, 34, 230, 9, 153, 68, 87, 150, 144, 104, 94, 120, 29, 29, 167, 24, 54, 114, 69, 183, 83, 4, 215, 77, 109, 204, 205, 57, 82, 24, 196, 128, 134, 135, 35, 47, 6, 203, 246, 82, 197, 160, 34, 136, 252, 205, 173, 165, 236, 156, 66, 248, 188, 52, 215, 63, 180, 198, 204, 182, 74, 171, 192, 65, 24, 141, 167, 209, 224, 159, 44, 234, 114, 76, 186, 28, 149, 124, 174, 124, 3, 32, 2, 10, 51, 86, 27, 21, 39, 67, 134, 81, 190, 76, 67, 139, 162, 63, 232, 3, 91, 10, 201, 234, 143, 193, 91, 54, 94, 24, 58, 168, 157, 188, 85, 207, 103, 204, 195, 75, 51, 98, 87, 19, 53, 105, 90, 222, 199, 7, 90, 117, 210, 68, 223, 253, 12, 212, 116, 103, 243, 162, 121, 22, 34, 13, 195, 46, 154, 101, 43, 103, 173, 63, 111, 75, 103, 149, 237, 93, 45, 221, 74, 212, 233, 54, 168, 187, 231, 100, 13, 201, 250, 75, 237, 55, 58, 11, 113, 12, 65, 70, 254, 86, 163, 189, 222, 207, 118, 130, 170, 114, 166, 218, 255, 42, 158, 156, 97, 156, 242, 167, 205, 233, 41, 15, 111, 53, 227, 175, 218, 136, 89, 138, 197, 251, 226, 216, 0, 191, 162, 196, 108, 75, 203, 153, 240, 244, 239, 117, 233, 206, 180, 110, 66, 13, 241, 217, 32, 111, 218, 17, 219, 146, 127, 202, 120, 63, 97, 71, 237, 127, 109, 184, 174, 53, 125, 148, 2, 3, 228, 9, 32, 147, 148, 42, 181, 43, 27, 228, 41, 48, 44, 233, 193, 87, 104, 95, 164, 87, 218, 11, 204, 55, 227, 97, 43, 224, 95, 147, 212, 4, 210, 203, 3, 89, 199, 11, 13, 56, 210, 61, 121, 140, 251, 107, 207, 59, 143, 84, 121, 59, 117, 90, 54, 86, 55, 149, 73, 159, 99, 19, 95, 157, 124, 221, 103, 23, 35, 75, 158, 232, 141, 16, 162, 215, 46, 6, 52, 123, 17, 35, 49, 216, 232, 26, 100, 28, 192, 84, 253, 174, 227, 130, 64, 69, 244, 99, 152, 194, 233, 233, 67, 242, 46, 189, 5, 215, 254, 58, 218, 235, 13, 54, 240, 61, 19, 27, 248, 209, 150, 206, 199, 88, 18, 187, 181, 169, 169, 54, 40, 8, 109, 144, 147, 19, 220, 152, 235, 21, 79, 178, 87, 34, 109, 65, 205, 181, 42, 178, 66, 102, 20, 154, 189, 159, 157, 196, 216, 161, 216, 115, 90, 48, 242, 54, 91, 173, 213, 138, 214, 116, 239, 59, 127, 249, 120, 220, 149, 172, 204, 76, 172, 64, 236, 218, 231, 119, 221, 221, 61, 129, 158, 28, 195, 199, 115, 150, 178, 93, 208, 28, 150, 93, 37, 139, 129, 144, 14, 238, 162, 95, 229, 106, 222, 167, 77, 228, 155, 29, 178, 52, 69, 72, 31, 44, 246, 127, 101, 3, 153, 169, 26, 162, 138, 83, 235, 167, 227, 166, 22, 189, 16, 128, 244, 9, 120, 53, 36, 24, 10, 15, 126, 82, 57, 191, 23, 244, 165, 19, 164, 57, 244, 119, 186, 195, 121, 179, 187, 91, 128, 91, 1, 10, 149, 25, 243, 26, 174, 119, 232, 143, 153, 23, 173, 92, 247, 192, 203, 89, 184, 112, 209, 200, 43, 198, 226, 59, 145, 26, 51, 232, 163, 145, 31, 208, 59, 156, 153, 188, 105, 86, 112, 202, 88, 176, 239, 126, 135, 24, 179, 57, 137, 235, 192, 60, 129, 190, 46, 222, 182, 46, 119, 192, 21, 150, 242, 208, 217, 125, 111, 7, 114, 175, 110, 210, 206, 3, 190, 85, 126, 207, 224, 221, 235, 125, 161, 46, 155, 172, 203, 230, 186, 157, 159, 238, 244, 244, 220, 71, 233, 34, 247, 195, 107, 160, 128, 253, 169, 164, 19, 209, 25, 41, 80, 39, 243, 163, 83, 90, 84, 91, 205, 212, 4, 103, 155, 158, 159, 195, 206, 81, 109, 152, 208, 57, 17, 23, 224, 248, 44, 10, 75, 39, 205, 14, 157, 90, 108, 210, 18, 254, 202, 104, 184, 198, 54, 208, 63, 206, 105, 99, 116, 157, 74, 82, 239, 125, 28, 12, 43, 23, 36, 52, 73, 178, 249, 164, 114, 53, 236, 148, 175, 79, 142, 210, 93, 25, 125, 83, 116, 109, 99, 248, 130, 214, 23, 205, 101, 249, 20, 36, 202, 194, 23, 151, 45, 119, 240, 12, 208, 173, 200, 59, 216, 77, 47, 10, 23, 181, 72, 255, 67, 176, 215, 24, 249, 82, 59, 241, 157, 84, 34, 62, 239, 215, 190, 129, 44, 161, 48, 74, 72, 107, 179, 20, 111, 11, 135, 197, 146, 249, 226, 237, 18, 13, 92, 242, 246, 23, 76, 8, 161, 198, 166, 141, 137, 226, 57, 236, 232, 128, 143, 147, 48, 24, 189, 70, 100, 115, 238, 194, 153, 95, 128, 74, 82, 182, 85, 175, 56, 194, 212, 130, 236, 145, 19, 115, 222, 58, 164, 155, 104, 84, 168, 229, 128, 72, 17, 53, 0, 17, 125, 77, 82, 125, 113, 206, 155, 31, 181, 54, 231, 47, 33, 34, 109, 145, 165, 139, 74, 43, 36, 157, 120, 169, 152, 128, 24, 173, 178, 209, 90, 76, 83, 225, 40, 33, 102, 96, 204, 187, 110, 75, 127, 121, 201, 173, 254, 232, 175, 150, 10, 156, 172, 181, 12, 220, 251, 70, 140, 206, 159, 131, 18, 29, 183, 130, 41, 121, 13, 51, 91, 132, 84, 192, 213, 182, 251, 239, 86, 165, 214, 206, 45, 17, 219, 239, 19, 137, 166, 86, 136, 94, 217, 156, 222, 47, 15, 62, 151, 249, 125, 32, 183, 126, 226, 225, 71, 28, 131, 135, 88, 65, 26, 137, 220, 229, 167, 234, 189, 88, 168, 175, 126, 25, 32, 252, 174, 123, 155, 19, 184, 26, 217, 194, 222, 201, 186, 240, 117, 60, 24, 97, 147, 56, 241, 39, 216, 12, 171, 174, 27, 162, 58, 59, 49, 155, 176, 255, 66, 77, 191, 173, 145, 29, 129, 52, 36, 172, 218, 237, 203, 16, 11, 7, 31, 205, 49, 64, 203, 29, 21, 211, 209, 142, 178, 219, 71, 75, 139, 200, 39, 233, 2, 156, 80, 162, 212, 108, 35, 138, 86, 156, 175, 202, 125, 190, 122, 70, 178, 117, 132, 136, 191, 178, 40, 234, 107, 59, 160, 65, 160, 247, 218, 42, 190, 55, 4, 253, 127, 66, 176, 222, 202, 89, 178, 103, 166, 243, 17, 21, 245, 42, 89, 108, 201, 47, 178, 174, 174, 151, 242, 10, 75, 135, 156, 119, 99, 243, 107, 87, 231, 43, 169, 232, 35, 216, 66, 77, 93, 86, 176, 140, 232, 199, 27, 22, 75, 192, 39, 180, 100, 59, 124, 60, 146, 241, 231, 98, 10, 199, 135, 228, 99, 75, 72, 229, 61, 123, 33, 79, 39, 228, 139, 74, 219, 27, 128, 149, 11, 96, 248, 176, 54, 239, 36, 223, 34, 137, 249, 221, 255, 13, 198, 236, 43, 108, 180, 189, 128, 180, 190, 92, 203, 247, 194, 115, 207, 88, 211, 156, 78, 177, 80, 82, 146, 213, 249, 171, 135, 125, 146, 78, 160, 163, 134, 205, 120, 49, 92, 50, 111, 125, 229, 155, 213, 91, 159, 62, 151, 11, 128, 124, 10, 147, 75, 84, 229, 46, 141, 94, 140, 119, 198, 62, 2, 102, 166, 164, 216, 52, 108, 40, 129, 27, 214, 28, 77, 87, 242, 159, 226, 181, 213, 138, 86, 69, 82, 237, 92, 103, 175, 87, 105, 55, 192, 44, 68, 21, 116, 81, 127, 167, 110, 22, 167, 132, 154, 221, 35, 176, 45, 72, 123, 29, 54, 126, 251, 214, 237, 18, 238, 160, 228, 145, 54, 137, 27, 67, 52, 69, 60, 37, 203, 191, 192, 238, 232, 128, 158, 65, 236, 182, 206, 183, 113, 125, 69, 123, 156, 14, 213, 45, 106, 214, 161, 63, 116, 47, 154, 146, 230, 243, 136, 104, 222, 162, 215, 109, 42, 87, 152, 202, 68, 89, 97, 237, 194, 61, 47, 29, 190, 111, 227, 14, 64, 140, 172, 4, 25, 68, 211, 99, 66, 109, 221, 176, 91, 137, 25, 136, 80, 63, 44, 105, 249, 65, 35, 155, 182, 16, 90, 253, 151, 162, 53, 229, 110, 36, 104, 85, 52, 248, 215, 182, 128, 182, 148, 33, 176, 118, 226, 219, 238, 20, 71, 50, 106, 117, 147, 130, 231, 49, 56, 227, 18, 69, 11, 20, 162, 34, 247, 104, 53, 127, 18, 75, 66, 128, 210, 87, 14, 15, 81, 63, 112, 202, 113, 2, 8, 41, 3, 120, 10, 190, 79, 9, 207, 122, 192, 191, 15, 209, 209, 117, 93, 231, 33, 97, 25, 152, 148, 114, 233, 195, 218, 175, 218, 150, 9, 136, 43, 75, 21, 6, 242, 163, 131, 124, 58, 60, 90, 205, 147, 163, 253, 42, 28, 64, 175, 62, 30, 235, 36, 240, 74, 196, 78, 149, 189, 90, 63, 24, 61, 32, 1, 25, 93, 208, 57, 156, 32, 217, 193, 229, 65, 103, 206, 153, 121, 255, 160, 22, 235, 205, 170, 68, 220, 212, 56, 203, 141, 115, 68, 192, 167, 24, 42, 96, 10, 156, 223, 224, 110, 233, 38, 86, 179, 157, 50, 164, 14, 181, 98, 29, 24, 222, 37, 204, 83, 253, 35, 7, 111, 250, 1, 19, 182, 191, 240, 217, 215, 118, 116, 223, 94, 109, 54, 28, 244, 251, 243, 25, 95, 105, 20, 177, 65, 156, 185, 49, 132, 191, 177, 169, 86, 121, 20, 119, 176, 24, 7, 35, 0, 200, 35, 7, 113, 70, 118, 146, 8, 102, 210, 140, 30, 23, 212, 133, 180, 50, 24, 177, 220, 200, 29, 22, 152, 179, 200, 170, 151, 18, 48, 106, 12, 99, 127, 51, 35, 123, 29, 252, 26, 150, 104, 104, 213, 4, 214, 115, 194, 155, 90, 155, 40, 232, 203, 195, 5, 114, 177, 232, 99, 36, 11, 107, 117, 123, 97, 106, 24, 212, 246, 16, 7, 159, 70, 27, 130, 31, 228, 62, 57, 151, 103, 29, 195, 2, 177, 228, 133, 13, 198, 36, 70, 18, 194, 157, 40, 28, 166, 247, 173, 249, 244, 183, 40, 156, 144, 50, 3, 68, 138, 226, 42, 193, 156, 88, 211, 26, 125, 160, 164, 65, 61, 16, 20, 106, 157, 182, 131, 142, 70, 244, 208, 95, 72, 117, 156, 39, 236, 28, 201, 47, 121, 50, 103, 9, 190, 234, 205, 43, 94, 61, 192, 147, 122, 248, 217, 3, 110, 216, 45, 136, 248, 174, 104, 155, 23, 235, 206, 118, 104, 199, 183, 188, 27, 149, 243, 62, 192, 140, 159, 189, 30, 115, 196, 0, 90, 69, 254, 228, 237, 246, 55, 64, 88, 119, 131, 134, 181, 140, 145, 230, 5, 129, 20, 45, 246, 34, 35, 195, 131, 151, 206, 7, 228, 11, 127, 196, 54, 45, 189, 219, 135, 202, 140, 33, 67, 205, 176, 247, 23, 83, 13, 60, 74, 234, 100, 193, 38, 204, 131, 134, 41, 5, 244, 101, 85, 177, 87, 243, 117, 14, 117, 196, 9, 229, 225, 199, 200, 92, 80, 179, 213, 60, 91, 57, 241, 203, 133, 151, 143, 231, 30, 162, 1, 179, 14, 11, 172, 57, 143, 146, 199, 251, 31, 12, 246, 14, 55, 162, 167, 148, 254, 161, 86, 62, 47, 155, 164, 37, 160, 105, 106, 253, 192, 200, 57, 14, 125, 223, 185, 190, 235, 164, 179, 69, 212, 9, 137, 82, 254, 4, 115, 177, 35, 177, 94, 155, 148, 33, 135, 12, 64, 175, 249, 151, 0, 60, 184, 102, 207, 162, 188, 156, 185, 198, 94, 99, 1, 189, 40, 179, 152, 105, 51, 187, 6, 172, 39, 184, 163, 166, 251, 243, 231, 20, 191, 77, 166, 236, 224, 110, 234, 115, 28, 55, 164, 218, 191, 138, 119, 81, 67, 37, 128, 67, 144, 103, 58, 16, 220, 61, 19, 95, 188, 28, 199, 112, 241, 185, 232, 193, 83, 110, 228, 158, 215, 0, 252, 62, 127, 193, 246, 196, 213, 164, 22, 23, 211, 56, 29, 37, 148, 59, 219, 87, 108, 74, 57, 212, 187, 243, 65, 57, 7, 254, 239, 46, 189, 94, 227, 220, 116, 173, 250, 162, 227, 181, 21, 1, 167, 37, 109, 166, 200, 199, 160, 156, 254, 74, 77, 62, 15, 109, 46, 9, 64, 173, 151, 117, 255, 209, 53, 80, 208, 253, 193, 248, 123, 88, 159, 208, 207, 111, 174, 223, 183, 148, 117, 31, 252, 69, 85, 130, 112, 23, 28, 12, 240, 244, 24, 237, 31, 154, 98, 83, 215, 98, 51, 35, 119, 109, 188, 247, 96, 56, 137, 132, 36, 241, 243, 60, 194, 101, 162, 111, 151, 165, 48, 241, 29, 16, 193, 34, 176, 93, 132, 25, 49, 113, 141, 88, 234, 192, 157, 154, 67, 253, 117, 166, 36, 165, 188, 105, 100, 176, 164, 202, 159, 81, 99, 48, 5, 223, 31, 154, 27, 4, 147, 185, 225, 51, 149, 71, 224, 56, 38, 34, 141, 12, 139, 209, 58, 170, 147, 27, 40, 55, 10, 58, 2, 76, 18, 197, 213, 242, 145, 197, 104, 180, 128, 91, 46, 165, 165, 8, 209, 2, 209, 87, 212, 69, 231, 223, 174, 245, 106, 161, 125, 13, 121, 12, 182, 248, 10, 71, 146, 224, 229, 103, 166, 75, 206, 84, 43, 40, 162, 19, 225, 219, 216, 54, 72, 190, 196, 89, 144, 90, 49, 68, 51, 113, 223, 196, 229, 169, 231, 70, 53, 203, 43, 158, 201, 183, 230, 5, 150, 240, 141, 142, 181, 170, 30, 147, 68, 26, 213, 128, 213, 108, 181, 74, 197, 43, 234, 29, 45, 86, 204, 224, 246, 147, 85, 69, 254, 173, 245, 15, 224, 176, 236, 78, 75, 123, 19, 110, 161, 172, 154, 59, 247, 133, 103, 222, 86, 224, 108, 51, 37, 96, 40, 66, 57, 28, 189, 72, 11, 36, 17, 238, 172, 118, 74, 89, 67, 136, 77, 239, 142, 101, 40, 78, 159, 117, 35, 168, 129, 74, 72, 46, 68, 192, 17, 63, 232, 211, 128, 87, 1, 223, 166, 152, 234, 210, 122, 246, 128, 138, 212, 56, 222, 192, 96, 163, 134, 79, 27, 49, 36, 107, 40, 246, 109, 182, 254, 227, 95, 94, 228, 154, 25, 152, 119, 53, 6, 160, 50, 247, 98, 168, 227, 190, 100, 41, 143, 204, 169, 197, 178, 66, 21, 95, 111, 215, 179, 102, 178, 85, 205, 28, 57, 50, 118, 175, 17, 188, 5, 62, 91, 21, 0, 2, 243, 247, 37, 184, 96, 108, 6, 240, 245, 36, 112, 131, 130, 246, 18, 161, 136, 220, 200, 111, 41, 196, 228, 128, 155, 248, 73, 133, 249, 181, 115, 98, 170, 84, 1, 83, 141, 243, 247, 23, 62, 250, 72, 144, 203, 130, 236, 76, 139, 126, 204, 177, 232, 133, 126, 188, 68, 244, 182, 197, 169, 66, 163, 6, 160, 17, 100, 81, 159, 47, 185, 29, 240, 223, 248, 186, 207, 103, 65, 79, 217, 124, 174, 212, 59, 89, 173, 161, 191, 20, 129, 18, 214, 74, 181, 135, 191, 173, 208, 84, 166, 66, 11, 4, 13, 234, 200, 108, 202, 83, 62, 36, 40, 242, 102, 51, 17, 190, 120, 166, 214, 81, 88, 54, 34, 0, 125, 182, 34, 54, 92, 190, 105, 135, 103, 229, 166, 24, 213, 234, 168, 80, 254, 191, 217, 247, 75, 175, 58, 31, 11, 174, 252, 12, 56, 77, 1, 0, 94, 76, 21, 112, 18, 151, 91, 148, 179, 8, 25, 125, 12, 199, 119, 153, 55, 17, 167, 127, 19, 46, 9, 28, 204, 32, 174, 154, 137, 140, 77, 150, 140, 105, 251, 127, 119, 80, 109, 253, 190, 19, 128, 5, 192, 70, 151, 171, 33, 144, 64, 191, 185, 216, 237, 11, 118, 7, 194, 240, 25, 232, 174, 149, 239, 153, 215, 74, 164, 125, 14, 213, 104, 99, 101, 0, 194, 90, 66, 27, 120, 60, 2, 134, 91, 87, 182, 73, 251, 60, 193, 66, 130, 140, 247, 40, 191, 220, 193, 222, 231, 74, 89, 207, 210, 230, 106, 32, 40, 221, 168, 205, 10, 249, 193, 233, 164, 180, 170, 251, 144, 81, 156, 158, 11, 43, 32, 243, 189, 3, 118, 121, 125, 100, 250, 50, 76, 226, 72, 206, 182, 183, 218, 210, 204, 225, 176, 148, 237, 175, 232, 49, 33, 214, 72, 203, 108, 47, 93, 230, 254, 1, 172, 179, 98, 143, 108, 93, 234, 32, 148, 38, 5, 1, 117, 95, 236, 163, 119, 160, 105, 240, 60, 13, 84, 24, 254, 184, 208, 82, 140, 2, 196, 140, 208, 136, 50, 195, 24, 37, 155, 59, 194, 211, 82, 92, 251, 31, 110, 127, 98, 212, 67, 151, 75, 98, 4, 123, 210, 52, 170, 138, 186, 170, 157, 81, 86, 12, 72, 129, 75, 195, 222, 150, 227, 21, 231, 231, 250, 41, 151, 160, 75, 191, 205, 77, 88, 162, 26, 156, 90, 221, 162, 143, 253, 153, 127, 206, 101, 19, 52, 128, 104, 254, 246, 168, 133, 192, 25, 158, 116, 197, 248, 155, 80, 103, 233, 153, 181, 209, 191, 228, 251, 112, 168, 183, 10, 93, 127, 90, 35, 140, 51, 0, 218, 175, 71, 70, 192, 212, 72, 145, 50, 127, 81, 139, 216, 211, 246, 230, 104, 34, 178, 240, 251, 115, 205, 123, 161, 223, 130, 246, 56, 47, 210, 217, 151, 224, 40, 32, 87, 117, 4, 51, 164, 226, 69, 165, 9, 241, 96, 187, 98, 184, 189, 61, 127, 86, 80, 4, 194, 111, 222, 38, 90, 192, 207, 195, 243, 176, 153, 0, 138, 61, 226, 139, 118, 130, 150, 155, 18, 236, 143, 89, 25, 179, 237, 43, 185, 239, 198, 34, 63, 73, 121, 130, 251, 252, 63, 78, 141, 244, 102, 19, 160, 88, 230, 107, 65, 58, 174, 61, 150, 161, 52, 138, 45, 90, 43, 116, 106, 114, 125, 228, 230, 251, 43, 14, 141, 238, 90, 213, 2, 144, 77, 65, 117, 94, 41, 69, 155, 72, 126, 28, 225, 200, 27, 61, 12, 166, 154, 11, 178, 153, 104, 131, 58, 248, 23, 215, 196, 152, 241, 5, 72, 32, 209, 109, 46, 218, 202, 188, 14, 11, 174, 167, 32, 42, 67, 55, 56, 113, 184, 105, 240, 233, 78, 130, 246, 89, 242, 147, 117, 50, 247, 44, 115, 78, 27, 229, 188, 247, 130, 89, 197, 210, 123, 105, 204, 28, 214, 224, 177, 213, 150, 96, 160, 38, 119, 169, 118, 165, 2, 93, 183, 127, 126, 33, 49, 129, 148, 168, 184, 51, 218, 20, 252, 156, 134, 111, 57, 50, 1, 112, 248, 246, 222, 38, 52, 248, 153, 69, 47, 94, 149, 83, 84, 107, 56, 251, 126, 47, 76, 12, 247, 30, 130, 196, 37, 43, 134, 196, 217, 254, 249, 72, 81, 246, 26, 53, 184, 105, 186, 125, 135, 91, 182, 18, 52, 94, 127, 122, 237, 110, 190, 192, 3, 67, 216, 101, 190, 105, 49, 67, 223, 3, 160, 64, 126, 124, 95, 225, 32, 68, 147, 70, 227, 55, 138, 47, 138, 106, 182, 187, 220, 11, 116, 67, 29, 109, 53, 42, 177, 125, 90, 87, 45, 46, 218, 60, 15, 255, 97, 1, 82, 70, 83, 47, 157, 123, 148, 38, 250, 54, 235, 192, 251, 0, 55, 209, 182, 33, 21, 171, 10, 208, 122, 121, 251, 171, 249, 206, 26, 62, 215, 66, 26, 224, 221, 155, 60, 0, 210, 187, 94, 75, 121, 193, 255, 14, 206, 4, 157, 57, 206, 219, 13, 76, 68, 171, 17, 49, 142, 28, 143, 115, 154, 136, 220, 199, 178, 73, 152, 87, 220, 95, 80, 132, 230, 27, 99, 86, 93, 135, 62, 49, 203, 77, 125, 17, 85, 50, 136, 237, 85, 161, 131, 53, 170, 48, 165, 79, 100, 146, 230, 30, 236, 30, 81, 198, 186, 29, 182, 164, 4, 36, 241, 229, 21, 148, 115, 251, 37, 70, 186, 171, 39, 187, 26, 96, 84, 126, 4, 84, 217, 254, 112, 150, 216, 244, 172, 198, 146, 189, 235, 244, 176, 242, 175, 75, 113, 106, 212, 193, 105, 35, 95, 204, 155, 92, 193, 145, 211, 54, 213, 205, 11, 253, 23, 239, 196, 224, 120, 74, 53, 114, 175, 65, 92, 140, 82, 10, 90, 158, 230, 2, 95, 214, 125, 41, 116, 30, 127, 67, 205, 47, 232, 79, 61, 67, 252, 217, 33, 182, 76, 222, 20, 197, 28, 132, 80, 216, 45, 115, 238, 61, 36, 141, 94, 78, 176, 119, 192, 91, 77, 161, 170, 171, 237, 33, 88, 70, 197, 252, 226, 117, 200, 181, 80, 83, 148, 23, 160, 162, 31, 171, 51, 145, 224, 247, 24, 242, 176, 96, 192, 4, 218, 92, 72, 212, 167, 58, 186, 100, 182, 156, 106, 60, 114, 95, 98, 132, 221, 59, 78, 238, 153, 45, 115, 175, 147, 251, 114, 86, 11, 101, 85, 9, 191, 169, 24, 234, 85, 153, 123, 18, 227, 148, 202, 96, 169, 39, 200, 137, 2, 206, 245, 119, 121, 17, 159, 153, 239, 159, 19, 205, 23, 34, 198, 28, 247, 5, 221, 209, 243, 12, 190, 27, 38, 177, 214, 204, 23, 95, 224, 46, 101, 0, 48, 52, 31, 184, 13, 111, 240, 122, 38, 10, 206, 155, 33, 59, 59, 65, 130, 9, 40, 150, 245, 162, 82, 53, 2, 147, 96, 132, 105, 174, 112, 223, 189, 175, 248, 162, 112, 10, 186, 121, 229, 45, 140, 113, 195, 99, 17, 53, 215, 166, 224, 83, 62, 43, 229, 96, 185, 175, 151, 24, 9, 227, 129, 100, 31, 182, 98, 27, 39, 13, 228, 69, 173, 57, 148, 22, 228, 113, 200, 113, 82, 152, 211, 219, 230, 9, 54, 148, 156, 227, 240, 185, 23, 231, 151, 122, 37, 207, 178, 174, 240, 240, 108, 137, 136, 89, 154, 26, 19, 182, 243, 82, 45, 153, 39, 8, 17, 246, 233, 219, 143, 114, 82, 37, 216, 102, 191, 207, 82, 1, 186, 54, 11, 172, 71, 177, 186, 73, 152, 33, 45, 26, 1, 162, 22, 8, 160, 195, 24, 13, 127, 183, 48, 24, 219, 166, 64, 221, 199, 148, 246, 238, 18, 88, 205, 46, 74, 68, 145, 74, 130, 118, 223, 75, 199, 43, 45, 162, 183, 184, 53, 190, 67, 51, 50, 178, 51, 194, 174, 241, 194, 109, 188, 107, 132, 171, 255, 120, 237, 5, 170, 20, 48, 0, 9, 82, 28, 218, 47, 224, 148, 245, 155, 115, 159, 53, 174, 18, 72, 207, 40, 254, 2, 21, 92, 10, 247, 234, 204, 103, 130, 18, 111, 252, 137, 217, 126, 92, 155, 99, 118, 83, 152, 167, 47, 155, 217, 181, 120, 93, 205, 97, 113, 47, 129, 191, 205, 58, 74, 58, 207, 226, 234, 69, 231, 33, 232, 252, 184, 242, 208, 191, 189, 166, 49, 220, 168, 192, 57, 97, 177, 184, 59, 172, 253, 6, 201, 169, 245, 123, 81, 197, 188, 52, 144, 35, 9, 166, 36, 245, 168, 145, 58, 140, 30, 19, 133, 206, 66, 4, 106, 84, 89, 96, 99, 233, 19, 66, 125, 234, 150, 69, 167, 141, 151, 163, 122, 205, 156, 61, 44, 195, 56, 51, 187, 252, 240, 129, 134, 87, 14, 22, 36, 3, 53, 127, 167, 205, 214, 13, 209, 73, 51, 4, 61, 230, 140, 131, 136, 221, 169, 233, 141, 69, 175, 214, 220, 239, 190, 11, 42, 39, 157, 104, 200, 226, 46, 38, 142, 166, 88, 107, 227, 75, 82, 163, 92, 110, 201, 129, 170, 13, 249, 212, 205, 125, 66, 88, 117, 208, 97, 245, 39, 120, 111, 112, 153, 110, 97, 21, 113, 162, 248, 235, 209, 246, 216, 78, 177, 159, 252, 158, 194, 202, 4, 131, 178, 40, 22, 20, 110, 153, 32, 119, 69, 94, 56, 59, 196, 38, 178, 118, 19, 148, 35, 73, 164, 192, 104, 157, 116, 187, 40, 80, 121, 51, 250, 97, 219, 57, 76, 27, 187, 106, 135, 217, 152, 66, 71, 170, 227, 199, 9, 243, 111, 130, 50, 105, 152, 170, 16, 95, 58, 191, 199, 206, 91, 155, 153, 176, 138, 118, 62, 133, 124, 153, 170, 250, 248, 212, 144, 150, 228, 147, 66, 90, 116, 145, 246, 115, 204, 228, 110, 134, 237, 160, 61, 105, 53, 239, 133, 60, 4, 53, 175, 84, 125, 95, 254, 230, 238, 0, 218, 160, 79, 93, 105, 169, 250, 156, 8, 22, 241, 61, 164, 78, 187, 222, 61, 63, 188, 184, 162, 50, 47, 156, 178, 62, 224, 41, 122, 196, 194, 67, 189, 148, 93, 231, 6, 130, 78, 253, 159, 65, 88, 122, 0, 0, 43, 136, 21, 86, 237, 192, 197, 146, 35, 87, 180, 52, 240, 112, 180, 188, 184, 131, 178, 224, 177, 252, 92, 47, 163, 242, 11, 28, 146, 219, 55, 165, 233, 208, 123, 49, 63, 245, 51, 92, 208, 161, 222, 7, 112, 76, 29, 121, 102, 212, 44, 147, 12, 151, 75, 219, 49, 33, 101, 69, 129, 15, 252, 27, 202, 74, 134, 46, 213, 36, 39, 236, 97, 75, 154, 119, 149, 114, 252, 91, 137, 9, 145, 97, 120, 211, 122, 122, 69, 21, 185, 112, 196, 40, 213, 52, 111, 236, 134, 54, 30, 58, 206, 18, 178, 73, 195, 50, 245, 40, 38, 101, 52, 81, 197, 167, 132, 64, 26, 192, 128, 227, 133, 79, 123, 116, 240, 17, 168, 11, 202, 250, 61, 132, 146, 94, 175, 107, 61, 155, 170, 76, 8, 187, 188, 122, 197, 204, 2, 190, 18, 227, 99, 88, 67, 253, 70, 243, 198, 11, 25, 219, 111, 108, 142, 113, 121, 173, 168, 204, 208, 207, 166, 238, 246, 3, 115, 66, 120, 156, 55, 224, 87, 5, 156, 243, 221, 11, 162, 240, 166, 28, 202, 26, 163, 120, 228, 190, 233, 131, 13, 120, 251, 129, 26, 102, 206, 46, 180, 46, 68, 193, 62, 72, 238, 49, 66, 179, 187, 144, 151, 125, 59, 239, 219, 9, 21, 153, 108, 40, 158, 145, 39, 109, 243, 16, 19, 75, 19, 178, 205, 240, 224, 61, 129, 112, 59, 64, 111, 171, 102, 172, 37, 134, 1, 48, 88, 178, 6, 101, 6, 104, 20, 151, 177, 138, 17, 253, 206, 182, 60, 208, 29, 171, 135, 161, 234, 183, 192, 243, 68, 87, 207, 167, 229, 229, 44, 89, 29, 124, 172, 154, 201, 56, 188, 209, 64, 113, 124, 195, 183, 165, 18, 109, 75, 199, 28, 252, 65, 51, 25, 113, 233, 214, 28, 84, 67, 38, 31, 142, 137, 124, 97, 15, 111, 224, 98, 115, 116, 83, 17, 93, 131, 51, 11, 135, 167, 84, 0, 0, 68, 177, 254, 186, 60, 14, 9, 18, 73, 54, 234, 131, 249, 153, 136, 102, 78, 34, 24, 59, 80, 215, 215, 126, 39, 104, 106, 132, 112, 142, 47, 208, 180, 228, 172, 171, 165, 22, 231, 147, 222, 72, 202, 191, 151, 52, 171, 77, 9, 144, 99, 40, 88, 180, 16, 112, 57, 52, 13, 197, 235, 137, 56, 102, 163, 232, 185, 67, 14, 205, 78, 154, 83, 129, 171, 198, 87, 183, 37, 170, 25, 33, 199, 176, 218, 159, 207, 195, 16, 160, 133, 43, 230, 31, 231, 167, 24, 27, 22, 42, 6, 187, 15, 135, 116, 7, 193, 135, 162, 86, 25, 131, 21, 10, 65, 253, 66, 161, 181, 211, 114, 201, 39, 158, 142, 23, 17, 242, 132, 178, 153, 169, 91, 179, 149, 93, 172, 70, 245, 130, 110, 112, 79, 142, 171, 230, 36, 224, 55, 96, 14, 11, 235, 141, 179, 160, 204, 132, 52, 222, 194, 20, 7, 2, 14, 147, 7, 157, 230, 212, 150, 218, 121, 16, 24, 163, 146, 119, 178, 254, 118, 252, 142, 19, 198, 64, 80, 11, 236, 208, 14, 237, 24, 144, 103, 252, 8, 28, 63, 153, 155, 4, 134, 215, 118, 242, 147, 208, 16, 206, 234, 28, 239, 248, 172, 63, 62, 254, 149, 53, 93, 36, 19, 254, 64, 244, 178, 144, 67, 92, 92, 56, 37, 26, 197, 159, 21, 17, 154, 21, 56, 6, 221, 108, 196, 119, 43, 54, 21, 214, 33, 82, 148, 163, 11, 224, 130, 115, 176, 227, 3, 173, 83, 227, 51, 255, 77, 3, 214, 207, 234, 221, 219, 36, 251, 174, 40, 23, 227, 14, 8, 16, 55, 133, 172, 82, 3, 25, 233, 110, 90, 17, 210, 232, 145, 3, 243, 252, 169, 131, 129, 195, 162, 59, 146, 91, 10, 28, 22, 238, 15, 55, 16, 91, 136, 21, 241, 100, 121, 191, 169, 108, 38, 189, 124, 128, 174, 121, 9, 50, 25, 225, 215, 26, 228, 206, 116, 62, 149, 110, 45, 62, 3, 199, 217, 227, 232, 136, 101, 91, 249, 150, 35, 161, 200, 120, 194, 248, 93, 130, 220, 148, 170, 174, 83, 213, 145, 220, 44, 238, 27, 148, 118, 79, 61, 75, 198, 147, 140, 207, 236, 198, 248, 107, 117, 200, 202, 95, 55, 98, 124, 224, 37, 222, 6, 6, 189, 47, 46, 154, 103, 58, 103, 70, 22, 250, 123, 178, 148, 116, 154, 80, 82, 117, 223, 52, 60, 235, 159, 249, 39, 106, 106, 137, 250, 30, 59, 154, 10, 165, 77, 210, 104, 119, 186, 83, 144, 174, 130, 152, 32, 24, 239, 83, 190, 73, 34, 210, 2, 40, 92, 211, 244, 29, 94, 153, 52, 243, 208, 72, 87, 34, 48, 213, 65, 68, 249, 91, 253, 125, 209, 91, 88, 156, 61, 142, 227, 241, 93, 28, 130, 138, 118, 239, 138, 100, 163, 147, 190, 208, 88, 109, 204, 50, 177, 24, 21, 34, 42, 255, 151, 221, 88, 241, 36, 0, 135, 84, 131, 57, 231, 158, 48, 199, 8, 92, 74, 133, 171, 159, 30, 163, 87, 218, 80, 2, 218, 162, 22, 70, 242, 19, 224, 72, 172, 82, 42, 235, 178, 217, 236, 56, 55, 7, 34, 140, 137, 28, 115, 208, 235, 248, 51, 142, 183, 108, 188, 171, 11, 41, 0, 120, 161, 111, 209, 177, 173, 233, 28, 172, 58, 98, 8, 230, 55, 62, 99, 101, 217, 19, 248, 87, 194, 224, 43, 97, 85, 212, 189, 227, 101, 97, 40, 155, 196, 255, 104, 76, 179, 191, 159, 108, 35, 144, 11, 90, 97, 228, 161, 133, 105, 130, 72, 70, 118, 33, 41, 35, 1, 155, 41, 153, 166, 17, 62, 223, 24, 225, 186, 214, 214, 165, 129, 98, 64, 47, 55, 78, 146, 196, 193, 196, 59, 232, 118, 205, 93, 39, 141, 71, 113, 116, 152, 74, 141, 88, 60, 235, 204, 70, 242, 126, 132, 39, 135, 113, 151, 149, 57, 247, 59, 63, 135, 201, 205, 14, 203, 32, 42, 74, 165, 244, 84, 126, 165, 212, 193, 122, 9, 117, 150, 46, 163, 98, 223, 92, 8, 142, 240, 44, 177, 239, 140, 92, 180, 76, 25, 242, 158, 77, 43, 101, 146, 5, 125, 163, 147, 16, 35, 38, 247, 187, 205, 5, 247, 50, 200, 158, 247, 30, 68, 121, 208, 156, 199, 246, 83, 73, 4, 120, 162, 192, 116, 46, 106, 90, 175, 62, 6, 112, 255, 85, 21, 3, 70, 20, 44, 193, 97, 85, 79, 21, 116, 206, 251, 105, 3, 29, 12, 84, 8, 41, 155, 135, 20, 248, 18, 234, 158, 63, 97, 252, 79, 88, 87, 249, 21, 58, 94, 4, 175, 44, 203, 171, 227, 244, 94, 128, 144, 89, 177, 67, 63, 117, 62, 153, 110, 92, 72, 17, 183, 175, 148, 239, 132, 172, 102, 172, 68, 66, 180, 210, 249, 82, 0, 174, 144, 169, 209, 222, 244, 17, 35, 204, 161, 92, 56, 92, 87, 238, 77, 173, 210, 231, 87, 31, 176, 196, 162, 129, 114, 73, 27, 42, 2, 224, 215, 230, 63, 140, 96, 43, 158, 70, 110, 3, 145, 23, 18, 183, 177, 207, 125, 224, 198, 2, 64, 230, 197, 44, 254, 1, 253, 50, 177, 171, 211, 33, 202, 6, 208, 231, 219, 246, 242, 135, 47, 214, 113, 148, 230, 95, 17, 11, 246, 157, 190, 27, 20, 156, 66, 6, 142, 169, 215, 122, 248, 117, 33, 172, 189, 190, 245, 118, 28, 255, 92, 100, 227, 64, 14, 164, 221, 239, 177, 27, 100, 161, 54, 21, 96, 65, 72, 203, 71, 140, 127, 213, 4, 43, 9, 20, 151, 246, 222, 43, 251, 197, 221, 106, 212, 252, 212, 68, 165, 94, 40, 222, 133, 3, 57, 18, 0, 87, 146, 169, 65, 191, 158, 187, 220, 139, 192, 182, 68, 23, 19, 30, 236, 125, 105, 43, 150, 248, 215, 29, 50, 120, 45, 117, 42, 146, 15, 254, 130, 210, 152, 85, 214, 135, 112, 250, 53, 249, 9, 51, 122, 57, 244, 184, 145, 110, 201, 144, 17, 228, 11, 59, 88, 239, 38, 8, 225, 4, 218, 171, 99, 169, 41, 34, 225, 251, 168, 219, 82, 103, 117, 70, 63, 200, 139, 0, 42, 77, 28, 81, 202, 169, 68, 89, 234, 35, 183, 0, 217, 36, 140, 199, 126, 110, 13, 153, 150, 190, 235, 138, 42, 7, 170, 20, 155, 182, 41, 207, 229, 15, 81, 201, 19, 70, 107, 188, 42, 108, 105, 177, 182, 138, 64, 234, 104, 163, 169, 97, 66, 18, 229, 142, 125, 11, 16, 226, 30, 104, 181, 232, 6, 87, 99, 230, 249, 247, 29, 54, 60, 170, 88, 219, 147, 145, 228, 152, 111, 33, 255, 97, 135, 126, 12, 82, 172, 66, 131, 123, 131, 142, 154, 55, 50, 118, 29, 81, 227, 14, 19, 233, 129, 65, 150, 252, 114, 55, 104, 142, 77, 233, 202, 196, 3, 38, 170, 129, 193, 19, 82, 202, 51, 126, 30, 44, 27, 172, 60, 232, 60, 197, 23, 72, 58, 20, 182, 208, 104, 55, 112, 240, 175, 146, 255, 208, 6, 84, 142, 210, 56, 22, 119, 172, 65, 94, 93, 169, 165, 100, 163, 232, 88, 154, 31, 239, 165, 64, 230, 42, 19, 23, 37, 184, 8, 20, 67, 103, 43, 106, 79, 29, 137, 221, 6, 177, 126, 242, 91, 72, 157, 217, 116, 31, 21, 190, 167, 65, 241, 186, 105, 27, 98, 86, 212, 117, 233, 219, 46, 66, 153, 46, 215, 82, 33, 44, 120, 93, 0, 156, 162, 76, 134, 127, 184, 170, 232, 110, 229, 29, 130, 158, 241, 82, 172, 98, 171, 26, 102, 169, 230, 60, 31, 184, 101, 203, 233, 91, 15, 98, 176, 227, 215, 255, 74, 179, 81, 43, 229, 244, 144, 251, 245, 119, 63, 135, 78, 15, 200, 77, 158, 169, 8, 211, 21, 115, 44, 45, 11, 180, 232, 172, 195, 47, 76, 36, 204, 102, 214, 15, 21, 125, 138, 134, 109, 130, 35, 74, 248, 6, 105, 79, 193, 4, 126, 63, 250, 153, 190, 27, 53, 209, 1, 219, 235, 180, 175, 103, 134, 42, 151, 206, 100, 17, 168, 73, 181, 130, 229, 96, 249, 185, 95, 50, 122, 184, 232, 50, 92, 146, 58, 74, 43, 28, 22, 244, 156, 33, 160, 12, 41, 43, 83, 131, 41, 124, 144, 191, 16, 0, 165, 133, 43, 155, 122, 214, 111, 192, 15, 240, 52, 57, 43, 154, 171, 168, 225, 134, 53, 118, 141, 156, 208, 12, 111, 113, 7, 126, 91, 182, 18, 202, 221, 94, 71, 181, 72, 120, 204, 67, 142, 126, 142, 34, 195, 54, 154, 46, 161, 96, 60, 134, 41, 223, 12, 188, 136, 57, 87, 74, 236, 249, 136, 226, 247, 253, 78, 245, 140, 165, 1, 142, 59, 14, 123, 144, 20, 61, 82, 118, 150, 178, 214, 36, 244, 48, 154, 99, 172, 131, 137, 146, 190, 252, 35, 114, 97, 251, 15, 171, 126, 26, 246, 61, 78, 126, 115, 70, 193, 64, 31, 3, 154, 4, 141, 96, 200, 21, 135, 205, 188, 201, 73, 31, 84, 162, 192, 75, 212, 100, 191, 247, 44, 45, 77, 48, 89, 175, 182, 118, 172, 107, 124]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eRvwZ5Nf7g6"
      },
      "source": [
        "k = 8       # Number of information bits per message, i.e., M=2**k\n",
        "n = 4       # Number of real channel uses per message\n",
        "seed = 2    # Seed RNG reproduce identical results"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28KCbvYif7hD"
      },
      "source": [
        "#### The Autoencoder Class\n",
        "In order to quickly experiment with different architecture and parameter choices, it is useful to create a Python class that has functions for training and inference. Each autoencoder instance has its own Tensorflow session and graph. Thus, you can have multiple instances running at the same time without interference between them."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######### CREATING A PATH LOSS MODEL OUTSIDE CREATE GRAPH(MAIN LOOP)\n",
        "#IEEE 802.11p is an approved amendment to the IEEE 802.11 standard to add wireless access in vehicular environments (WAVE), a vehicular communication system. \n",
        "#IEEE 802.11p standard typically uses channels of 10 MHz bandwidth in the 5.9 GHz band (5.8505.925 GHz).\n",
        "s = np.floor(np.random.uniform(0,M, batch_size))\n",
        "s = s.astype('int64')\n",
        "f = 5.9*10**9;#in Hz corresponding to IEEE 802.11p\n",
        "lamda = 0.05;  #in metres\n",
        "Pt = 1; #BS transmitted power in watts\n",
        "Lo = 8;   #Total system losses in dB\n",
        "Nf = 5;    #Mobile receiver noise figure in dB\n",
        "T = 290;   #temperature in degree kelvin\n",
        "BW = 10*10**6; #in Hz\n",
        "Gb = 8;  #in dB\n",
        "Gm = 0;   #in dB\n",
        "Hb = 1;  #in metres\n",
        "Hm = 1.1;   #in metres\n",
        "B = 1.38*10**-23; #Boltzmann's constant\n",
        "Te = T*(3.162-1)\n",
        "Pn = B*(Te+T)*BW\n",
        "Free_Lp = {}\n",
        "Pr = {}\n",
        "\n",
        "SNR_var = {}\n",
        "#Calculations&Results\n",
        " "
      ],
      "metadata": {
        "id": "mpnHSzmqYXny"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs9Rtd01f7hG"
      },
      "source": [
        "\n",
        "class AE(object):\n",
        "    def __init__(self, k, n, seed=None, filename=None):\n",
        "        assert (n%2 == 0), \"Channel only allows complex symbols -> n must be a multiple of 2\"\n",
        "        self.k = k \n",
        "        self.n = n\n",
        "        self.n_complex = int(self.n/2)\n",
        "        self.bits_per_symbol = self.k/self.n_complex\n",
        "        self.M = 2**self.k\n",
        "        self.seed = seed if (seed is not None) else int(time.time())           \n",
        "        self.graph = None\n",
        "        self.sess = None   \n",
        "        self.vars = None\n",
        "        self.saver = None   \n",
        "        self.constellations = None\n",
        "        self.blers = None\n",
        "        self.create_graph()\n",
        "        self.create_session()\n",
        "        if filename is not None:    \n",
        "            self.load(filename)       \n",
        "        return\n",
        "    \n",
        "    def generate_distances(self,int_batch_size,bins):\n",
        "        # int_batch_size: 1000\n",
        "        # bins = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "\n",
        "        t = np.linspace(0,1,int_batch_size)\n",
        "        triangle1 = signal.sawtooth(2 * np.pi * 5 * t, 0.5)\n",
        "        triangle1 = 10*triangle1\n",
        "        triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "        triangle2 = signal.sawtooth(5 * np.pi * 5 * t, 0.5)\n",
        "        triangle2 = 10*triangle2\n",
        "        triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "        triangle3 = triangle1 + triangle2\n",
        "        triangle3 = triangle3/15*(7)\n",
        "        #print(s)\n",
        "\n",
        "        bins = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "        tr = np.digitize(triangle3, bins, right=True)\n",
        "        replacements = {0:7, 1:6, 2:5, 3:4, 4:3, 5:2, 6:1, 7:0}\n",
        "        replacer = replacements.get\n",
        "        tr = ([replacer(n, n) for n in tr])\n",
        "     \n",
        "        return tr\n",
        "\n",
        "\n",
        "    \n",
        "    def create_graph(self):\n",
        "        '''This function creates the computation graph of the autoencoder'''\n",
        "        self.graph = tf.Graph()        \n",
        "        with self.graph.as_default():    \n",
        "            tf.set_random_seed(self.seed)\n",
        "            batch_size = tf.placeholder(tf.int32, shape=())\n",
        "            \n",
        "            for i in range(1000):\n",
        "\n",
        "              Free_Lp[i] = 20*math.log10(Hm*Hb/s[i]**2)\n",
        "              Pr[i] = Free_Lp[i]-Lo+Gm+Gb+Pt  #in dBW\n",
        "              SNR_var[i] = Pr[i]-10*math.log10(Pn) #Provides SNR (stddev) values for all distances spanning s\n",
        "\n",
        "            ad_noise_std = []  #The values of the dictionary, finally providing an array of stdevs\n",
        "            for i in SNR_var.values():\n",
        "              ad_noise_std.append(i)\n",
        "\n",
        "            final_ad_noise_std = {}\n",
        "\n",
        "            for i in range(1000):\n",
        "              final_ad_noise_std[i] = np.random.normal(0.0, ad_noise_std[i], (2, 2))\n",
        "              #final_ad_noise_std[i] = tf.random_normal([batch_size,2,2], mean=0.0, stddev=ad_noise_std[i])\n",
        "\n",
        "\n",
        "            noise = []\n",
        "            for i in final_ad_noise_std.values():\n",
        "              noise.append(i)\n",
        "\n",
        "            noise = np.asarray(noise, np.float32)\n",
        "            noise = tf.convert_to_tensor(noise, np.float32)  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "            # Transmitter\n",
        "            #s = tf.random_uniform(shape=[batch_size], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            #s = tf.random_uniform(shape=[1000], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            #s = tf.cast(tf.floor(tf.linspace(0.0, M, batch_size, name=\"linspace\")), tf.int64)\n",
        "            print(\"Initial shape of S:\",s)\n",
        "            #s = tf.random_uniform(shape=[1000], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            \n",
        "            #plt.plot(t, triangle3, 'o')\n",
        "            #plt.plot(t, triangle3)\n",
        "            #print(s)\n",
        "            #plt.plot(t, s)\n",
        "            #s = tf.convert_to_tensor(s, dtype=tf.int64)\n",
        "            x = self.encoder(s)     \n",
        "            print(\"Initial shape of x:\",x)\n",
        "            \n",
        "            # Channel\n",
        "            noise_std = tf.placeholder(tf.float32, shape=())\n",
        "            #ad_noise_std = (tf.linspace(0.0, noise_std, tf.shape(x), name=\"linspace\"))\n",
        "            #ad_noise_std = tf.random_normal(tf.shape(x), mean=0.0, stddev=7)\n",
        "            #print(\"Initial shape of ad_noise:\",ad_noise_std)\n",
        "            #noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=ad_SNR)\n",
        "            #x = self.encoder(s)     \n",
        "            \n",
        "            # Channel\n",
        "            #noise_std = tf.placeholder(tf.float32, shape=())\n",
        "            #noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=noise_std) \n",
        "\n",
        "            fade = tf.random.normal(shape=tf.shape(x))\n",
        "            sparr1,sparr2 = tf.split(fade,num_or_size_splits=2, axis=2)\n",
        "            complex_fade = tf.complex(sparr1, sparr2)\n",
        "            fade = tf.abs(complex_fade)\n",
        "            #fade = tf.math.sqrt(1/2)*fade\n",
        "            print(\"Initial shape of fade:\",fade)\n",
        "            #print(\"Initial shape of noise:\",noise)\n",
        "            #fade = 1\n",
        "            y = tf.multiply(x,fade) + noise\n",
        "            print(\"Initial shape of y:\",y)\n",
        "           \n",
        "            # Receiver\n",
        "            s_hat = self.decoder(y)\n",
        "            correct_s_hat = tf.argmax(tf.nn.softmax(s_hat), axis=1)\n",
        "\n",
        "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat, weights= (s+1)**(4))\n",
        "\n",
        "\n",
        "            correct_predictions = tf.equal(tf.argmax(tf.nn.softmax(s_hat), axis=1), s)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "            bler = 1-accuracy\n",
        "\n",
        "            # Optimizer\n",
        "            lr = tf.placeholder(tf.float32, shape=()) # We can feed in any desired learning rate for each step     \n",
        "            train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
        "            #train_op_0 = tf.train.AdamOptimizer(lr).minimize(cross_entropy_0)\n",
        "            #lr = tf.train.exponential_decay(1e-10, global_step=cross_entropy, decay_steps=100, decay_rate=1.30)\n",
        "\n",
        "        \n",
        "            # References to graph variables we need to access later \n",
        "            self.vars = {\n",
        "                'accuracy': accuracy,\n",
        "                'batch_size': batch_size,\n",
        "                'bler': bler,\n",
        "                'cross_entropy': cross_entropy,\n",
        "                'init': tf.global_variables_initializer(),\n",
        "                'lr': lr,\n",
        "                'noise_std': noise_std,\n",
        "                'train_op': train_op,\n",
        "                's': s,\n",
        "                's_hat': s_hat,\n",
        "                'correct_s_hat': correct_s_hat,\n",
        "                'x': x,\n",
        "            }            \n",
        "            self.saver = tf.train.Saver()\n",
        "        return\n",
        "    \n",
        "    def create_session(self):\n",
        "        '''Create a session for the autoencoder instance with the compuational graph'''\n",
        "        self.sess = tf.Session(graph=self.graph)        \n",
        "        self.sess.run(self.vars['init'])\n",
        "        return\n",
        "    \n",
        "    def encoder(self, input):\n",
        "        '''The transmitter'''\n",
        "        self.weight_var_rec = self.weight_variable((self.M,self.M)) # shape = (8,8)\n",
        "        self.embedding_lookup_rec = tf.nn.embedding_lookup(self.weight_var_rec, input)\n",
        "        print(self.embedding_lookup_rec)\n",
        "        x = tf.nn.elu(self.embedding_lookup_rec)\n",
        "        #x = tf.layers.dense(self.embedding_lookup_rec, self.M, activation=tf.nn.relu)\n",
        "        x = tf.layers.dense(x, self.M, activation=None)\n",
        "        x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        x = tf.reshape(x, shape=[-1,self.n_complex,2])\n",
        "        print(x);\n",
        "        #Average power normalization\n",
        "        x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x))) \n",
        "        return x\n",
        "    \n",
        "    def decoder(self, input):\n",
        "        '''The Receiver'''\n",
        "        #input = self.flip_decoder(input)\n",
        "        y = tf.reshape(input, shape=[-1,self.n])\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        print(y);\n",
        "        return y\n",
        "    \n",
        "    def EbNo2Sigma(self, ebnodb):\n",
        "        '''Convert Eb/No in dB to noise standard deviation'''\n",
        "        ebno = 10**(ebnodb/10)\n",
        "        return 1/np.sqrt(2*self.bits_per_symbol*ebno)\n",
        "    \n",
        "    def gen_feed_dict(self, batch_size, ebnodb, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['lr']: lr,\n",
        "        }\n",
        "\n",
        "    def gen_e2e_feed_dict(self, batch_size, ebnodb, s_input):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['s']: s_input,\n",
        "        }   \n",
        "    \n",
        "    def load(self, filename):\n",
        "        '''Load an pre_trained model'''\n",
        "        return self.saver.restore(self.sess, filename)\n",
        "        \n",
        "    def plot_constellation(self, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = self.transmit(range(self.M))\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(self.n_complex):\n",
        "            image = plt.figure(figsize=(6,6))\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-maxrange,maxrange)\n",
        "            plt.ylim(-maxrange,maxrange)\n",
        "            for i in range(self.M):       \n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\",marker='x')   \n",
        "            image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            image.suptitle('%d. complex symbol' % (k+1))\n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "        return x, image\n",
        "    \n",
        "    def save(self, filename):\n",
        "        '''Save the current model'''\n",
        "        return self.saver.save(self.sess, filename)  \n",
        "    \n",
        "    def test_step(self, batch_size, ebnodb):\n",
        "        '''Compute the BLER over a single batch and Eb/No'''\n",
        "        bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n",
        "        return bler\n",
        "    \n",
        "    def transmit(self, s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['x'], feed_dict={self.vars['s']: s})\n",
        "\n",
        "    def end2end(self, batch_size, ebnodb, input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['correct_s_hat'], feed_dict=self.gen_e2e_feed_dict(batch_size, ebnodb, input_s)) \n",
        "        #print(self.sess.run(self.vars['correct_s_hat'], feed_dict={self.vars['s']: input_s}))     \n",
        "\n",
        "    #print(self.sess.run(self.vars['s_hat'], feed_dict={self.vars['s']: s}))\n",
        "\n",
        "    def train(self, training_params, validation_params):  \n",
        "        #s_input = self.generate_distances(100,np.array([0, 1, 2, 3, 4, 5, 6]))\n",
        "        \n",
        "        '''Training and validation loop'''\n",
        "        for index, params in enumerate(training_params):            \n",
        "            batch_size, lr, ebnodb, iterations = params            \n",
        "            print('\\nBatch Size: ' + str(batch_size) +\n",
        "                  ', Learning Rate: ' + str(lr) +\n",
        "                  ', EbNodB: ' + str(ebnodb) +\n",
        "                  ', Iterations: ' + str(iterations))\n",
        "            \n",
        "            val_size, val_ebnodb, val_steps = validation_params[index]\n",
        "            \n",
        "            for i in range(iterations):\n",
        "                self.train_step(batch_size, ebnodb, lr)    \n",
        "                if (i%val_steps==0):\n",
        "                    #bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_new_feed_dict(val_size, val_ebnodb, lr, s_input))\n",
        "                    bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size, val_ebnodb, lr))\n",
        "                    print(bler)                           \n",
        "        return       \n",
        "    \n",
        "    def train_step(self, batch_size, ebnodb, lr):\n",
        "        '''A single training step'''\n",
        "        #self.sess.run(self.vars['train_op'], feed_dict=self.gen_new_feed_dict(batch_size, ebnodb, lr, s_input))\n",
        "        self.sess.run(self.vars['train_op'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr))\n",
        "        return \n",
        "    \n",
        "    def weight_variable(self, shape):\n",
        "        '''Xavier-initialized weights optimized for ReLU Activations'''\n",
        "        (fan_in, fan_out) = shape\n",
        "        low = np.sqrt(6.0/(fan_in + fan_out)) \n",
        "        high = -np.sqrt(6.0/(fan_in + fan_out))\n",
        "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n",
        "    \n",
        "    def bler_sim(self, ebnodbs, batch_size, iterations):\n",
        "        '''Monte Carlo simulations of BLER for a range of Eb/No\n",
        "           Sometimes we to compute statistics for batch sizes that do not fit into the GPUs memory.\n",
        "           You can average over multiple batches with small size instead.           \n",
        "        '''\n",
        "        BLER = np.zeros_like(ebnodbs)\n",
        "        for i in range(iterations):\n",
        "            bler = np.array([self.sess.run(self.vars['bler'],\n",
        "                            feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0)) for ebnodb in ebnodbs])\n",
        "            BLER = BLER + bler/iterations\n",
        "        return BLER\n",
        "    \n",
        "    def plot_bler(self, EbNodB, BLER):\n",
        "        '''Plot a BLER curve'''\n",
        "        image = plt.figure(figsize=(10,8))\n",
        "        plt.plot(EbNodB, BLER, '-r', linewidth=2.0)\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('EbNo (dB)', fontsize=18)\n",
        "        plt.ylabel('Block-error rate', fontsize=18)\n",
        "        plt.grid(True)\n",
        "        plt.ylim([1e-5,1])\n",
        "        return image"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bANUdLIsf7hM"
      },
      "source": [
        "## Training parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeHghWVRf7hO",
        "outputId": "00babb6b-9ffb-4947-b494-fb2ba343247e"
      },
      "source": [
        "train_EbNodB = 40\n",
        "val_EbNodB = train_EbNodB\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "\n",
        "#epoch = [10000]\n",
        "\n",
        "#for i in epoch:\n",
        "training_params = [\n",
        "    #batch_size, lr, ebnodb, iterations\n",
        "    [1000 , lr, train_EbNodB, 10000]\n",
        "    ]\n",
        "\n",
        "validation_params = [\n",
        "    #batch_size, ebnodb, val_steps \n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000]\n",
        "    ]\n",
        "\n",
        "model_file_uw = 'models/ae_k_{}_n_{}'.format(k,n)\n",
        "ae = AE(k,n,seed)\n",
        "ae.train(training_params, validation_params)\n",
        "ae.save(model_file_uw);\n",
        "ae = AE(k,n,seed, filename=model_file_uw)\n",
        "  #ae.plot_constellation();\n",
        "  #ae.end2end(tr)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in true_divide\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial shape of S: [196 242 105 228  57  73  33 160 180  43 142 243  13 129 105 202 185 236\n",
            "  95 127   9 252 136 120 207 240 150  59 108 175   6  21 151 177 158 243\n",
            "  26   4 124 255  68 110  60  27 255 194 228 163  87  14 168 255 135 169\n",
            " 157 220 196 151 252 184 148  64  96  96  30  65  57 234 211 106  71  91\n",
            "  17  89  17 128 195 246  13 137 198 108  78 209  84 241 147 215  44 103\n",
            " 217  50 124  46  41  48  69  56 209 159 179 203 146   8  85 162 150 102\n",
            " 149 155  75 155  38 168   4  55 220 106 223 160   0 136  46   8  87 182\n",
            " 168  29 126 199 225 161 132 184  42  37 162 135  11 226 136  69 172  68\n",
            "  43 169  14 123 151 123  32  46 162  70 246 245 235 226 155  27 198 162\n",
            " 145  75 196 185 122 235 243  47 218 100  30 183  56  18 147 136  91 136\n",
            "  31 249 193  15 151  49  13 139  59  99 202 167   0  40 144  55  23  79\n",
            " 173 207 133 236 237  69  42 222 106 252 111  37 155 225 251 134  90 148\n",
            "  48 254 143 159  51 152 191 180  70  22  35 206  51 227  18 251 207  88\n",
            "  99  21 167 123  53 172 169 198 189 116 216  37 136  93   1 167 246 142\n",
            "  10 223 233 154 190 118 192 112   2   6  96 253 188   7 200  57  66 108\n",
            "  66 125 166 157 107  82 229 100  64 232  49 161  50 210   4 170 179 157\n",
            " 169  89 232 169 133 224 143  21  58  54 147 249  99 214 139 193 171  61\n",
            " 193 126 242 188 155 225  99 194  83 200 191 123 251  78 151  67  10 150\n",
            "  89   1 149 107 109  36  99 236 195 148 162 127 204  85 216  64 145 230\n",
            "   2 188  93  91 173  58   8  98 186  92  48 116  85  99  46 111 129 173\n",
            " 181 135  92 222 129  77 226 108 224  67 105  58 230  54 223 234 220  43\n",
            "  61  79 170  80 250 229 171  55  13  20  72 198 197 207 152 109 149 206\n",
            " 124 178 253 255  77 144 116 180 186  17  49  49  49 216  97 112  86 126\n",
            " 156 103 212 129  77  56  17 153 115 140  95 186  85  18 144 184  91 163\n",
            " 159  65 125 158 245  39 145 148  93 132  44  72  46   1 227  36 223 179\n",
            " 177 106  21 110 115 104  37 201  31 240 112  75  77  45 198  71  20 217\n",
            "  63 124 195  51  89 212  40 105   3 101 216 116 136 117 149  70 164 244\n",
            "  11 223 203  19 118  28 131  42  74 191  91 203 217  81 106 162 100  61\n",
            "  30   0 248 193   3 238 135 203  96 122 103  11 181 206  60  24  30 106\n",
            "  66 216  46  62  90 128 230 145 222 198 247 182  72 167 122  66  78 153\n",
            " 216  96  52  76 120  41 212  10  70 169  14 192 125 134  67 157  22  28\n",
            " 163 103  27 233  60  85 245  24 216 239 253 191 109 252 196  36 172 217\n",
            " 227  62  37  36  44   4   5 210 202 132 161 136  73 182  62  95  34 166\n",
            "  72  19 185   2  92  28  74  15 125  41 179 185  18 184  97  57 188  12\n",
            " 154 196 184  76 152 185 121 104 201  19 219 126  78  64 193  25 174  66\n",
            "  39 138 157  95 170 184 183  88 205 116 119 193 186  17 140   0  35 250\n",
            " 255 143 195 138   7  12  95 108  90  57 236 127  31 230 186  31 199 107\n",
            " 150   2 192  25 247 185 242 192  98   8 206 162 210 250 254  24 107 182\n",
            " 109  67  75 153  96 158 190  43 231 210   4  91 164 205 254 181 167 105\n",
            " 118  24 199  65 106 216 140 245 230 180 127 217 153 138   3 136 207 196\n",
            " 191 243  26 136  50  21 208 181 238 214 201 147  25  41 235  34  42 128\n",
            " 190  61  17  71  91  21  78  11  48  97 162 194 121  10  47  46 210  29\n",
            " 214 140 198  93  96 124 198 126 235  14  11  20 234 111  79 133 106  22\n",
            "  82   2 211 203  52 249 123  12  39 235  77 141 218 198 170 255 159 190\n",
            " 241  89  65 166 224 240 211 242  18 135  82  81  79 218  42  93  87 125\n",
            " 103 183 202 239 120  87 198  70 129 198 134   7 237 117 196  38  37  66\n",
            "  56  39 180  32   8 143 145 236   8  22 107 114 207 248  91 159  79 220\n",
            "  45 247 130  64  98  23 144 223  60 177 143 195 225  71  13 223 155 177\n",
            " 156 202 236 107  39 238 110  10 116 245 206 219 179 142 159  18  87 217\n",
            "  47 125 222  35 213 141  21  93  68  74 169 255   4 176 247  60 143 235\n",
            "   2  45  53 147   3 206  87 132 228  90 241 212   0 244 230 217 111 120\n",
            "  35  76  49 248  54  98  71 238 210  99 175 229  19  29 125 104  14  77\n",
            " 201  92 221 220 155 205 242  22  35  13 236 226  11 132 233  23  23 161\n",
            "   7   6 219 203 142 250 231  40 238 175 181 200  25 131 152  93  45 215\n",
            " 163  13  14 198  17 243 120 156 230 136 172   0 100  37 124 123 163 216\n",
            " 125 152  95 135 127  24  59  37 235 141]\n",
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of x: Tensor(\"truediv:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of fade: Tensor(\"Abs:0\", shape=(1000, 2, 1), dtype=float32)\n",
            "Initial shape of y: Tensor(\"add:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n",
            "\n",
            "Batch Size: 1000, Learning Rate: 0.01, EbNodB: 40, Iterations: 10000\n",
            "0.994\n",
            "0.994\n",
            "0.994\n",
            "0.994\n",
            "0.994\n",
            "0.994\n",
            "0.994\n",
            "0.994\n",
            "0.994\n",
            "0.994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in true_divide\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial shape of S: [196 242 105 228  57  73  33 160 180  43 142 243  13 129 105 202 185 236\n",
            "  95 127   9 252 136 120 207 240 150  59 108 175   6  21 151 177 158 243\n",
            "  26   4 124 255  68 110  60  27 255 194 228 163  87  14 168 255 135 169\n",
            " 157 220 196 151 252 184 148  64  96  96  30  65  57 234 211 106  71  91\n",
            "  17  89  17 128 195 246  13 137 198 108  78 209  84 241 147 215  44 103\n",
            " 217  50 124  46  41  48  69  56 209 159 179 203 146   8  85 162 150 102\n",
            " 149 155  75 155  38 168   4  55 220 106 223 160   0 136  46   8  87 182\n",
            " 168  29 126 199 225 161 132 184  42  37 162 135  11 226 136  69 172  68\n",
            "  43 169  14 123 151 123  32  46 162  70 246 245 235 226 155  27 198 162\n",
            " 145  75 196 185 122 235 243  47 218 100  30 183  56  18 147 136  91 136\n",
            "  31 249 193  15 151  49  13 139  59  99 202 167   0  40 144  55  23  79\n",
            " 173 207 133 236 237  69  42 222 106 252 111  37 155 225 251 134  90 148\n",
            "  48 254 143 159  51 152 191 180  70  22  35 206  51 227  18 251 207  88\n",
            "  99  21 167 123  53 172 169 198 189 116 216  37 136  93   1 167 246 142\n",
            "  10 223 233 154 190 118 192 112   2   6  96 253 188   7 200  57  66 108\n",
            "  66 125 166 157 107  82 229 100  64 232  49 161  50 210   4 170 179 157\n",
            " 169  89 232 169 133 224 143  21  58  54 147 249  99 214 139 193 171  61\n",
            " 193 126 242 188 155 225  99 194  83 200 191 123 251  78 151  67  10 150\n",
            "  89   1 149 107 109  36  99 236 195 148 162 127 204  85 216  64 145 230\n",
            "   2 188  93  91 173  58   8  98 186  92  48 116  85  99  46 111 129 173\n",
            " 181 135  92 222 129  77 226 108 224  67 105  58 230  54 223 234 220  43\n",
            "  61  79 170  80 250 229 171  55  13  20  72 198 197 207 152 109 149 206\n",
            " 124 178 253 255  77 144 116 180 186  17  49  49  49 216  97 112  86 126\n",
            " 156 103 212 129  77  56  17 153 115 140  95 186  85  18 144 184  91 163\n",
            " 159  65 125 158 245  39 145 148  93 132  44  72  46   1 227  36 223 179\n",
            " 177 106  21 110 115 104  37 201  31 240 112  75  77  45 198  71  20 217\n",
            "  63 124 195  51  89 212  40 105   3 101 216 116 136 117 149  70 164 244\n",
            "  11 223 203  19 118  28 131  42  74 191  91 203 217  81 106 162 100  61\n",
            "  30   0 248 193   3 238 135 203  96 122 103  11 181 206  60  24  30 106\n",
            "  66 216  46  62  90 128 230 145 222 198 247 182  72 167 122  66  78 153\n",
            " 216  96  52  76 120  41 212  10  70 169  14 192 125 134  67 157  22  28\n",
            " 163 103  27 233  60  85 245  24 216 239 253 191 109 252 196  36 172 217\n",
            " 227  62  37  36  44   4   5 210 202 132 161 136  73 182  62  95  34 166\n",
            "  72  19 185   2  92  28  74  15 125  41 179 185  18 184  97  57 188  12\n",
            " 154 196 184  76 152 185 121 104 201  19 219 126  78  64 193  25 174  66\n",
            "  39 138 157  95 170 184 183  88 205 116 119 193 186  17 140   0  35 250\n",
            " 255 143 195 138   7  12  95 108  90  57 236 127  31 230 186  31 199 107\n",
            " 150   2 192  25 247 185 242 192  98   8 206 162 210 250 254  24 107 182\n",
            " 109  67  75 153  96 158 190  43 231 210   4  91 164 205 254 181 167 105\n",
            " 118  24 199  65 106 216 140 245 230 180 127 217 153 138   3 136 207 196\n",
            " 191 243  26 136  50  21 208 181 238 214 201 147  25  41 235  34  42 128\n",
            " 190  61  17  71  91  21  78  11  48  97 162 194 121  10  47  46 210  29\n",
            " 214 140 198  93  96 124 198 126 235  14  11  20 234 111  79 133 106  22\n",
            "  82   2 211 203  52 249 123  12  39 235  77 141 218 198 170 255 159 190\n",
            " 241  89  65 166 224 240 211 242  18 135  82  81  79 218  42  93  87 125\n",
            " 103 183 202 239 120  87 198  70 129 198 134   7 237 117 196  38  37  66\n",
            "  56  39 180  32   8 143 145 236   8  22 107 114 207 248  91 159  79 220\n",
            "  45 247 130  64  98  23 144 223  60 177 143 195 225  71  13 223 155 177\n",
            " 156 202 236 107  39 238 110  10 116 245 206 219 179 142 159  18  87 217\n",
            "  47 125 222  35 213 141  21  93  68  74 169 255   4 176 247  60 143 235\n",
            "   2  45  53 147   3 206  87 132 228  90 241 212   0 244 230 217 111 120\n",
            "  35  76  49 248  54  98  71 238 210  99 175 229  19  29 125 104  14  77\n",
            " 201  92 221 220 155 205 242  22  35  13 236 226  11 132 233  23  23 161\n",
            "   7   6 219 203 142 250 231  40 238 175 181 200  25 131 152  93  45 215\n",
            " 163  13  14 198  17 243 120 156 230 136 172   0 100  37 124 123 163 216\n",
            " 125 152  95 135 127  24  59  37 235 141]\n",
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of x: Tensor(\"truediv:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of fade: Tensor(\"Abs:0\", shape=(1000, 2, 1), dtype=float32)\n",
            "Initial shape of y: Tensor(\"add:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl3hENsvf7hW"
      },
      "source": [
        "## Create and train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0GA7ihjf7hZ"
      },
      "source": [
        "#model_file = 'models/ae_k_{}_n_{}'.format(k,n)\n",
        "#ae = AE(k,n,seed)\n",
        "#ae.train(training_params, validation_params)\n",
        "#ae.save(model_file); # Save the trained autoencoder if you want to reuse it later\n",
        "#sess = tf.Session()\n",
        "#print(sess.run((rmse_uw_0)))"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzqpcwGaf7hm"
      },
      "source": [
        "## Evaluate trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y33vV4xKf7hn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24a9b11e-7208-4f64-a5d5-8df0961c2041"
      },
      "source": [
        "ae_uw = AE(k,n,seed, filename=model_file_uw) #Load a pretrained model that you have saved if needed"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in true_divide\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial shape of S: [196 242 105 228  57  73  33 160 180  43 142 243  13 129 105 202 185 236\n",
            "  95 127   9 252 136 120 207 240 150  59 108 175   6  21 151 177 158 243\n",
            "  26   4 124 255  68 110  60  27 255 194 228 163  87  14 168 255 135 169\n",
            " 157 220 196 151 252 184 148  64  96  96  30  65  57 234 211 106  71  91\n",
            "  17  89  17 128 195 246  13 137 198 108  78 209  84 241 147 215  44 103\n",
            " 217  50 124  46  41  48  69  56 209 159 179 203 146   8  85 162 150 102\n",
            " 149 155  75 155  38 168   4  55 220 106 223 160   0 136  46   8  87 182\n",
            " 168  29 126 199 225 161 132 184  42  37 162 135  11 226 136  69 172  68\n",
            "  43 169  14 123 151 123  32  46 162  70 246 245 235 226 155  27 198 162\n",
            " 145  75 196 185 122 235 243  47 218 100  30 183  56  18 147 136  91 136\n",
            "  31 249 193  15 151  49  13 139  59  99 202 167   0  40 144  55  23  79\n",
            " 173 207 133 236 237  69  42 222 106 252 111  37 155 225 251 134  90 148\n",
            "  48 254 143 159  51 152 191 180  70  22  35 206  51 227  18 251 207  88\n",
            "  99  21 167 123  53 172 169 198 189 116 216  37 136  93   1 167 246 142\n",
            "  10 223 233 154 190 118 192 112   2   6  96 253 188   7 200  57  66 108\n",
            "  66 125 166 157 107  82 229 100  64 232  49 161  50 210   4 170 179 157\n",
            " 169  89 232 169 133 224 143  21  58  54 147 249  99 214 139 193 171  61\n",
            " 193 126 242 188 155 225  99 194  83 200 191 123 251  78 151  67  10 150\n",
            "  89   1 149 107 109  36  99 236 195 148 162 127 204  85 216  64 145 230\n",
            "   2 188  93  91 173  58   8  98 186  92  48 116  85  99  46 111 129 173\n",
            " 181 135  92 222 129  77 226 108 224  67 105  58 230  54 223 234 220  43\n",
            "  61  79 170  80 250 229 171  55  13  20  72 198 197 207 152 109 149 206\n",
            " 124 178 253 255  77 144 116 180 186  17  49  49  49 216  97 112  86 126\n",
            " 156 103 212 129  77  56  17 153 115 140  95 186  85  18 144 184  91 163\n",
            " 159  65 125 158 245  39 145 148  93 132  44  72  46   1 227  36 223 179\n",
            " 177 106  21 110 115 104  37 201  31 240 112  75  77  45 198  71  20 217\n",
            "  63 124 195  51  89 212  40 105   3 101 216 116 136 117 149  70 164 244\n",
            "  11 223 203  19 118  28 131  42  74 191  91 203 217  81 106 162 100  61\n",
            "  30   0 248 193   3 238 135 203  96 122 103  11 181 206  60  24  30 106\n",
            "  66 216  46  62  90 128 230 145 222 198 247 182  72 167 122  66  78 153\n",
            " 216  96  52  76 120  41 212  10  70 169  14 192 125 134  67 157  22  28\n",
            " 163 103  27 233  60  85 245  24 216 239 253 191 109 252 196  36 172 217\n",
            " 227  62  37  36  44   4   5 210 202 132 161 136  73 182  62  95  34 166\n",
            "  72  19 185   2  92  28  74  15 125  41 179 185  18 184  97  57 188  12\n",
            " 154 196 184  76 152 185 121 104 201  19 219 126  78  64 193  25 174  66\n",
            "  39 138 157  95 170 184 183  88 205 116 119 193 186  17 140   0  35 250\n",
            " 255 143 195 138   7  12  95 108  90  57 236 127  31 230 186  31 199 107\n",
            " 150   2 192  25 247 185 242 192  98   8 206 162 210 250 254  24 107 182\n",
            " 109  67  75 153  96 158 190  43 231 210   4  91 164 205 254 181 167 105\n",
            " 118  24 199  65 106 216 140 245 230 180 127 217 153 138   3 136 207 196\n",
            " 191 243  26 136  50  21 208 181 238 214 201 147  25  41 235  34  42 128\n",
            " 190  61  17  71  91  21  78  11  48  97 162 194 121  10  47  46 210  29\n",
            " 214 140 198  93  96 124 198 126 235  14  11  20 234 111  79 133 106  22\n",
            "  82   2 211 203  52 249 123  12  39 235  77 141 218 198 170 255 159 190\n",
            " 241  89  65 166 224 240 211 242  18 135  82  81  79 218  42  93  87 125\n",
            " 103 183 202 239 120  87 198  70 129 198 134   7 237 117 196  38  37  66\n",
            "  56  39 180  32   8 143 145 236   8  22 107 114 207 248  91 159  79 220\n",
            "  45 247 130  64  98  23 144 223  60 177 143 195 225  71  13 223 155 177\n",
            " 156 202 236 107  39 238 110  10 116 245 206 219 179 142 159  18  87 217\n",
            "  47 125 222  35 213 141  21  93  68  74 169 255   4 176 247  60 143 235\n",
            "   2  45  53 147   3 206  87 132 228  90 241 212   0 244 230 217 111 120\n",
            "  35  76  49 248  54  98  71 238 210  99 175 229  19  29 125 104  14  77\n",
            " 201  92 221 220 155 205 242  22  35  13 236 226  11 132 233  23  23 161\n",
            "   7   6 219 203 142 250 231  40 238 175 181 200  25 131 152  93  45 215\n",
            " 163  13  14 198  17 243 120 156 230 136 172   0 100  37 124 123 163 216\n",
            " 125 152  95 135 127  24  59  37 235 141]\n",
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of x: Tensor(\"truediv:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of fade: Tensor(\"Abs:0\", shape=(1000, 2, 1), dtype=float32)\n",
            "Initial shape of y: Tensor(\"add:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fasS-Rz1lapW"
      },
      "source": [
        "\n",
        "class AE_Weighted(object):\n",
        "    def __init__(self, k, n, seed=None, filename=None):\n",
        "        assert (n%2 == 0), \"Channel only allows complex symbols -> n must be a multiple of 2\"\n",
        "        self.k = k \n",
        "        self.n = n\n",
        "        self.n_complex = int(self.n/2)\n",
        "        self.bits_per_symbol = self.k/self.n_complex\n",
        "        self.M = 2**self.k\n",
        "        self.seed = seed if (seed is not None) else int(time.time())           \n",
        "        self.graph = None\n",
        "        self.sess = None   \n",
        "        self.vars = None\n",
        "        self.saver = None   \n",
        "        self.constellations = None\n",
        "        self.blers = None\n",
        "        self.create_graph()\n",
        "        self.create_session()\n",
        "        if filename is not None:    \n",
        "            self.load(filename)       \n",
        "        return\n",
        "    \n",
        "    def generate_distances(self,int_batch_size,bins):\n",
        "        # int_batch_size: 1000\n",
        "        # bins = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "\n",
        "        t = np.linspace(0,1,int_batch_size)\n",
        "        triangle1 = signal.sawtooth(2 * np.pi * 5 * t, 0.5)\n",
        "        triangle1 = 10*triangle1\n",
        "        triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "        triangle2 = signal.sawtooth(5 * np.pi * 5 * t, 0.5)\n",
        "        triangle2 = 10*triangle2\n",
        "        triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "        triangle3 = triangle1 + triangle2\n",
        "        triangle3 = triangle3/15*(7)\n",
        "        #print(s)\n",
        "\n",
        "        bins = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "        tr = np.digitize(triangle3, bins, right=True)\n",
        "        replacements = {0:7, 1:6, 2:5, 3:4, 4:3, 5:2, 6:1, 7:0}\n",
        "        replacer = replacements.get\n",
        "        tr = ([replacer(n, n) for n in tr])\n",
        "     \n",
        "        return tr\n",
        "\n",
        "\n",
        "    \n",
        "    def create_graph(self):\n",
        "        '''This function creates the computation graph of the autoencoder'''\n",
        "        self.graph = tf.Graph()        \n",
        "        with self.graph.as_default():    \n",
        "            tf.set_random_seed(self.seed)\n",
        "            batch_size = tf.placeholder(tf.int32, shape=())\n",
        "            \n",
        "            for i in range(1000):\n",
        "\n",
        "              Free_Lp[i] = 20*math.log10(Hm*Hb/s[i]**2)\n",
        "              Pr[i] = Free_Lp[i]-Lo+Gm+Gb+Pt  #in dBW\n",
        "              SNR_var[i] = Pr[i]-10*math.log10(Pn) #Provides SNR (stddev) values for all distances spanning s\n",
        "\n",
        "            ad_noise_std = []  #The values of the dictionary, finally providing an array of stdevs\n",
        "            for i in SNR_var.values():\n",
        "              ad_noise_std.append(i)\n",
        "\n",
        "            final_ad_noise_std = {}\n",
        "\n",
        "            for i in range(1000):\n",
        "              final_ad_noise_std[i] = np.random.normal(0.0, ad_noise_std[i], (2, 2))\n",
        "              #final_ad_noise_std[i] = tf.random_normal([batch_size,2,2], mean=0.0, stddev=ad_noise_std[i])\n",
        "\n",
        "\n",
        "            noise = []\n",
        "            for i in final_ad_noise_std.values():\n",
        "              noise.append(i)\n",
        "\n",
        "            noise = np.asarray(noise, np.float32)\n",
        "            noise = tf.convert_to_tensor(noise, np.float32)  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "            # Transmitter\n",
        "            #s = tf.random_uniform(shape=[batch_size], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            #s = tf.random_uniform(shape=[1000], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            #s = tf.cast(tf.floor(tf.linspace(0.0, M, batch_size, name=\"linspace\")), tf.int64)\n",
        "            print(\"Initial shape of S:\",s)\n",
        "            #s = tf.random_uniform(shape=[1000], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            \n",
        "            #plt.plot(t, triangle3, 'o')\n",
        "            #plt.plot(t, triangle3)\n",
        "            #print(s)\n",
        "            #plt.plot(t, s)\n",
        "            #s = tf.convert_to_tensor(s, dtype=tf.int64)\n",
        "            x = self.encoder(s)     \n",
        "            print(\"Initial shape of x:\",x)\n",
        "            \n",
        "            # Channel\n",
        "            noise_std = tf.placeholder(tf.float32, shape=())        \n",
        "            # Channel\n",
        "            #noise_std = tf.placeholder(tf.float32, shape=())\n",
        "            #noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=noise_std) \n",
        "\n",
        "            fade = tf.random.normal(shape=tf.shape(x))\n",
        "            sparr1,sparr2 = tf.split(fade,num_or_size_splits=2, axis=2)\n",
        "            complex_fade = tf.complex(sparr1, sparr2)\n",
        "            fade = tf.abs(complex_fade)\n",
        "            #fade = tf.math.sqrt(1/2)*fade\n",
        "\n",
        "            #fade = 1\n",
        "            y = tf.multiply(x,fade) + noise\n",
        "            \n",
        "           \n",
        "            # Receiver\n",
        "            s_hat = self.decoder(y)\n",
        "            correct_s_hat = tf.argmax(tf.nn.softmax(s_hat), axis=1)\n",
        "\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat, weights= (s+1)**(4))\n",
        "\n",
        "\n",
        "            correct_predictions = tf.equal(tf.argmax(tf.nn.softmax(s_hat), axis=1), s)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "            bler = 1-accuracy\n",
        "\n",
        "            # Optimizer\n",
        "            lr = tf.placeholder(tf.float32, shape=()) # We can feed in any desired learning rate for each step     \n",
        "            train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
        "            #train_op_0 = tf.train.AdamOptimizer(lr).minimize(cross_entropy_0)\n",
        "            #lr = tf.train.exponential_decay(1e-10, global_step=cross_entropy, decay_steps=100, decay_rate=1.30)\n",
        "\n",
        "        \n",
        "            # References to graph variables we need to access later \n",
        "            self.vars = {\n",
        "                'accuracy': accuracy,\n",
        "                'batch_size': batch_size,\n",
        "                'bler': bler,\n",
        "                'cross_entropy': cross_entropy,\n",
        "                'init': tf.global_variables_initializer(),\n",
        "                'lr': lr,\n",
        "                'noise_std': noise_std,\n",
        "                'train_op': train_op,\n",
        "                's': s,\n",
        "                's_hat': s_hat,\n",
        "                'correct_s_hat': correct_s_hat,\n",
        "                'x': x,\n",
        "            }            \n",
        "            self.saver = tf.train.Saver()\n",
        "        return\n",
        "    \n",
        "    def create_session(self):\n",
        "        '''Create a session for the autoencoder instance with the compuational graph'''\n",
        "        self.sess = tf.Session(graph=self.graph)        \n",
        "        self.sess.run(self.vars['init'])\n",
        "        return\n",
        "    \n",
        "    def encoder(self, input):\n",
        "        '''The transmitter'''\n",
        "        self.weight_var_rec = self.weight_variable((self.M,self.M)) # shape = (8,8)\n",
        "        self.embedding_lookup_rec = tf.nn.embedding_lookup(self.weight_var_rec, input)\n",
        "        print(self.embedding_lookup_rec)\n",
        "        x = tf.nn.elu(self.embedding_lookup_rec)\n",
        "        #x = tf.layers.dense(self.embedding_lookup_rec, self.M, activation=tf.nn.relu)\n",
        "        x = tf.layers.dense(x, self.M, activation=None)\n",
        "        x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        x = tf.reshape(x, shape=[-1,self.n_complex,2])\n",
        "        print(x);\n",
        "        #Average power normalization\n",
        "        x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x))) \n",
        "        return x\n",
        "    \n",
        "    def decoder(self, input):\n",
        "        '''The Receiver'''\n",
        "        #input = self.flip_decoder(input)\n",
        "        y = tf.reshape(input, shape=[-1,self.n])\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        print(y);\n",
        "        return y\n",
        "    \n",
        "    def EbNo2Sigma(self, ebnodb):\n",
        "        '''Convert Eb/No in dB to noise standard deviation'''\n",
        "        ebno = 10**(ebnodb/10)\n",
        "        return 1/np.sqrt(2*self.bits_per_symbol*ebno)\n",
        "    \n",
        "    def gen_feed_dict(self, batch_size, ebnodb, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['lr']: lr,\n",
        "        }\n",
        "\n",
        "    def gen_e2e_feed_dict(self, batch_size, ebnodb, s_input):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['s']: s_input,\n",
        "        }   \n",
        "    \n",
        "    def load(self, filename):\n",
        "        '''Load an pre_trained model'''\n",
        "        return self.saver.restore(self.sess, filename)\n",
        "        \n",
        "    def plot_constellation(self, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = self.transmit(range(self.M))\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(self.n_complex):\n",
        "            image = plt.figure(figsize=(6,6))\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-maxrange,maxrange)\n",
        "            plt.ylim(-maxrange,maxrange)\n",
        "            for i in range(self.M):       \n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\",marker='x')   \n",
        "            image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            image.suptitle('%d. complex symbol' % (k+1))\n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "        return x, image\n",
        "    \n",
        "    def save(self, filename):\n",
        "        '''Save the current model'''\n",
        "        return self.saver.save(self.sess, filename)  \n",
        "    \n",
        "    def test_step(self, batch_size, ebnodb):\n",
        "        '''Compute the BLER over a single batch and Eb/No'''\n",
        "        bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n",
        "        return bler\n",
        "    \n",
        "    def transmit(self, s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['x'], feed_dict={self.vars['s']: s})\n",
        "\n",
        "    def end2end(self, batch_size, ebnodb, input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['correct_s_hat'], feed_dict=self.gen_e2e_feed_dict(batch_size, ebnodb, input_s)) \n",
        "        #print(self.sess.run(self.vars['correct_s_hat'], feed_dict={self.vars['s']: input_s}))     \n",
        "\n",
        "    #print(self.sess.run(self.vars['s_hat'], feed_dict={self.vars['s']: s}))\n",
        "\n",
        "    def train(self, training_params, validation_params):  \n",
        "        #s_input = self.generate_distances(100,np.array([0, 1, 2, 3, 4, 5, 6]))\n",
        "        \n",
        "        '''Training and validation loop'''\n",
        "        for index, params in enumerate(training_params):            \n",
        "            batch_size, lr, ebnodb, iterations = params            \n",
        "            print('\\nBatch Size: ' + str(batch_size) +\n",
        "                  ', Learning Rate: ' + str(lr) +\n",
        "                  ', EbNodB: ' + str(ebnodb) +\n",
        "                  ', Iterations: ' + str(iterations))\n",
        "            \n",
        "            val_size, val_ebnodb, val_steps = validation_params[index]\n",
        "            \n",
        "            for i in range(iterations):\n",
        "                self.train_step(batch_size, ebnodb, lr)    \n",
        "                if (i%val_steps==0):\n",
        "                    #bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_new_feed_dict(val_size, val_ebnodb, lr, s_input))\n",
        "                    bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size, val_ebnodb, lr))\n",
        "                    print(bler)                           \n",
        "        return       \n",
        "    \n",
        "    def train_step(self, batch_size, ebnodb, lr):\n",
        "        '''A single training step'''\n",
        "        #self.sess.run(self.vars['train_op'], feed_dict=self.gen_new_feed_dict(batch_size, ebnodb, lr, s_input))\n",
        "        self.sess.run(self.vars['train_op'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr))\n",
        "        return \n",
        "    \n",
        "    def weight_variable(self, shape):\n",
        "        '''Xavier-initialized weights optimized for ReLU Activations'''\n",
        "        (fan_in, fan_out) = shape\n",
        "        low = np.sqrt(6.0/(fan_in + fan_out)) \n",
        "        high = -np.sqrt(6.0/(fan_in + fan_out))\n",
        "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n",
        "    \n",
        "    def bler_sim(self, ebnodbs, batch_size, iterations):\n",
        "        '''Monte Carlo simulations of BLER for a range of Eb/No\n",
        "           Sometimes we to compute statistics for batch sizes that do not fit into the GPUs memory.\n",
        "           You can average over multiple batches with small size instead.           \n",
        "        '''\n",
        "        BLER = np.zeros_like(ebnodbs)\n",
        "        for i in range(iterations):\n",
        "            bler = np.array([self.sess.run(self.vars['bler'],\n",
        "                            feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0)) for ebnodb in ebnodbs])\n",
        "            BLER = BLER + bler/iterations\n",
        "        return BLER\n",
        "    \n",
        "    def plot_bler(self, EbNodB, BLER):\n",
        "        '''Plot a BLER curve'''\n",
        "        image = plt.figure(figsize=(10,8))\n",
        "        plt.plot(EbNodB, BLER, '-r', linewidth=2.0)\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('EbNo (dB)', fontsize=18)\n",
        "        plt.ylabel('Block-error rate', fontsize=18)\n",
        "        plt.grid(True)\n",
        "        plt.ylim([1e-5,1])\n",
        "        return image"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjh_J3MIlyrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "972dcb40-463c-4ffa-e29f-fb595f6e35f8"
      },
      "source": [
        "train_EbNodB = 40\n",
        "val_EbNodB = train_EbNodB\n",
        "\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "training_params = [\n",
        "    #batch_size, lr, ebnodb, iterations\n",
        "    [1000 , lr, train_EbNodB, 1000]\n",
        "    ]\n",
        "\n",
        "validation_params = [\n",
        "    #batch_size, ebnodb, val_steps \n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000]\n",
        "    ]\n",
        "\n",
        "model_file = 'models/ae_k_{}_n_{}'.format(k,n)\n",
        "ae_Weighted = AE_Weighted(k,n,seed)\n",
        "ae_Weighted.train(training_params, validation_params)\n",
        "ae_Weighted.save(model_file);\n",
        "ae_Weighted = AE_Weighted(k,n,seed, filename=model_file)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in true_divide\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial shape of S: [196 242 105 228  57  73  33 160 180  43 142 243  13 129 105 202 185 236\n",
            "  95 127   9 252 136 120 207 240 150  59 108 175   6  21 151 177 158 243\n",
            "  26   4 124 255  68 110  60  27 255 194 228 163  87  14 168 255 135 169\n",
            " 157 220 196 151 252 184 148  64  96  96  30  65  57 234 211 106  71  91\n",
            "  17  89  17 128 195 246  13 137 198 108  78 209  84 241 147 215  44 103\n",
            " 217  50 124  46  41  48  69  56 209 159 179 203 146   8  85 162 150 102\n",
            " 149 155  75 155  38 168   4  55 220 106 223 160   0 136  46   8  87 182\n",
            " 168  29 126 199 225 161 132 184  42  37 162 135  11 226 136  69 172  68\n",
            "  43 169  14 123 151 123  32  46 162  70 246 245 235 226 155  27 198 162\n",
            " 145  75 196 185 122 235 243  47 218 100  30 183  56  18 147 136  91 136\n",
            "  31 249 193  15 151  49  13 139  59  99 202 167   0  40 144  55  23  79\n",
            " 173 207 133 236 237  69  42 222 106 252 111  37 155 225 251 134  90 148\n",
            "  48 254 143 159  51 152 191 180  70  22  35 206  51 227  18 251 207  88\n",
            "  99  21 167 123  53 172 169 198 189 116 216  37 136  93   1 167 246 142\n",
            "  10 223 233 154 190 118 192 112   2   6  96 253 188   7 200  57  66 108\n",
            "  66 125 166 157 107  82 229 100  64 232  49 161  50 210   4 170 179 157\n",
            " 169  89 232 169 133 224 143  21  58  54 147 249  99 214 139 193 171  61\n",
            " 193 126 242 188 155 225  99 194  83 200 191 123 251  78 151  67  10 150\n",
            "  89   1 149 107 109  36  99 236 195 148 162 127 204  85 216  64 145 230\n",
            "   2 188  93  91 173  58   8  98 186  92  48 116  85  99  46 111 129 173\n",
            " 181 135  92 222 129  77 226 108 224  67 105  58 230  54 223 234 220  43\n",
            "  61  79 170  80 250 229 171  55  13  20  72 198 197 207 152 109 149 206\n",
            " 124 178 253 255  77 144 116 180 186  17  49  49  49 216  97 112  86 126\n",
            " 156 103 212 129  77  56  17 153 115 140  95 186  85  18 144 184  91 163\n",
            " 159  65 125 158 245  39 145 148  93 132  44  72  46   1 227  36 223 179\n",
            " 177 106  21 110 115 104  37 201  31 240 112  75  77  45 198  71  20 217\n",
            "  63 124 195  51  89 212  40 105   3 101 216 116 136 117 149  70 164 244\n",
            "  11 223 203  19 118  28 131  42  74 191  91 203 217  81 106 162 100  61\n",
            "  30   0 248 193   3 238 135 203  96 122 103  11 181 206  60  24  30 106\n",
            "  66 216  46  62  90 128 230 145 222 198 247 182  72 167 122  66  78 153\n",
            " 216  96  52  76 120  41 212  10  70 169  14 192 125 134  67 157  22  28\n",
            " 163 103  27 233  60  85 245  24 216 239 253 191 109 252 196  36 172 217\n",
            " 227  62  37  36  44   4   5 210 202 132 161 136  73 182  62  95  34 166\n",
            "  72  19 185   2  92  28  74  15 125  41 179 185  18 184  97  57 188  12\n",
            " 154 196 184  76 152 185 121 104 201  19 219 126  78  64 193  25 174  66\n",
            "  39 138 157  95 170 184 183  88 205 116 119 193 186  17 140   0  35 250\n",
            " 255 143 195 138   7  12  95 108  90  57 236 127  31 230 186  31 199 107\n",
            " 150   2 192  25 247 185 242 192  98   8 206 162 210 250 254  24 107 182\n",
            " 109  67  75 153  96 158 190  43 231 210   4  91 164 205 254 181 167 105\n",
            " 118  24 199  65 106 216 140 245 230 180 127 217 153 138   3 136 207 196\n",
            " 191 243  26 136  50  21 208 181 238 214 201 147  25  41 235  34  42 128\n",
            " 190  61  17  71  91  21  78  11  48  97 162 194 121  10  47  46 210  29\n",
            " 214 140 198  93  96 124 198 126 235  14  11  20 234 111  79 133 106  22\n",
            "  82   2 211 203  52 249 123  12  39 235  77 141 218 198 170 255 159 190\n",
            " 241  89  65 166 224 240 211 242  18 135  82  81  79 218  42  93  87 125\n",
            " 103 183 202 239 120  87 198  70 129 198 134   7 237 117 196  38  37  66\n",
            "  56  39 180  32   8 143 145 236   8  22 107 114 207 248  91 159  79 220\n",
            "  45 247 130  64  98  23 144 223  60 177 143 195 225  71  13 223 155 177\n",
            " 156 202 236 107  39 238 110  10 116 245 206 219 179 142 159  18  87 217\n",
            "  47 125 222  35 213 141  21  93  68  74 169 255   4 176 247  60 143 235\n",
            "   2  45  53 147   3 206  87 132 228  90 241 212   0 244 230 217 111 120\n",
            "  35  76  49 248  54  98  71 238 210  99 175 229  19  29 125 104  14  77\n",
            " 201  92 221 220 155 205 242  22  35  13 236 226  11 132 233  23  23 161\n",
            "   7   6 219 203 142 250 231  40 238 175 181 200  25 131 152  93  45 215\n",
            " 163  13  14 198  17 243 120 156 230 136 172   0 100  37 124 123 163 216\n",
            " 125 152  95 135 127  24  59  37 235 141]\n",
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of x: Tensor(\"truediv:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n",
            "\n",
            "Batch Size: 1000, Learning Rate: 0.01, EbNodB: 40, Iterations: 1000\n",
            "0.994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: divide by zero encountered in true_divide\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial shape of S: [196 242 105 228  57  73  33 160 180  43 142 243  13 129 105 202 185 236\n",
            "  95 127   9 252 136 120 207 240 150  59 108 175   6  21 151 177 158 243\n",
            "  26   4 124 255  68 110  60  27 255 194 228 163  87  14 168 255 135 169\n",
            " 157 220 196 151 252 184 148  64  96  96  30  65  57 234 211 106  71  91\n",
            "  17  89  17 128 195 246  13 137 198 108  78 209  84 241 147 215  44 103\n",
            " 217  50 124  46  41  48  69  56 209 159 179 203 146   8  85 162 150 102\n",
            " 149 155  75 155  38 168   4  55 220 106 223 160   0 136  46   8  87 182\n",
            " 168  29 126 199 225 161 132 184  42  37 162 135  11 226 136  69 172  68\n",
            "  43 169  14 123 151 123  32  46 162  70 246 245 235 226 155  27 198 162\n",
            " 145  75 196 185 122 235 243  47 218 100  30 183  56  18 147 136  91 136\n",
            "  31 249 193  15 151  49  13 139  59  99 202 167   0  40 144  55  23  79\n",
            " 173 207 133 236 237  69  42 222 106 252 111  37 155 225 251 134  90 148\n",
            "  48 254 143 159  51 152 191 180  70  22  35 206  51 227  18 251 207  88\n",
            "  99  21 167 123  53 172 169 198 189 116 216  37 136  93   1 167 246 142\n",
            "  10 223 233 154 190 118 192 112   2   6  96 253 188   7 200  57  66 108\n",
            "  66 125 166 157 107  82 229 100  64 232  49 161  50 210   4 170 179 157\n",
            " 169  89 232 169 133 224 143  21  58  54 147 249  99 214 139 193 171  61\n",
            " 193 126 242 188 155 225  99 194  83 200 191 123 251  78 151  67  10 150\n",
            "  89   1 149 107 109  36  99 236 195 148 162 127 204  85 216  64 145 230\n",
            "   2 188  93  91 173  58   8  98 186  92  48 116  85  99  46 111 129 173\n",
            " 181 135  92 222 129  77 226 108 224  67 105  58 230  54 223 234 220  43\n",
            "  61  79 170  80 250 229 171  55  13  20  72 198 197 207 152 109 149 206\n",
            " 124 178 253 255  77 144 116 180 186  17  49  49  49 216  97 112  86 126\n",
            " 156 103 212 129  77  56  17 153 115 140  95 186  85  18 144 184  91 163\n",
            " 159  65 125 158 245  39 145 148  93 132  44  72  46   1 227  36 223 179\n",
            " 177 106  21 110 115 104  37 201  31 240 112  75  77  45 198  71  20 217\n",
            "  63 124 195  51  89 212  40 105   3 101 216 116 136 117 149  70 164 244\n",
            "  11 223 203  19 118  28 131  42  74 191  91 203 217  81 106 162 100  61\n",
            "  30   0 248 193   3 238 135 203  96 122 103  11 181 206  60  24  30 106\n",
            "  66 216  46  62  90 128 230 145 222 198 247 182  72 167 122  66  78 153\n",
            " 216  96  52  76 120  41 212  10  70 169  14 192 125 134  67 157  22  28\n",
            " 163 103  27 233  60  85 245  24 216 239 253 191 109 252 196  36 172 217\n",
            " 227  62  37  36  44   4   5 210 202 132 161 136  73 182  62  95  34 166\n",
            "  72  19 185   2  92  28  74  15 125  41 179 185  18 184  97  57 188  12\n",
            " 154 196 184  76 152 185 121 104 201  19 219 126  78  64 193  25 174  66\n",
            "  39 138 157  95 170 184 183  88 205 116 119 193 186  17 140   0  35 250\n",
            " 255 143 195 138   7  12  95 108  90  57 236 127  31 230 186  31 199 107\n",
            " 150   2 192  25 247 185 242 192  98   8 206 162 210 250 254  24 107 182\n",
            " 109  67  75 153  96 158 190  43 231 210   4  91 164 205 254 181 167 105\n",
            " 118  24 199  65 106 216 140 245 230 180 127 217 153 138   3 136 207 196\n",
            " 191 243  26 136  50  21 208 181 238 214 201 147  25  41 235  34  42 128\n",
            " 190  61  17  71  91  21  78  11  48  97 162 194 121  10  47  46 210  29\n",
            " 214 140 198  93  96 124 198 126 235  14  11  20 234 111  79 133 106  22\n",
            "  82   2 211 203  52 249 123  12  39 235  77 141 218 198 170 255 159 190\n",
            " 241  89  65 166 224 240 211 242  18 135  82  81  79 218  42  93  87 125\n",
            " 103 183 202 239 120  87 198  70 129 198 134   7 237 117 196  38  37  66\n",
            "  56  39 180  32   8 143 145 236   8  22 107 114 207 248  91 159  79 220\n",
            "  45 247 130  64  98  23 144 223  60 177 143 195 225  71  13 223 155 177\n",
            " 156 202 236 107  39 238 110  10 116 245 206 219 179 142 159  18  87 217\n",
            "  47 125 222  35 213 141  21  93  68  74 169 255   4 176 247  60 143 235\n",
            "   2  45  53 147   3 206  87 132 228  90 241 212   0 244 230 217 111 120\n",
            "  35  76  49 248  54  98  71 238 210  99 175 229  19  29 125 104  14  77\n",
            " 201  92 221 220 155 205 242  22  35  13 236 226  11 132 233  23  23 161\n",
            "   7   6 219 203 142 250 231  40 238 175 181 200  25 131 152  93  45 215\n",
            " 163  13  14 198  17 243 120 156 230 136 172   0 100  37 124 123 163 216\n",
            " 125 152  95 135 127  24  59  37 235 141]\n",
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of x: Tensor(\"truediv:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsoF-I7Rf7hy"
      },
      "source": [
        "### Plot of learned constellations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjGW8S_df7h0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "40598c78-cbb3-4e11-9249-f6fed9ca6d2f"
      },
      "source": [
        "import itertools\n",
        "def plot_constellation_2(ae, arr, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = ae.transmit(arr)\n",
        "        #marker = itertools.cycle(('1', '2', '3', '4', '^', '6', '7', '8'))\n",
        "        weights = [1,4,9,16,25,36,49,64]\n",
        "        #weights = [1,1,1,1,1,1,1,1]\n",
        "        #weights = [1,8,27,64,125,216,343,512]\n",
        "        #weights = [1,2,3,4,5,6,7,8]\n",
        "        #weights = [1,16,81,256,625, 1296, 2401, 4096]\n",
        "        #weights = [1,256,6561,65536, 390625, 1.6, 5.7, 16.7]\n",
        "        #x = ae.transmit(np.ones(ae.M)*n)\n",
        "        #print(ae.M)\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(ae.n_complex):\n",
        "            \n",
        "#            image = plt.figure(figsize=(6,6))\n",
        "            image = plt.plot()\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-2,2)\n",
        "            plt.ylim(-2,2)\n",
        "            #plt.show() \n",
        "            #plt.ion()\n",
        "            xshape = np.shape(x)\n",
        "            for i in range(xshape[0]):      \n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\")\n",
        "                plt.annotate(weights[i], (x[i,k,0],x[i,k,1]))\n",
        "\n",
        "\n",
        "                #plt.show()              \n",
        "                #plt.pause(1)\n",
        "                #plt.hold(True)\n",
        "            #image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            #image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            #image.suptitle('%d. complex symbol' % (k+1))\n",
        "            #image.canvas.draw()\n",
        "            #image.canvas.flush_events()\n",
        "            \n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "            #time.sleep(0.1)\n",
        "        return x, image\n",
        "\n",
        "#plot_constellation_2(ae,range(0,ae.M))\n",
        "plot_constellation_2(ae_Weighted, range(0, ae_Weighted.M))"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-126-ad5ab0cfffa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#plot_constellation_2(ae,range(0,ae.M))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mplot_constellation_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mae_Weighted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mae_Weighted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-126-ad5ab0cfffa9>\u001b[0m in \u001b[0;36mplot_constellation_2\u001b[0;34m(ae, arr, maxrange)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_constellation_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxrange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m'''Generate a plot of the current constellation'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;31m#marker = itertools.cycle(('1', '2', '3', '4', '^', '6', '7', '8'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m49\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-124-c87be910016b>\u001b[0m in \u001b[0;36mtransmit\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;34m'''Returns the transmitted sigals corresponding to message indices'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mend2end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mebnodb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMbROTHNMa79"
      },
      "source": [
        "**THE RMSE Calculations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7-W-Be2auJV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "b00d687d-5f92-4036-bc33-f1207a2ffd09"
      },
      "source": [
        "def rmse(predictions,targets):\n",
        "  return np.sqrt((np.subtract(predictions,targets) ** 2).mean())   \n",
        "\n",
        "tr_hat = ae.end2end(len(tr), 40, tr)\n",
        "print(np.shape(tr))\n",
        "print(np.shape(tr_hat))\n",
        "print(tr_hat)\n",
        "\n",
        "\n",
        "tr_hat_w = ae_Weighted.end2end(len(tr), 7, tr)\n",
        "\n",
        "rmse_uw_0 = rmse(([tr_hat[x] for x in s_ind_0]), ([tr[x] for x in s_ind_0]))\n",
        "rmse_uw_1 = rmse(([tr_hat[x] for x in s_ind_1]), ([tr[x] for x in s_ind_1]))\n",
        "rmse_uw_2 = rmse(([tr_hat[x] for x in s_ind_2]), ([tr[x] for x in s_ind_2]))\n",
        "rmse_uw_3 = rmse(([tr_hat[x] for x in s_ind_3]), ([tr[x] for x in s_ind_3]))\n",
        "rmse_uw_4 = rmse(([tr_hat[x] for x in s_ind_4]), ([tr[x] for x in s_ind_4]))\n",
        "rmse_uw_5 = rmse(([tr_hat[x] for x in s_ind_5]), ([tr[x] for x in s_ind_5]))\n",
        "rmse_uw_6 = rmse(([tr_hat[x] for x in s_ind_6]), ([tr[x] for x in s_ind_6]))\n",
        "rmse_uw_7 = rmse(([tr_hat[x] for x in s_ind_7]), ([tr[x] for x in s_ind_7]))\n",
        "\n",
        "\n",
        "rmse_w_0 = rmse(([tr_hat_w[x] for x in s_ind_0]), ([tr[x] for x in s_ind_0]))\n",
        "rmse_w_1 = rmse(([tr_hat_w[x] for x in s_ind_1]), ([tr[x] for x in s_ind_1]))\n",
        "rmse_w_2 = rmse(([tr_hat_w[x] for x in s_ind_2]), ([tr[x] for x in s_ind_2]))\n",
        "rmse_w_3 = rmse(([tr_hat_w[x] for x in s_ind_3]), ([tr[x] for x in s_ind_3]))\n",
        "rmse_w_4 = rmse(([tr_hat_w[x] for x in s_ind_4]), ([tr[x] for x in s_ind_4]))\n",
        "rmse_w_5 = rmse(([tr_hat_w[x] for x in s_ind_5]), ([tr[x] for x in s_ind_5]))\n",
        "rmse_w_6 = rmse(([tr_hat_w[x] for x in s_ind_6]), ([tr[x] for x in s_ind_6]))\n",
        "rmse_w_7 = rmse(([tr_hat_w[x] for x in s_ind_7]), ([tr[x] for x in s_ind_7]))\n",
        "\n",
        "\n",
        "\n",
        "rmse_uw = [rmse_uw_0, rmse_uw_1, rmse_uw_2, rmse_uw_3, rmse_uw_4, rmse_uw_5, rmse_uw_6, rmse_uw_7]\n",
        "rmse_w = [rmse_w_0, rmse_w_1, rmse_w_2, rmse_w_3, rmse_w_4, rmse_w_5, rmse_w_6, rmse_w_7]\n",
        "message = [0,1,2,3,4,5,6,7]\n",
        "\n",
        "plt.plot(message, rmse_uw, '-o');\n",
        "plt.plot(message, rmse_w, '-*');\n",
        "plt.xlabel('Message Bit')\n",
        "plt.ylabel('RMSE')\n",
        "print(rmse_uw_0)\n",
        "plt.legend(['Unweighted', 'Weighted(^2)'])"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-4c6d566a63e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtr_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend2end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-120-b7a5e29f499a>\u001b[0m in \u001b[0;36mend2end\u001b[0;34m(self, batch_size, ebnodb, input_s)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mend2end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mebnodb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;34m'''Returns the transmitted sigals corresponding to message indices'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'correct_s_hat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_e2e_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mebnodb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;31m#print(self.sess.run(self.vars['correct_s_hat'], feed_dict={self.vars['s']: input_s}))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-120-b7a5e29f499a>\u001b[0m in \u001b[0;36mgen_e2e_feed_dict\u001b[0;34m(self, batch_size, ebnodb, s_input)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'noise_std'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEbNo2Sigma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mebnodb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         }   \n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxostCb-vY1i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "a61f831a-c85c-4ea9-f9f0-68e7ef305ab2"
      },
      "source": [
        "ae.plot_constellation();"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-128-2d5d5bfc5e9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_constellation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-120-b7a5e29f499a>\u001b[0m in \u001b[0;36mplot_constellation\u001b[0;34m(self, maxrange)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplot_constellation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxrange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;34m'''Generate a plot of the current constellation'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmaxrange\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mmaxrange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-120-b7a5e29f499a>\u001b[0m in \u001b[0;36mtransmit\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;34m'''Returns the transmitted sigals corresponding to message indices'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mend2end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mebnodb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtoc7OKCUgko"
      },
      "source": [
        "# 8-PSK Modulation\n",
        "#8PSK constellation \n",
        "#Demodulation matrx\n",
        "#Qfunction \n",
        "\n",
        "import numpy as np\n",
        "from scipy import special\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import subprocess\n",
        "import shlex\n",
        "\n",
        "\n",
        "#Generating constellation points\n",
        "s = np.zeros((8,2))\n",
        "s_comp = np.zeros((8,1))+1j*np.zeros((8,1))\n",
        "for i in range(8):\n",
        "\ts[i,:] = np.array(([np.cos(i*2*np.pi/8),np.sin(i*2*np.pi/8)])) #vector\n",
        "\ts_comp[i] = s[i,0]+1j*s[i,1] #equivalent complex number\n",
        "\n",
        "#Generating demodulation matrix\n",
        "A = np.zeros((8,2,2))\n",
        "A[0,:,:] = np.array(([np.sqrt(2)-1,1],[np.sqrt(2)-1,-1]))\n",
        "A[1,:,:] = np.array(([np.sqrt(2)+1,-1],[-(np.sqrt(2)-1),1]))\n",
        "A[2,:,:] = np.array(([-(np.sqrt(2)+1),1],[np.sqrt(2)+1,1]))\n",
        "A[3,:,:] = np.array(([np.sqrt(2)-1,1],[-(np.sqrt(2)+1),-1]))\n",
        "A[4,:,:] = np.array(([-(np.sqrt(2)-1),-1],[-(np.sqrt(2)-1),1]))\n",
        "A[5,:,:] = np.array(([-(np.sqrt(2)+1),1],[np.sqrt(2)-1,-1]))\n",
        "A[6,:,:] = np.array(([np.sqrt(2)+1,-1],[-(np.sqrt(2)+1),-1]))\n",
        "A[7,:,:] = np.array(([-(np.sqrt(2)-1),-1],[np.sqrt(2)+1,1]))\n",
        "\n",
        "#Gray code\n",
        "gray = np.zeros((8,3))\n",
        "gray[0,:] = np.array(([0,0,0]))\n",
        "gray[1,:] = np.array(([0,0,1]))\n",
        "gray[2,:] = np.array(([0,1,1]))\n",
        "gray[3,:] = np.array(([0,1,0]))\n",
        "gray[4,:] = np.array(([1,1,0]))\n",
        "gray[5,:] = np.array(([1,1,1]))\n",
        "gray[6,:] = np.array(([1,0,1]))\n",
        "gray[7,:] = np.array(([1,0,0]))\n",
        "\n",
        "\n",
        "#Q-function\n",
        "def qfunc(x):\n",
        "\treturn 0.5*special.erfc(x/np.sqrt(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LDujO11Ulo5"
      },
      "source": [
        "def decode(vec_comp):\n",
        "\tvec = np.zeros((2,1))\n",
        "\tvec[0] = np.real(vec_comp)\n",
        "\tvec[1] = np.imag(vec_comp)\n",
        "\tfor i in range(8):\n",
        "\t\ty = A[i,:,:]@vec\n",
        "\t\tif (y [0] >= 0) and (y[1] >= 0):\n",
        "\t\t\treturn s_comp[i]\n",
        "\n",
        "#Extracting bits from demodulated symbols\n",
        "def detect(vec_comp):\n",
        "\tvec = np.zeros((2,1))\n",
        "\tvec[0] = np.real(vec_comp)\n",
        "\tvec[1] = np.imag(vec_comp)\n",
        "\tfor i in range(8):\n",
        "\t\tif s[i,0]==vec[0] and s[i,1] == vec[1]:\n",
        "\t\t\treturn gray[i,:]\n",
        "\n",
        "#Demodulating symbol stream from received noisy  symbols\n",
        "def rx_symb(mat):\n",
        "\tlen = mat.shape[1]\n",
        "\trx_symb_stream = []\n",
        "\tfor i in range(len):\n",
        "\t\trx_symb_stream.append(decode(mat[:,i]))\n",
        "\treturn rx_symb_stream\n",
        "\n",
        "#Getting received bit stream from demodulated symbols\n",
        "def rx_bit(mat):\n",
        "\tlen = mat.shape[1]\n",
        "\trx_bit_stream = []\n",
        "\tfor i in range(len):\n",
        "\t\trx_bit_stream.append(detect(mat[:,i]))\n",
        "\treturn rx_bit_stream"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPu3HX_SUov_"
      },
      "source": [
        "#Generates a bitstream\n",
        "def bitstream(n):\n",
        "\treturn np.random.randint(0,2,n)\n",
        "\n",
        "#Converts bits to 8-PSK symbols using gray code\n",
        "def mapping(b0,b1,b2):\n",
        "\tif (b0 == 0 and b1 == 0 and b2 == 0):\n",
        "\t\treturn s[0,:]\n",
        "\telif (b0 == 0 and b1 == 0 and b2 == 1):\n",
        "\t\treturn s[1,:]\n",
        "\telif (b0 == 0 and b1 == 1 and b2 == 1):\n",
        "\t\treturn s[2,:]\n",
        "\telif (b0 == 0 and b1 == 1 and b2 == 0):\n",
        "\t\treturn s[3,:]\n",
        "\telif( b0 == 1 and b1 == 1 and b2 == 0):\n",
        "\t\treturn s[4,:]\n",
        "\telif(b0==1 and b1 == 1 and b2 == 1):\n",
        "\t\treturn s[5,:]\n",
        "\telif(b0==1 and b1 == 0 and b2 == 1):\n",
        "\t\treturn s[6,:]\n",
        "\telif(b0==1 and b1 == 0 and b2 == 0):\n",
        "\t\treturn s[7,:]\n",
        "\n",
        "\n",
        "#Converts bitstream to 8-PSK symbol stream\n",
        "def symb(bits):\n",
        "\tsymbol =[]\n",
        "\ti = 0\n",
        "\twhile(1):\n",
        "\t\ttry:\n",
        "\t\t\tsymbol.append(mapping(bits[i],bits[i+1],bits[i+2]))\n",
        "\t\t\ti = i+3\n",
        "\t\texcept IndexError:\n",
        "\t\t\treturn symbol\n",
        "\n",
        "#Converts bitstream to 8-PSK complex symbol stream\n",
        "def CompSymb(bits):\n",
        "\tsymbols_lst = symb(bits)\n",
        "\tsymbols = np.array(symbols_lst).T #Symbol vectors\n",
        "\tsymbols_comp = symbols[0,:]+1j*symbols[1,:] #Equivalent complex symbols\n",
        "\treturn symbols_comp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPNtUtWBUryp"
      },
      "source": [
        "\n",
        "#SNR range\n",
        "snrlen=15\n",
        "\n",
        "#SNR in dB and actual per bit \n",
        "#(Check Proakis for factor of 6)\n",
        "snr_db = np.linspace(0,snrlen,snrlen)\n",
        "snr = 6*10**(0.1*snr_db)\n",
        "\n",
        "#Bitstream size\n",
        "bitsimlen = 99999\n",
        "\n",
        "#Symbol stream size\n",
        "simlen = bitsimlen //3\n",
        "\n",
        "#Generating bitstream\n",
        "bits = bitstream(bitsimlen)\n",
        "\n",
        "#Converting bits to Gray coded 8-PSK symbols\n",
        "#Intermediate steps  required for converting list to\n",
        "#numpy matrix\n",
        "symbols_lst = symb(bits)\n",
        "symbols = np.array(symbols_lst).T #Symbol vectors\n",
        "symbols_comp = symbols[0,:]+1j*symbols[1,:] #Equivalent complex symbols\n",
        "\n",
        "ser =[]\n",
        "ser_anal=[]\n",
        "ber = []\n",
        "\n",
        "#SNRloop\n",
        "for k in range(0,snrlen):\n",
        "\treceived = []\n",
        "\tt=0\n",
        "\t#Complex noise\n",
        "\tnoise_comp = np.random.normal(0,1,simlen)+1j*np.random.normal(0,1,simlen)\n",
        "\t#Generating complex received symbols\n",
        "  #fade_comp = np.random.normal(0,1,simlen)+1j*np.random.normal(0,1,simlen)\n",
        "  #fade_comp = np.abs(fade_comp)\n",
        "  #fade_comp = np.math.sqrt(1/2)*fade_comp\n",
        "\ty_comp = np.sqrt(snr[k])*symbols_comp +noise_comp\n",
        "\tbrx = []\n",
        "\tfor i in range(simlen):\n",
        "\t\tsrx_comp = decode(y_comp[i]) #Received Symbol\n",
        "\t\tbrx.append(detect(srx_comp))  #Received Bits\n",
        "\t\tif symbols_comp[i]==srx_comp:\n",
        "\t\t\tt+=1; #Counting symbol errors\n",
        "\t#Evaluating SER\n",
        "\tser.append(1-(t/33334.0))\n",
        "\tser_anal.append(2*qfunc((np.sqrt(snr[k]))*np.sin(np.pi/8)))\n",
        "\t#Received bitstream\n",
        "\tbrx=np.array(brx).flatten()\n",
        "\t#Evaluating BER\n",
        "\tbit_diff = bits-brx\n",
        "\tber.append(1-len(np.where(bit_diff == 0)[0])/bitsimlen)\n",
        "\n",
        "\n",
        "\n",
        "#Plots\n",
        "plt.semilogy(snr_db,ser_anal,label='SER Analysis')\n",
        "plt.semilogy(snr_db,ser,'o',label='SER Sim')\n",
        "plt.semilogy(snr_db,ber,label='BER Sim')\n",
        "plt.xlabel('SNR$\\\\left(\\\\frac{E_b}{N_0}\\\\right)$')\n",
        "plt.ylabel('$P_e$')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFZCQNe9f7iD"
      },
      "source": [
        "### BLER Simulations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wukzCBJff7iE"
      },
      "source": [
        " ebnodbs = np.linspace(0,14,15)\n",
        "BLER_8PSK = [0.3478959, 0.2926128, 0.2378847, 0.1854187, 0.1372344, 0.0953536, 0.0614003, 0.0360195, 0.0185215, 0.0082433, 0.0030178, 0.0008626, 0.0001903, 0.0000289, 0.0000027, ]\n",
        "blers = ae.bler_sim(ebnodbs, 1000000, 1);\n",
        "ae.plot_bler(ebnodbs, blers);\n",
        "blers_w = ae_Weighted.bler_sim(ebnodbs, 1000000, 1);\n",
        "#ae_Weighted.plot_bler(ebnodbs, blers_w);\n",
        "plt.plot(ebnodbs, blers_w)\n",
        "plt.semilogy(snr_db,ser,'o')\n",
        "plt.plot(ebnodbs,BLER_8PSK);\n",
        "plt.legend(['Autoencoder (Rayleigh+AWGN)', 'Weighted Autoencoder(Rayleigh+AWGN)', 'SER Sim(AWGN)', '8PSK(AWGN)'], prop={'size': 16}, loc='lower left');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMl-Rljrf7iR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}