{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Adaptive_SNR_Testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neelabhro/Deep-Learning-based-Wireless-Communications/blob/main/Adaptive_SNR_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIoYASfEf7go"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKl9e4A0H1lk",
        "outputId": "a9fcdbfc-5ec3-4baf-e53d-23704b228821"
      },
      "source": [
        "# magic command to use TF 1.X in colaboraty when importing tensorflow\n",
        "%tensorflow_version 1.x \n",
        "import tensorflow as tf                       # imports the tensorflow library to the python kernel\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)    # sets the amount of debug information from TF (INFO, WARNING, ERROR)\n",
        "import time\n",
        "import random\n",
        "print(\"Using tensorflow version:\", tf.__version__)\n",
        "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using tensorflow version: 1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiCDuiGqf7gr"
      },
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "import math\n",
        "pi = tf.constant(math.pi)"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrLsO1Nxf7g3"
      },
      "source": [
        "#### System parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spN0MeqFD7x-",
        "outputId": "97da1a45-40dc-4cf3-a8fc-3adaaf120575"
      },
      "source": [
        "batch_size = 1000\n",
        "########## BIT FLIPPING ON\n",
        "#print(tr)\n",
        "#plt.plot(t, tr)\n",
        "T1 = random.randint(1, 10)\n",
        "T2 = random.randint(1, 10)\n",
        "M = 256\n",
        "t = np.linspace(0, 1, batch_size)\n",
        "triangle1 = signal.sawtooth(T1 * np.pi * 5 * t, 0.5)\n",
        "triangle1 = M/2*(triangle1)\n",
        "triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "triangle2 = signal.sawtooth(T2 * np.pi * 5 * t, 0.5)\n",
        "triangle2 = M/2*(triangle2)\n",
        "triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "triangle3 = triangle1 + triangle2\n",
        "#triangle3 = triangle3/15*(7)\n",
        "tr = triangle3\n",
        "#bins = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])\n",
        "bins = np.arange(M-1)\n",
        "tr = np.digitize(triangle3, bins, right=True)\n",
        "#plt.plot(t, triangle3)\n",
        "\n",
        "#tr = np.flip(tr)\n",
        "#print(tr)\n",
        "tr = np.floor(np.random.uniform(0,M, 10000))\n",
        "print(tr)\n",
        "#replacements = {0:7, 1:6, 2:5, 3:4, 4:3, 5:2, 6:1, 7:0}\n",
        "#replacements = {0:15, 1:14, 2:13, 3:12, 4:11, 5:10, 6:9, 7:8, 8:7, 9:6, 10:5, 11:4, 12:3, 13:2, 14:1, 15:0}\n",
        "rp1 = np.arange(M)\n",
        "rp2 = np.flip(np.arange(M))\n",
        "replacements = dict(zip(rp1,rp2))\n",
        "#print(rp2)\n",
        "replacer = replacements.get\n",
        "tr = ([replacer(n, n) for n in tr])\n",
        "\n",
        "#tr2 = ([replacer(n, n) for n in triangle3])\n",
        "print(tr)\n",
        "#plt.plot(t, tr)\n",
        "#plt.legend(['Input signal', 'Quantized and bit flipped'])\n",
        "\n",
        "#s_ind = np.empty(shape=(M,batch_size))\n",
        "s_ind = {}\n",
        "for j in range(M):\n",
        "  s_ind[j] = [i for i, x in enumerate(tr) if x == j]\n",
        "\n",
        "#print([tr[x] for x in s_ind_0])"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 71.  72. 191. ... 198. 112. 210.]\n",
            "[184, 183, 64, 53, 115, 239, 152, 18, 74, 127, 89, 122, 198, 121, 226, 222, 138, 250, 60, 99, 115, 36, 174, 14, 15, 220, 41, 80, 234, 233, 229, 195, 61, 73, 53, 121, 28, 125, 107, 228, 241, 89, 55, 59, 117, 142, 59, 63, 203, 205, 131, 101, 176, 188, 249, 30, 202, 128, 153, 236, 236, 133, 121, 125, 247, 243, 53, 171, 178, 23, 204, 123, 109, 216, 14, 146, 231, 166, 12, 254, 169, 150, 120, 142, 148, 32, 218, 48, 13, 60, 99, 181, 28, 162, 132, 206, 3, 65, 140, 232, 174, 131, 84, 217, 172, 76, 47, 127, 58, 242, 179, 216, 87, 35, 88, 72, 23, 190, 178, 217, 45, 152, 39, 38, 159, 16, 199, 8, 58, 42, 8, 9, 54, 46, 235, 65, 70, 193, 12, 241, 213, 254, 47, 126, 97, 193, 24, 179, 106, 143, 226, 234, 171, 74, 49, 38, 9, 20, 197, 185, 38, 58, 15, 136, 208, 53, 180, 19, 61, 124, 241, 222, 148, 43, 47, 190, 51, 105, 161, 111, 150, 216, 83, 66, 78, 98, 109, 84, 176, 1, 41, 204, 77, 190, 35, 163, 117, 59, 37, 62, 225, 255, 0, 245, 230, 42, 235, 20, 26, 79, 201, 115, 128, 13, 71, 20, 96, 19, 70, 239, 59, 48, 188, 165, 11, 174, 190, 91, 195, 30, 79, 86, 120, 238, 155, 118, 77, 166, 207, 81, 143, 76, 175, 229, 199, 187, 33, 167, 216, 4, 46, 158, 152, 143, 162, 6, 184, 13, 16, 247, 63, 198, 40, 185, 172, 19, 213, 110, 187, 145, 156, 248, 188, 160, 18, 113, 55, 34, 52, 220, 33, 36, 0, 155, 64, 84, 87, 49, 229, 65, 177, 165, 19, 51, 4, 251, 105, 236, 118, 237, 9, 217, 215, 208, 33, 8, 119, 21, 89, 199, 59, 67, 115, 36, 176, 201, 68, 24, 66, 4, 186, 65, 125, 249, 227, 250, 129, 218, 238, 194, 228, 0, 22, 62, 157, 94, 101, 67, 197, 241, 114, 104, 89, 59, 186, 244, 208, 245, 152, 128, 61, 117, 79, 245, 7, 152, 183, 135, 205, 56, 18, 33, 164, 91, 171, 95, 81, 8, 222, 74, 161, 119, 151, 28, 207, 183, 225, 139, 243, 224, 104, 72, 99, 192, 162, 168, 9, 182, 223, 79, 191, 75, 135, 178, 170, 176, 213, 167, 30, 144, 181, 168, 20, 56, 255, 29, 141, 150, 180, 146, 64, 14, 207, 64, 176, 44, 117, 94, 162, 58, 51, 217, 196, 42, 111, 148, 107, 203, 162, 200, 109, 35, 109, 131, 175, 134, 92, 153, 176, 177, 218, 124, 127, 210, 99, 210, 58, 88, 172, 159, 132, 185, 247, 116, 96, 122, 203, 185, 56, 106, 170, 169, 10, 113, 201, 125, 14, 67, 238, 211, 66, 197, 16, 61, 192, 178, 58, 9, 175, 15, 4, 230, 205, 254, 70, 140, 106, 75, 40, 43, 209, 80, 95, 137, 125, 32, 56, 118, 205, 112, 224, 81, 204, 216, 150, 201, 189, 2, 104, 232, 248, 196, 22, 235, 160, 213, 25, 110, 60, 186, 139, 247, 80, 230, 66, 196, 142, 197, 63, 215, 176, 69, 89, 21, 103, 118, 235, 154, 87, 155, 154, 128, 3, 204, 112, 234, 158, 98, 121, 11, 101, 193, 38, 241, 125, 148, 137, 135, 17, 62, 37, 24, 117, 79, 69, 231, 97, 100, 156, 143, 244, 28, 40, 15, 222, 113, 0, 154, 15, 204, 235, 106, 92, 93, 56, 77, 134, 101, 72, 225, 86, 128, 146, 104, 187, 248, 160, 21, 226, 221, 212, 119, 180, 5, 75, 55, 81, 29, 195, 226, 16, 50, 194, 87, 63, 22, 95, 127, 220, 215, 101, 202, 133, 152, 227, 117, 117, 64, 246, 125, 77, 245, 253, 167, 138, 140, 3, 95, 42, 227, 87, 199, 57, 98, 146, 27, 207, 208, 119, 187, 149, 254, 40, 121, 199, 39, 202, 245, 243, 181, 67, 22, 90, 43, 216, 73, 70, 189, 101, 121, 191, 65, 19, 254, 237, 191, 118, 32, 116, 33, 197, 169, 56, 240, 227, 245, 158, 27, 157, 173, 186, 177, 21, 229, 69, 34, 194, 239, 235, 121, 93, 116, 119, 167, 64, 15, 24, 32, 133, 97, 129, 155, 153, 200, 93, 110, 98, 124, 123, 46, 19, 118, 183, 44, 106, 112, 9, 232, 183, 4, 98, 137, 235, 126, 61, 13, 212, 223, 117, 3, 20, 172, 96, 130, 55, 154, 87, 79, 151, 225, 238, 30, 65, 60, 126, 130, 74, 95, 110, 45, 248, 199, 57, 192, 136, 13, 42, 233, 153, 153, 217, 202, 163, 175, 21, 142, 51, 0, 79, 202, 95, 8, 214, 77, 212, 184, 198, 161, 156, 106, 91, 249, 127, 89, 243, 17, 163, 227, 236, 153, 170, 16, 207, 43, 17, 113, 126, 67, 238, 220, 254, 132, 220, 142, 197, 47, 66, 191, 103, 108, 24, 79, 170, 204, 60, 231, 168, 123, 30, 18, 20, 222, 150, 47, 141, 195, 219, 140, 128, 13, 73, 101, 143, 215, 42, 35, 209, 96, 107, 3, 76, 147, 209, 66, 144, 198, 42, 125, 230, 108, 227, 216, 187, 231, 214, 158, 19, 171, 96, 81, 251, 141, 240, 109, 211, 94, 224, 115, 42, 254, 204, 120, 192, 174, 29, 228, 28, 255, 157, 183, 21, 19, 26, 69, 98, 169, 144, 108, 204, 26, 215, 52, 247, 253, 143, 73, 150, 201, 117, 92, 8, 167, 46, 22, 244, 165, 13, 224, 243, 2, 96, 121, 215, 179, 104, 31, 162, 160, 135, 157, 24, 202, 202, 210, 191, 29, 195, 107, 151, 194, 145, 77, 89, 122, 156, 37, 177, 61, 211, 119, 207, 77, 93, 90, 194, 87, 212, 17, 11, 210, 229, 42, 20, 146, 80, 65, 97, 172, 10, 116, 183, 33, 5, 79, 144, 93, 249, 97, 100, 241, 50, 255, 166, 180, 39, 34, 31, 166, 243, 177, 184, 62, 237, 102, 116, 132, 176, 184, 12, 237, 204, 1, 85, 197, 80, 155, 168, 131, 200, 89, 169, 100, 67, 23, 175, 191, 42, 156, 145, 136, 92, 216, 148, 55, 238, 116, 24, 156, 186, 233, 200, 97, 104, 72, 121, 202, 116, 15, 8, 47, 93, 64, 234, 106, 3, 16, 234, 237, 153, 90, 111, 210, 16, 183, 109, 226, 63, 75, 7, 55, 35, 187, 119, 81, 227, 106, 73, 131, 99, 181, 98, 24, 88, 226, 60, 243, 221, 44, 211, 61, 71, 102, 122, 232, 43, 41, 192, 254, 154, 217, 190, 33, 131, 165, 115, 138, 100, 109, 234, 229, 174, 106, 86, 106, 173, 98, 42, 237, 29, 70, 148, 221, 207, 47, 193, 108, 26, 14, 33, 172, 197, 100, 106, 142, 118, 108, 224, 227, 182, 42, 130, 158, 240, 162, 193, 237, 108, 102, 77, 217, 23, 162, 20, 20, 179, 6, 154, 3, 63, 109, 243, 43, 204, 135, 178, 109, 71, 30, 202, 117, 246, 55, 126, 245, 237, 196, 142, 150, 155, 224, 181, 190, 220, 0, 151, 66, 156, 210, 141, 36, 14, 184, 170, 94, 79, 115, 40, 253, 255, 214, 115, 231, 150, 245, 168, 56, 172, 232, 44, 53, 47, 208, 16, 154, 14, 137, 191, 192, 231, 32, 244, 28, 238, 22, 81, 226, 242, 46, 215, 121, 123, 250, 175, 42, 51, 13, 145, 113, 7, 238, 241, 218, 21, 127, 183, 159, 67, 92, 176, 252, 5, 105, 238, 128, 117, 158, 48, 210, 32, 156, 243, 219, 252, 221, 237, 241, 224, 3, 245, 60, 41, 69, 148, 237, 15, 93, 188, 184, 245, 176, 30, 105, 75, 151, 21, 188, 202, 8, 150, 158, 126, 37, 24, 115, 177, 3, 244, 240, 25, 230, 67, 2, 162, 192, 61, 19, 105, 99, 174, 27, 220, 10, 56, 249, 230, 126, 116, 134, 47, 209, 201, 37, 250, 69, 5, 13, 55, 35, 90, 101, 160, 13, 196, 202, 222, 152, 224, 209, 12, 192, 128, 93, 250, 217, 186, 146, 41, 165, 131, 169, 212, 179, 0, 43, 134, 69, 221, 46, 170, 166, 14, 19, 68, 41, 4, 250, 149, 59, 198, 140, 17, 57, 20, 69, 153, 226, 38, 220, 101, 137, 203, 170, 65, 167, 49, 20, 205, 32, 196, 13, 66, 222, 1, 243, 198, 38, 199, 69, 227, 114, 23, 125, 81, 108, 72, 64, 123, 158, 51, 248, 133, 237, 112, 32, 167, 97, 115, 8, 83, 143, 243, 1, 24, 65, 139, 67, 123, 163, 166, 20, 169, 116, 165, 248, 90, 214, 223, 73, 30, 172, 73, 87, 190, 98, 218, 153, 57, 81, 251, 122, 219, 140, 68, 84, 129, 203, 139, 227, 78, 111, 119, 183, 163, 135, 144, 176, 141, 185, 83, 61, 78, 20, 16, 83, 136, 29, 251, 122, 216, 9, 243, 69, 252, 195, 55, 118, 222, 250, 115, 244, 117, 120, 104, 232, 13, 88, 136, 245, 149, 105, 25, 208, 185, 29, 126, 25, 137, 54, 58, 131, 212, 109, 45, 242, 170, 222, 214, 23, 83, 130, 22, 58, 82, 169, 9, 195, 69, 97, 181, 61, 197, 208, 122, 3, 192, 142, 132, 181, 109, 131, 255, 195, 108, 226, 195, 57, 25, 158, 72, 66, 76, 24, 78, 11, 141, 33, 209, 7, 125, 94, 121, 181, 130, 94, 78, 154, 41, 210, 226, 143, 203, 218, 67, 113, 254, 167, 156, 5, 13, 106, 122, 40, 173, 217, 9, 129, 200, 144, 135, 200, 36, 208, 45, 205, 233, 179, 1, 91, 52, 196, 148, 27, 123, 28, 205, 79, 95, 238, 230, 238, 135, 48, 130, 119, 128, 132, 64, 246, 204, 169, 247, 157, 211, 31, 158, 94, 66, 187, 98, 37, 107, 199, 147, 44, 50, 130, 229, 1, 213, 184, 136, 156, 97, 56, 76, 70, 222, 242, 217, 40, 182, 218, 243, 5, 68, 61, 249, 242, 114, 159, 229, 227, 225, 101, 134, 100, 117, 143, 89, 40, 130, 1, 12, 78, 134, 58, 126, 195, 20, 167, 85, 119, 37, 68, 200, 89, 195, 127, 207, 80, 18, 58, 107, 58, 97, 255, 142, 209, 92, 174, 2, 135, 84, 41, 15, 39, 35, 149, 56, 0, 16, 45, 105, 208, 7, 137, 74, 69, 228, 232, 0, 186, 58, 64, 172, 108, 101, 137, 39, 49, 189, 87, 131, 58, 194, 123, 86, 163, 67, 74, 230, 70, 61, 196, 84, 37, 147, 220, 87, 66, 134, 38, 103, 135, 120, 151, 120, 0, 4, 54, 158, 46, 46, 242, 49, 120, 132, 184, 90, 196, 224, 119, 154, 158, 40, 140, 243, 35, 130, 11, 202, 60, 39, 166, 179, 69, 222, 14, 109, 35, 226, 69, 104, 71, 244, 238, 158, 148, 86, 26, 66, 102, 110, 114, 50, 137, 149, 74, 162, 153, 138, 0, 247, 241, 77, 208, 176, 64, 102, 138, 164, 217, 43, 85, 43, 221, 152, 187, 70, 9, 47, 205, 61, 199, 200, 49, 204, 15, 208, 0, 226, 180, 205, 213, 200, 208, 100, 87, 226, 163, 39, 91, 14, 148, 223, 240, 181, 152, 149, 51, 110, 76, 116, 174, 113, 22, 193, 54, 23, 210, 41, 43, 176, 148, 185, 166, 48, 134, 41, 9, 142, 63, 19, 228, 67, 66, 217, 15, 55, 150, 9, 248, 135, 222, 102, 14, 245, 227, 60, 253, 46, 215, 104, 15, 58, 60, 34, 20, 55, 68, 107, 41, 110, 197, 160, 44, 146, 253, 153, 35, 46, 248, 1, 242, 23, 231, 114, 96, 72, 224, 2, 111, 153, 173, 191, 18, 166, 30, 17, 244, 155, 122, 42, 223, 207, 100, 255, 52, 175, 147, 236, 5, 31, 88, 46, 87, 145, 231, 168, 80, 35, 66, 52, 162, 6, 242, 141, 41, 126, 195, 94, 216, 220, 11, 179, 226, 190, 32, 148, 44, 116, 48, 222, 77, 167, 94, 90, 126, 186, 102, 163, 162, 236, 5, 43, 169, 146, 46, 72, 128, 86, 14, 68, 138, 87, 206, 140, 73, 241, 82, 39, 123, 95, 22, 251, 35, 88, 89, 2, 105, 25, 134, 79, 230, 96, 86, 121, 44, 104, 254, 59, 185, 21, 26, 174, 151, 173, 121, 28, 79, 126, 243, 160, 31, 141, 167, 66, 209, 51, 133, 64, 146, 63, 32, 33, 217, 143, 235, 251, 17, 107, 111, 97, 36, 112, 31, 34, 79, 49, 189, 137, 104, 69, 91, 127, 54, 20, 128, 87, 38, 65, 56, 52, 106, 135, 207, 197, 208, 125, 89, 101, 69, 102, 241, 238, 77, 28, 253, 224, 126, 142, 98, 222, 169, 13, 90, 11, 138, 220, 216, 132, 108, 108, 120, 230, 115, 218, 250, 204, 150, 204, 23, 241, 45, 203, 162, 207, 70, 171, 214, 133, 216, 167, 93, 93, 73, 56, 209, 213, 67, 250, 189, 207, 33, 15, 236, 161, 181, 194, 219, 212, 157, 41, 17, 19, 127, 235, 124, 210, 236, 80, 108, 172, 123, 104, 242, 41, 249, 242, 97, 43, 208, 40, 154, 96, 204, 228, 116, 95, 218, 150, 111, 248, 34, 142, 69, 225, 79, 241, 252, 151, 138, 2, 59, 250, 203, 97, 75, 98, 173, 145, 30, 101, 187, 21, 221, 111, 156, 108, 39, 163, 44, 2, 194, 248, 164, 110, 109, 210, 222, 74, 241, 63, 93, 23, 2, 242, 147, 242, 121, 175, 199, 69, 238, 165, 29, 139, 2, 212, 149, 222, 9, 82, 145, 110, 203, 114, 250, 209, 237, 153, 139, 46, 80, 59, 180, 24, 216, 109, 203, 143, 205, 179, 188, 143, 222, 20, 167, 251, 175, 19, 107, 102, 49, 117, 214, 57, 172, 238, 189, 144, 57, 171, 128, 35, 153, 240, 35, 37, 179, 161, 152, 218, 90, 177, 161, 55, 125, 88, 150, 175, 226, 158, 226, 6, 107, 164, 85, 96, 238, 109, 109, 171, 201, 59, 160, 143, 234, 95, 29, 140, 173, 192, 247, 154, 7, 97, 96, 117, 65, 165, 197, 147, 46, 243, 243, 172, 185, 101, 196, 215, 93, 16, 170, 98, 135, 251, 252, 95, 41, 144, 58, 60, 203, 55, 164, 26, 80, 132, 191, 93, 58, 187, 201, 43, 56, 182, 182, 23, 95, 151, 233, 95, 77, 62, 72, 35, 114, 19, 242, 221, 208, 88, 161, 114, 15, 3, 53, 146, 34, 153, 91, 102, 1, 167, 71, 44, 38, 156, 63, 49, 86, 23, 78, 108, 74, 21, 190, 216, 241, 217, 161, 170, 165, 8, 156, 76, 93, 43, 73, 214, 150, 84, 248, 136, 242, 59, 213, 16, 255, 153, 136, 79, 132, 235, 47, 115, 112, 136, 122, 4, 61, 159, 164, 208, 211, 110, 145, 51, 136, 223, 77, 195, 253, 96, 140, 104, 148, 177, 181, 142, 145, 199, 30, 86, 176, 191, 120, 94, 247, 5, 25, 191, 136, 182, 83, 25, 234, 136, 94, 205, 99, 182, 75, 248, 8, 155, 109, 206, 60, 222, 76, 61, 62, 20, 22, 161, 151, 31, 244, 8, 188, 140, 39, 106, 50, 31, 231, 246, 120, 92, 190, 40, 239, 78, 238, 154, 242, 21, 103, 184, 220, 215, 188, 62, 221, 217, 254, 135, 158, 234, 115, 2, 65, 83, 230, 72, 73, 33, 201, 251, 235, 8, 243, 236, 170, 83, 126, 64, 86, 150, 98, 140, 155, 113, 52, 45, 233, 233, 73, 11, 203, 89, 220, 173, 157, 113, 131, 88, 2, 153, 8, 61, 150, 45, 61, 92, 185, 91, 152, 183, 136, 94, 204, 164, 77, 254, 20, 39, 72, 238, 247, 134, 48, 121, 59, 163, 154, 91, 248, 158, 222, 40, 190, 0, 141, 185, 76, 154, 116, 180, 76, 119, 221, 46, 255, 92, 148, 239, 9, 41, 174, 30, 66, 96, 179, 30, 80, 236, 193, 60, 55, 153, 60, 219, 19, 254, 50, 91, 72, 125, 127, 218, 212, 135, 75, 193, 36, 75, 50, 144, 31, 20, 245, 231, 177, 90, 71, 158, 48, 135, 149, 28, 150, 3, 251, 173, 230, 254, 151, 151, 197, 2, 79, 13, 216, 210, 41, 141, 247, 213, 172, 127, 54, 254, 157, 46, 67, 31, 181, 5, 28, 189, 241, 169, 109, 192, 95, 111, 126, 40, 81, 206, 233, 107, 58, 42, 103, 186, 155, 153, 164, 47, 32, 101, 215, 118, 168, 158, 88, 15, 48, 5, 16, 231, 39, 164, 211, 247, 29, 119, 79, 36, 177, 95, 67, 161, 122, 139, 110, 213, 76, 189, 126, 15, 13, 47, 171, 184, 109, 64, 89, 170, 46, 61, 14, 53, 130, 101, 203, 228, 73, 41, 228, 66, 4, 70, 52, 160, 159, 118, 9, 187, 253, 81, 74, 251, 239, 51, 155, 59, 61, 56, 203, 50, 166, 60, 42, 77, 235, 172, 116, 63, 89, 224, 254, 169, 40, 180, 5, 211, 93, 134, 91, 229, 128, 49, 54, 213, 213, 229, 26, 184, 198, 177, 123, 206, 87, 227, 115, 206, 128, 216, 75, 48, 246, 97, 216, 221, 238, 53, 214, 131, 94, 246, 126, 8, 7, 100, 21, 209, 164, 75, 210, 122, 24, 131, 18, 36, 15, 22, 177, 152, 244, 124, 211, 10, 35, 57, 32, 237, 176, 133, 206, 184, 120, 112, 75, 107, 23, 131, 54, 248, 83, 254, 196, 222, 40, 151, 182, 118, 77, 134, 124, 126, 207, 182, 38, 43, 108, 2, 14, 163, 156, 185, 45, 70, 227, 145, 8, 140, 89, 141, 229, 165, 104, 84, 87, 177, 12, 112, 247, 140, 172, 124, 150, 108, 139, 119, 255, 181, 51, 19, 115, 159, 198, 189, 162, 201, 183, 121, 245, 193, 97, 193, 92, 237, 40, 106, 123, 74, 31, 48, 221, 179, 128, 112, 58, 3, 99, 226, 151, 8, 13, 225, 92, 193, 3, 255, 181, 182, 156, 132, 204, 44, 203, 225, 169, 158, 200, 97, 71, 155, 96, 67, 63, 26, 140, 196, 132, 129, 12, 25, 104, 49, 39, 173, 54, 230, 233, 74, 196, 30, 58, 101, 81, 164, 210, 35, 35, 112, 174, 122, 73, 135, 69, 249, 218, 237, 171, 222, 199, 221, 126, 145, 225, 168, 122, 164, 85, 69, 103, 146, 221, 101, 121, 24, 162, 189, 97, 133, 220, 229, 162, 41, 114, 33, 173, 189, 131, 124, 225, 43, 149, 12, 89, 143, 79, 199, 28, 50, 227, 155, 207, 87, 190, 223, 129, 105, 15, 218, 193, 161, 215, 243, 30, 131, 117, 92, 222, 3, 191, 153, 163, 230, 57, 234, 116, 71, 47, 228, 196, 139, 205, 198, 75, 208, 115, 66, 115, 212, 243, 150, 187, 97, 234, 80, 69, 94, 148, 160, 191, 63, 251, 11, 133, 135, 162, 34, 171, 222, 51, 195, 148, 5, 98, 15, 179, 176, 43, 41, 18, 15, 88, 243, 76, 140, 184, 40, 98, 215, 29, 64, 193, 89, 74, 64, 81, 98, 81, 211, 18, 231, 151, 20, 189, 251, 208, 158, 228, 64, 196, 120, 8, 119, 209, 181, 169, 203, 216, 245, 203, 118, 3, 172, 5, 106, 168, 241, 78, 28, 147, 140, 143, 252, 70, 168, 107, 41, 88, 227, 126, 209, 155, 62, 185, 250, 216, 243, 15, 134, 191, 205, 124, 155, 216, 210, 93, 43, 20, 120, 196, 255, 81, 137, 21, 168, 32, 12, 20, 33, 100, 112, 106, 238, 70, 26, 16, 230, 163, 24, 184, 21, 2, 188, 228, 32, 28, 118, 170, 128, 214, 25, 79, 37, 0, 195, 154, 127, 172, 80, 41, 106, 217, 158, 49, 170, 113, 173, 189, 6, 44, 207, 67, 9, 244, 113, 221, 22, 202, 94, 78, 134, 107, 176, 119, 183, 8, 32, 35, 252, 207, 13, 77, 232, 229, 103, 38, 112, 144, 123, 243, 171, 51, 146, 172, 11, 19, 92, 184, 150, 26, 212, 210, 129, 137, 240, 95, 160, 35, 7, 6, 90, 179, 181, 103, 98, 191, 251, 235, 215, 145, 245, 231, 177, 136, 159, 17, 167, 221, 100, 252, 14, 6, 240, 85, 191, 79, 205, 230, 199, 134, 122, 57, 161, 41, 103, 239, 154, 132, 202, 139, 115, 135, 9, 151, 16, 107, 249, 240, 40, 115, 224, 251, 133, 170, 45, 50, 186, 187, 1, 193, 208, 83, 243, 229, 88, 208, 238, 134, 115, 239, 153, 138, 244, 72, 102, 31, 21, 121, 79, 151, 217, 70, 99, 243, 61, 7, 22, 168, 65, 27, 253, 85, 255, 2, 231, 151, 137, 87, 86, 21, 132, 16, 178, 63, 95, 110, 20, 182, 87, 167, 167, 234, 65, 172, 3, 160, 204, 53, 73, 224, 0, 81, 67, 32, 240, 167, 133, 240, 144, 19, 52, 108, 5, 54, 230, 155, 122, 250, 32, 125, 193, 214, 160, 181, 75, 243, 227, 236, 137, 203, 83, 221, 238, 134, 139, 68, 33, 216, 186, 100, 149, 33, 32, 86, 249, 141, 179, 10, 143, 218, 35, 41, 74, 253, 0, 176, 58, 63, 153, 38, 2, 121, 107, 102, 252, 176, 197, 195, 2, 95, 211, 79, 142, 102, 2, 138, 167, 139, 20, 100, 106, 203, 57, 102, 136, 82, 236, 251, 97, 100, 148, 184, 31, 65, 16, 164, 181, 87, 232, 52, 173, 236, 219, 206, 224, 55, 250, 34, 180, 219, 40, 225, 181, 18, 175, 160, 86, 197, 87, 232, 183, 128, 88, 175, 36, 161, 35, 151, 85, 44, 194, 184, 151, 120, 112, 89, 246, 13, 17, 245, 216, 254, 197, 233, 241, 90, 45, 95, 23, 255, 73, 225, 66, 149, 80, 100, 206, 194, 146, 66, 213, 95, 156, 188, 187, 16, 77, 108, 236, 85, 108, 158, 113, 234, 74, 20, 19, 158, 201, 243, 79, 95, 137, 227, 68, 165, 205, 129, 135, 119, 253, 61, 228, 241, 24, 225, 147, 113, 191, 48, 122, 199, 129, 101, 4, 121, 41, 128, 79, 116, 238, 158, 177, 96, 15, 31, 13, 33, 7, 91, 141, 144, 218, 143, 203, 233, 78, 6, 188, 135, 97, 181, 227, 118, 110, 190, 234, 80, 252, 18, 205, 50, 148, 43, 41, 50, 162, 189, 62, 190, 215, 55, 84, 171, 237, 224, 158, 202, 76, 129, 211, 110, 14, 196, 205, 40, 213, 212, 214, 83, 58, 25, 156, 21, 42, 251, 206, 207, 92, 22, 90, 76, 225, 10, 117, 123, 108, 92, 86, 102, 202, 106, 207, 45, 157, 220, 57, 107, 148, 206, 246, 253, 76, 119, 82, 115, 68, 252, 42, 239, 195, 76, 74, 204, 1, 110, 208, 127, 2, 0, 53, 244, 47, 148, 64, 106, 196, 167, 129, 94, 231, 65, 237, 37, 151, 113, 11, 152, 80, 108, 183, 189, 154, 9, 78, 24, 254, 213, 14, 18, 247, 50, 158, 255, 174, 148, 52, 178, 85, 171, 198, 231, 85, 131, 163, 48, 245, 249, 86, 206, 58, 40, 23, 182, 2, 162, 135, 81, 35, 227, 158, 225, 130, 109, 210, 124, 192, 22, 153, 79, 181, 141, 215, 57, 69, 255, 48, 178, 69, 123, 5, 198, 162, 191, 21, 204, 96, 94, 56, 231, 37, 255, 219, 6, 27, 153, 101, 248, 212, 249, 210, 228, 36, 196, 46, 241, 153, 133, 10, 118, 78, 126, 79, 175, 205, 73, 164, 233, 0, 21, 67, 219, 4, 68, 210, 138, 89, 1, 50, 158, 223, 129, 185, 174, 185, 126, 13, 48, 30, 107, 159, 46, 21, 199, 114, 127, 121, 179, 119, 180, 220, 21, 83, 144, 182, 74, 173, 232, 221, 254, 0, 222, 240, 123, 77, 36, 217, 242, 87, 108, 238, 168, 234, 92, 119, 123, 221, 204, 29, 1, 59, 0, 154, 107, 30, 204, 187, 173, 144, 52, 220, 23, 88, 10, 70, 214, 22, 206, 94, 26, 243, 128, 9, 69, 211, 36, 198, 15, 96, 3, 241, 1, 229, 58, 193, 91, 37, 209, 156, 131, 157, 252, 40, 165, 150, 84, 82, 14, 153, 199, 209, 69, 86, 88, 96, 69, 69, 210, 194, 11, 106, 216, 102, 102, 189, 140, 94, 39, 82, 25, 169, 189, 20, 204, 141, 136, 200, 67, 81, 16, 37, 94, 188, 200, 140, 225, 75, 246, 168, 123, 128, 254, 165, 227, 252, 150, 165, 175, 103, 222, 128, 2, 185, 121, 78, 96, 105, 39, 1, 31, 240, 17, 89, 170, 5, 145, 197, 163, 217, 48, 104, 189, 110, 181, 46, 43, 135, 58, 152, 96, 11, 230, 211, 91, 173, 234, 115, 141, 126, 141, 173, 182, 144, 204, 104, 7, 126, 33, 80, 137, 110, 95, 236, 9, 25, 183, 107, 49, 50, 83, 4, 69, 99, 85, 19, 177, 67, 102, 199, 106, 29, 161, 100, 139, 234, 130, 162, 229, 193, 69, 48, 76, 193, 253, 213, 147, 92, 240, 128, 124, 13, 77, 75, 29, 233, 26, 178, 14, 213, 207, 141, 1, 63, 161, 168, 94, 107, 173, 223, 43, 58, 213, 226, 227, 58, 72, 169, 201, 102, 99, 212, 148, 152, 159, 7, 158, 252, 83, 235, 214, 233, 100, 105, 42, 84, 54, 15, 169, 164, 85, 134, 72, 117, 147, 203, 133, 246, 229, 145, 211, 138, 31, 25, 251, 241, 22, 212, 172, 253, 72, 125, 36, 67, 66, 39, 204, 41, 54, 155, 89, 153, 168, 51, 67, 249, 225, 25, 235, 40, 82, 169, 158, 243, 24, 66, 23, 78, 37, 50, 148, 104, 182, 96, 3, 144, 255, 5, 95, 131, 193, 46, 190, 100, 116, 56, 164, 221, 69, 158, 53, 143, 93, 106, 167, 244, 73, 124, 85, 88, 54, 86, 133, 84, 51, 244, 46, 63, 122, 97, 192, 141, 174, 207, 48, 185, 171, 99, 66, 120, 141, 33, 239, 191, 157, 232, 253, 188, 202, 44, 77, 69, 29, 85, 107, 196, 42, 255, 56, 174, 24, 193, 221, 171, 232, 227, 191, 190, 54, 49, 140, 136, 71, 71, 68, 94, 65, 175, 109, 131, 179, 217, 144, 152, 233, 248, 35, 175, 255, 78, 226, 184, 199, 211, 101, 35, 144, 79, 232, 34, 131, 141, 181, 52, 246, 169, 212, 45, 148, 161, 187, 135, 241, 183, 136, 98, 206, 206, 81, 154, 121, 129, 240, 115, 239, 215, 175, 247, 6, 132, 204, 108, 222, 207, 138, 155, 81, 90, 178, 219, 145, 212, 186, 169, 204, 215, 39, 56, 68, 226, 68, 171, 100, 230, 210, 43, 233, 2, 100, 14, 45, 234, 127, 6, 161, 118, 222, 41, 97, 54, 120, 226, 105, 225, 13, 160, 213, 91, 110, 76, 18, 19, 55, 207, 155, 110, 116, 120, 64, 80, 3, 66, 232, 103, 16, 230, 62, 189, 176, 166, 190, 202, 34, 67, 216, 135, 77, 149, 37, 21, 224, 252, 153, 181, 213, 126, 69, 14, 253, 33, 11, 69, 33, 93, 87, 245, 133, 10, 184, 4, 18, 109, 105, 96, 177, 9, 197, 30, 38, 255, 213, 233, 123, 161, 150, 140, 83, 43, 2, 200, 108, 227, 189, 223, 149, 23, 114, 249, 254, 121, 52, 14, 123, 34, 92, 124, 76, 99, 167, 82, 232, 63, 122, 195, 112, 27, 126, 155, 212, 48, 167, 26, 130, 136, 219, 81, 50, 6, 248, 18, 213, 140, 133, 83, 18, 72, 47, 200, 42, 5, 189, 59, 215, 185, 252, 233, 75, 79, 206, 21, 95, 220, 200, 168, 185, 93, 73, 84, 155, 248, 43, 96, 113, 246, 118, 71, 190, 181, 42, 49, 154, 78, 201, 232, 190, 245, 199, 125, 133, 36, 225, 96, 74, 211, 193, 6, 57, 244, 43, 166, 212, 135, 122, 88, 139, 20, 183, 72, 20, 77, 57, 30, 198, 238, 48, 56, 239, 222, 88, 167, 50, 128, 180, 65, 51, 243, 212, 188, 15, 168, 111, 159, 93, 155, 88, 111, 211, 225, 206, 94, 142, 131, 53, 188, 248, 184, 39, 207, 126, 222, 91, 84, 11, 232, 93, 92, 34, 235, 123, 250, 37, 27, 68, 79, 23, 192, 224, 136, 160, 34, 233, 39, 228, 71, 21, 43, 195, 133, 142, 255, 189, 35, 43, 98, 143, 236, 15, 209, 213, 130, 75, 243, 74, 9, 128, 83, 234, 254, 173, 29, 191, 211, 196, 227, 147, 28, 139, 11, 135, 19, 83, 51, 178, 204, 97, 225, 59, 226, 149, 24, 95, 242, 93, 21, 215, 103, 112, 215, 76, 50, 23, 88, 77, 55, 32, 36, 223, 102, 68, 28, 88, 138, 201, 90, 161, 14, 218, 170, 120, 108, 138, 71, 79, 49, 154, 96, 52, 128, 101, 137, 153, 27, 123, 95, 185, 59, 212, 32, 37, 119, 184, 142, 43, 226, 23, 204, 103, 226, 177, 202, 204, 6, 135, 230, 10, 144, 147, 35, 136, 140, 211, 95, 97, 93, 135, 101, 31, 122, 141, 245, 64, 169, 30, 226, 218, 60, 110, 76, 161, 229, 160, 239, 115, 132, 14, 32, 120, 225, 13, 149, 4, 167, 46, 77, 148, 14, 31, 128, 37, 39, 96, 43, 141, 185, 248, 171, 133, 13, 5, 170, 139, 100, 77, 99, 8, 105, 130, 38, 87, 113, 170, 67, 46, 14, 150, 175, 203, 119, 206, 49, 170, 166, 160, 202, 178, 55, 226, 194, 162, 189, 195, 225, 184, 174, 159, 248, 15, 96, 255, 243, 72, 49, 240, 90, 162, 93, 199, 108, 97, 155, 198, 33, 201, 118, 247, 6, 53, 114, 138, 78, 63, 216, 58, 95, 206, 76, 202, 33, 183, 176, 4, 188, 237, 231, 23, 173, 158, 240, 152, 62, 115, 182, 249, 213, 163, 246, 99, 125, 143, 50, 192, 235, 5, 113, 18, 108, 109, 74, 185, 217, 114, 182, 67, 255, 56, 118, 152, 76, 14, 108, 142, 154, 58, 61, 135, 208, 75, 118, 120, 218, 13, 111, 41, 145, 95, 139, 227, 214, 20, 191, 206, 115, 222, 57, 170, 82, 37, 83, 121, 165, 45, 139, 202, 92, 106, 1, 42, 188, 5, 157, 206, 28, 130, 37, 166, 90, 242, 199, 25, 84, 24, 29, 169, 196, 114, 101, 1, 203, 194, 99, 91, 67, 198, 197, 116, 90, 102, 36, 210, 221, 150, 126, 133, 168, 35, 124, 154, 149, 162, 147, 124, 36, 178, 22, 185, 213, 192, 97, 173, 228, 221, 107, 192, 124, 173, 185, 32, 163, 56, 186, 24, 179, 178, 61, 244, 64, 94, 49, 233, 117, 199, 86, 221, 79, 51, 177, 0, 241, 195, 192, 203, 69, 237, 245, 96, 25, 127, 17, 177, 254, 157, 56, 251, 46, 223, 47, 199, 47, 156, 122, 3, 4, 108, 42, 164, 136, 116, 47, 108, 109, 177, 146, 47, 24, 22, 120, 121, 44, 81, 48, 11, 53, 171, 35, 206, 156, 247, 193, 209, 149, 96, 60, 41, 112, 140, 103, 16, 142, 116, 118, 2, 31, 249, 94, 232, 241, 82, 4, 113, 22, 17, 193, 108, 230, 64, 112, 65, 132, 58, 144, 197, 53, 208, 243, 240, 58, 192, 22, 209, 198, 59, 92, 52, 6, 103, 14, 212, 224, 213, 35, 194, 244, 20, 221, 181, 222, 122, 8, 107, 194, 185, 130, 26, 162, 217, 23, 35, 33, 98, 40, 200, 221, 215, 206, 222, 169, 31, 154, 12, 197, 113, 13, 171, 121, 9, 229, 96, 215, 228, 15, 4, 210, 116, 40, 125, 89, 37, 19, 24, 241, 226, 25, 214, 87, 32, 243, 120, 198, 192, 95, 57, 152, 7, 32, 171, 167, 120, 139, 132, 203, 91, 17, 176, 35, 93, 165, 21, 239, 7, 249, 56, 41, 150, 36, 132, 133, 167, 97, 30, 78, 2, 217, 155, 77, 104, 68, 30, 13, 227, 81, 185, 160, 23, 221, 250, 26, 95, 54, 82, 136, 179, 48, 237, 134, 193, 81, 115, 184, 219, 86, 62, 156, 16, 158, 66, 36, 177, 70, 155, 81, 35, 241, 200, 82, 251, 187, 162, 162, 142, 197, 217, 158, 18, 189, 183, 216, 172, 5, 10, 40, 59, 138, 200, 91, 161, 125, 102, 106, 173, 173, 193, 199, 135, 199, 27, 56, 182, 33, 165, 206, 19, 103, 50, 233, 241, 114, 230, 193, 129, 114, 121, 137, 112, 44, 50, 169, 13, 38, 231, 85, 149, 156, 52, 120, 113, 1, 18, 169, 62, 200, 218, 213, 13, 49, 101, 18, 115, 112, 206, 177, 151, 110, 138, 89, 175, 246, 13, 60, 212, 167, 239, 84, 33, 101, 77, 178, 230, 78, 201, 247, 148, 110, 194, 176, 46, 6, 190, 134, 79, 76, 102, 189, 76, 192, 127, 251, 202, 106, 90, 1, 231, 142, 45, 177, 184, 88, 200, 81, 97, 2, 237, 128, 174, 108, 138, 166, 60, 14, 64, 142, 59, 5, 33, 180, 123, 15, 121, 252, 246, 178, 166, 151, 188, 52, 32, 170, 135, 217, 1, 73, 205, 16, 169, 214, 93, 8, 12, 125, 248, 154, 117, 37, 195, 51, 97, 145, 92, 122, 116, 42, 178, 46, 150, 49, 132, 196, 58, 103, 104, 6, 241, 30, 57, 205, 94, 110, 186, 21, 213, 4, 120, 165, 124, 108, 78, 63, 2, 89, 214, 74, 160, 190, 192, 161, 151, 175, 5, 168, 229, 19, 144, 132, 97, 164, 86, 163, 52, 243, 140, 176, 86, 183, 73, 207, 113, 237, 60, 89, 131, 252, 103, 194, 201, 121, 61, 141, 154, 173, 196, 200, 75, 39, 57, 184, 223, 130, 22, 234, 213, 167, 251, 40, 51, 219, 97, 144, 56, 44, 225, 93, 12, 227, 230, 97, 27, 62, 234, 130, 228, 44, 224, 12, 41, 228, 229, 152, 176, 196, 167, 136, 74, 154, 189, 167, 42, 0, 140, 185, 116, 91, 124, 67, 11, 79, 50, 20, 152, 242, 224, 11, 45, 131, 177, 50, 134, 144, 125, 22, 54, 88, 186, 34, 130, 43, 38, 104, 239, 51, 37, 75, 79, 3, 246, 10, 45, 28, 211, 227, 185, 198, 251, 8, 202, 229, 244, 30, 133, 184, 75, 229, 133, 25, 32, 139, 237, 222, 240, 245, 240, 254, 12, 164, 175, 173, 156, 150, 85, 85, 120, 42, 40, 40, 27, 109, 221, 154, 172, 182, 230, 98, 124, 244, 246, 51, 110, 129, 176, 124, 11, 151, 52, 180, 154, 74, 107, 65, 60, 213, 220, 14, 80, 7, 108, 33, 58, 1, 224, 41, 27, 108, 92, 183, 206, 88, 233, 138, 91, 81, 57, 79, 20, 195, 110, 1, 194, 188, 2, 217, 142, 197, 79, 69, 213, 220, 116, 73, 53, 132, 105, 190, 218, 126, 26, 58, 206, 3, 97, 2, 107, 185, 189, 56, 36, 182, 197, 183, 230, 161, 137, 84, 103, 116, 222, 182, 103, 57, 10, 109, 226, 85, 131, 152, 97, 56, 181, 19, 228, 245, 26, 122, 124, 197, 92, 235, 238, 179, 42, 113, 35, 19, 170, 59, 143, 227, 211, 204, 95, 191, 77, 10, 54, 146, 36, 227, 178, 185, 194, 241, 47, 63, 86, 106, 28, 115, 37, 63, 178, 79, 164, 234, 4, 32, 135, 222, 192, 142, 15, 137, 109, 2, 149, 124, 2, 254, 251, 114, 176, 73, 222, 173, 17, 110, 181, 179, 69, 190, 105, 224, 133, 81, 238, 238, 192, 50, 192, 161, 213, 76, 69, 133, 128, 89, 170, 86, 49, 128, 157, 102, 181, 185, 6, 130, 239, 114, 221, 247, 11, 199, 90, 162, 216, 10, 64, 44, 84, 119, 59, 22, 91, 103, 45, 188, 240, 250, 195, 188, 25, 252, 231, 11, 250, 123, 15, 55, 13, 99, 168, 47, 80, 225, 140, 80, 238, 93, 214, 16, 1, 97, 177, 113, 11, 244, 112, 87, 234, 6, 96, 85, 111, 102, 255, 220, 152, 53, 133, 236, 122, 255, 164, 236, 128, 202, 163, 143, 201, 44, 103, 156, 205, 116, 207, 235, 234, 41, 215, 69, 87, 240, 44, 60, 141, 173, 220, 142, 31, 9, 27, 139, 245, 148, 250, 132, 118, 94, 76, 88, 149, 220, 232, 228, 38, 93, 115, 47, 242, 34, 176, 154, 99, 103, 110, 133, 13, 36, 158, 154, 18, 118, 143, 166, 123, 137, 22, 146, 87, 240, 181, 72, 23, 215, 15, 59, 143, 65, 214, 221, 139, 101, 249, 8, 17, 252, 61, 19, 18, 235, 130, 109, 146, 216, 196, 132, 174, 254, 61, 172, 84, 105, 43, 189, 64, 200, 90, 218, 231, 83, 248, 79, 247, 232, 178, 71, 0, 89, 155, 58, 231, 195, 37, 121, 223, 73, 107, 65, 31, 210, 29, 50, 136, 115, 189, 88, 236, 213, 107, 22, 80, 207, 106, 245, 52, 187, 83, 119, 183, 111, 82, 217, 72, 212, 114, 197, 121, 117, 238, 16, 213, 106, 243, 46, 24, 146, 115, 88, 254, 72, 73, 107, 250, 245, 193, 16, 114, 249, 239, 173, 62, 129, 156, 160, 104, 235, 188, 66, 102, 138, 187, 152, 195, 87, 13, 177, 63, 243, 236, 111, 153, 152, 235, 245, 51, 129, 48, 99, 195, 60, 105, 161, 131, 92, 242, 16, 153, 143, 128, 181, 54, 18, 28, 40, 70, 227, 29, 204, 78, 43, 90, 201, 22, 57, 182, 189, 160, 62, 162, 96, 192, 170, 19, 129, 53, 161, 12, 140, 151, 158, 82, 254, 225, 89, 126, 1, 199, 142, 7, 12, 58, 117, 67, 162, 168, 239, 86, 11, 45, 53, 103, 98, 11, 234, 164, 254, 122, 216, 222, 194, 229, 244, 87, 49, 169, 34, 195, 26, 156, 1, 92, 236, 62, 176, 56, 151, 170, 108, 5, 150, 131, 33, 179, 52, 17, 104, 227, 124, 198, 100, 125, 43, 207, 9, 222, 39, 214, 39, 114, 91, 9, 18, 17, 255, 253, 193, 28, 66, 20, 31, 222, 61, 93, 76, 177, 72, 164, 156, 63, 121, 120, 64, 61, 175, 6, 171, 126, 216, 222, 45, 92, 98, 138, 195, 12, 54, 27, 3, 251, 49, 36, 118, 105, 59, 206, 211, 90, 171, 196, 189, 234, 2, 2, 73, 67, 72, 188, 167, 0, 82, 53, 72, 85, 15, 114, 164, 147, 40, 246, 73, 135, 13, 53, 225, 113, 125, 134, 171, 176, 61, 207, 33, 204, 24, 252, 178, 203, 35, 128, 17, 109, 136, 152, 209, 252, 222, 193, 72, 218, 192, 172, 111, 239, 30, 230, 49, 52, 38, 177, 82, 120, 236, 8, 58, 152, 94, 170, 83, 69, 62, 255, 28, 103, 183, 173, 224, 140, 65, 185, 167, 78, 167, 108, 31, 182, 92, 125, 226, 94, 30, 124, 108, 80, 178, 100, 78, 226, 240, 181, 209, 101, 179, 91, 140, 64, 41, 145, 2, 86, 133, 123, 191, 162, 93, 70, 221, 193, 174, 119, 83, 212, 173, 17, 189, 65, 122, 199, 132, 26, 170, 78, 0, 186, 33, 64, 77, 250, 43, 56, 254, 56, 233, 108, 124, 70, 153, 131, 69, 206, 220, 129, 95, 208, 113, 71, 90, 35, 232, 35, 131, 79, 20, 179, 12, 41, 134, 90, 129, 252, 186, 63, 15, 114, 99, 94, 220, 16, 81, 67, 65, 1, 116, 181, 252, 196, 89, 146, 44, 128, 59, 145, 55, 145, 144, 95, 237, 186, 115, 115, 72, 162, 249, 91, 71, 24, 35, 96, 190, 60, 81, 56, 178, 121, 173, 119, 23, 16, 124, 156, 57, 216, 191, 5, 161, 16, 228, 43, 182, 228, 126, 9, 219, 25, 248, 37, 6, 72, 86, 45, 109, 169, 89, 170, 174, 124, 134, 131, 221, 154, 35, 69, 131, 37, 4, 85, 219, 70, 132, 118, 79, 55, 176, 45, 2, 179, 66, 17, 59, 60, 12, 110, 124, 248, 115, 16, 222, 141, 57, 74, 77, 252, 109, 226, 47, 56, 48, 22, 192, 183, 126, 86, 102, 226, 108, 136, 155, 87, 38, 111, 240, 155, 141, 95, 64, 157, 80, 107, 244, 2, 212, 88, 54, 153, 236, 65, 103, 112, 234, 28, 50, 2, 10, 208, 141, 102, 137, 243, 204, 193, 174, 163, 57, 4, 157, 149, 0, 52, 103, 134, 99, 9, 54, 253, 130, 108, 26, 206, 37, 97, 43, 94, 44, 28, 210, 87, 100, 66, 190, 25, 130, 99, 230, 233, 118, 15, 79, 201, 214, 157, 144, 15, 90, 51, 125, 39, 22, 83, 136, 62, 227, 171, 146, 104, 112, 48, 242, 157, 28, 75, 19, 76, 66, 75, 7, 214, 172, 39, 2, 223, 38, 153, 37, 154, 168, 193, 197, 225, 45, 51, 122, 98, 218, 29, 1, 199, 200, 104, 44, 225, 84, 64, 237, 226, 224, 186, 178, 252, 71, 142, 106, 234, 234, 100, 29, 117, 212, 189, 183, 18, 39, 76, 61, 25, 158, 144, 148, 189, 27, 174, 101, 115, 175, 214, 196, 199, 177, 138, 230, 28, 155, 23, 7, 225, 229, 130, 170, 30, 134, 24, 203, 250, 100, 204, 235, 9, 80, 138, 207, 159, 203, 161, 5, 244, 226, 127, 246, 3, 220, 104, 53, 143, 94, 186, 142, 71, 16, 29, 104, 50, 30, 42, 181, 215, 206, 39, 131, 42, 205, 132, 142, 118, 73, 186, 212, 144, 29, 236, 54, 227, 211, 13, 160, 246, 42, 19, 199, 22, 99, 42, 44, 31, 220, 255, 136, 208, 218, 162, 164, 121, 72, 101, 36, 136, 248, 232, 117, 96, 146, 168, 242, 42, 30, 227, 75, 7, 239, 48, 15, 41, 17, 28, 222, 6, 239, 164, 112, 229, 57, 94, 222, 18, 98, 137, 148, 148, 2, 156, 95, 146, 209, 62, 232, 84, 125, 239, 247, 29, 154, 100, 208, 69, 172, 134, 169, 109, 52, 218, 236, 246, 168, 237, 210, 210, 219, 62, 44, 28, 205, 12, 4, 223, 220, 41, 197, 43, 173, 70, 164, 168, 29, 253, 243, 192, 57, 159, 254, 175, 228, 249, 194, 95, 92, 98, 115, 18, 210, 209, 249, 22, 216, 114, 136, 162, 238, 13, 180, 27, 164, 105, 73, 91, 42, 89, 145, 192, 142, 65, 216, 26, 38, 41, 47, 149, 142, 15, 145, 120, 249, 78, 170, 69, 218, 243, 40, 169, 101, 16, 121, 46, 177, 6, 64, 24, 225, 129, 144, 118, 147, 33, 247, 214, 175, 218, 211, 72, 104, 164, 185, 134, 51, 183, 231, 123, 25, 251, 45, 183, 223, 194, 95, 103, 209, 107, 92, 34, 53, 173, 92, 33, 249, 180, 150, 107, 238, 17, 84, 201, 245, 189, 41, 50, 131, 131, 31, 238, 21, 185, 19, 214, 243, 125, 120, 248, 78, 40, 131, 223, 78, 117, 69, 57, 134, 131, 247, 208, 129, 255, 238, 12, 94, 203, 47, 60, 78, 255, 136, 46, 235, 161, 189, 216, 57, 167, 28, 17, 29, 162, 221, 129, 74, 219, 242, 51, 157, 81, 11, 94, 75, 226, 77, 221, 154, 83, 96, 91, 13, 76, 8, 127, 31, 124, 187, 6, 181, 27, 182, 158, 227, 217, 62, 236, 52, 237, 173, 180, 213, 191, 243, 162, 35, 30, 51, 215, 131, 69, 183, 129, 107, 192, 31, 189, 51, 104, 47, 149, 16, 32, 107, 46, 13, 178, 2, 22, 230, 133, 60, 46, 129, 95, 119, 63, 150, 51, 187, 71, 11, 248, 200, 120, 101, 59, 97, 104, 53, 36, 0, 12, 203, 176, 110, 184, 94, 252, 63, 148, 142, 43, 79, 141, 109, 216, 11, 9, 140, 65, 169, 226, 252, 83, 82, 241, 119, 55, 94, 120, 180, 117, 60, 189, 141, 40, 227, 61, 220, 171, 238, 48, 48, 66, 18, 156, 245, 213, 62, 244, 210, 37, 37, 236, 23, 217, 131, 68, 112, 60, 69, 243, 15, 239, 64, 213, 197, 175, 67, 32, 49, 158, 189, 86, 201, 89, 255, 19, 148, 245, 36, 99, 137, 17, 3, 200, 229, 139, 198, 226, 249, 66, 54, 206, 208, 99, 138, 43, 75, 20, 166, 48, 189, 132, 110, 222, 56, 47, 71, 211, 146, 31, 30, 8, 188, 206, 195, 216, 193, 205, 174, 13, 201, 6, 76, 228, 93, 245, 109, 186, 160, 155, 151, 72, 247, 225, 119, 152, 40, 0, 28, 251, 198, 66, 228, 79, 137, 151, 122, 55, 42, 133, 15, 203, 22, 114, 158, 213, 224, 221, 9, 43, 129, 111, 180, 170, 166, 198, 191, 244, 186, 24, 104, 134, 18, 77, 151, 87, 53, 254, 22, 123, 81, 62, 79, 176, 53, 129, 134, 167, 179, 126, 23, 233, 51, 240, 89, 71, 243, 24, 102, 34, 42, 215, 108, 105, 237, 188, 15, 159, 18, 47, 41, 15, 228, 183, 100, 133, 4, 136, 200, 182, 6, 167, 89, 143, 198, 101, 250, 29, 113, 43, 196, 160, 154, 104, 132, 213, 132, 173, 60, 195, 232, 2, 23, 230, 159, 171, 165, 210, 119, 213, 211, 148, 192, 105, 153, 84, 229, 209, 159, 64, 219, 56, 7, 143, 59, 47, 17, 65, 163, 97, 128, 208, 50, 110, 210, 51, 244, 134, 238, 174, 187, 251, 150, 141, 7, 182, 137, 224, 174, 187, 153, 162, 23, 163, 34, 157, 250, 159, 249, 227, 23, 218, 172, 33, 188, 97, 215, 186, 244, 72, 0, 67, 236, 237, 82, 35, 158, 7, 80, 32, 54, 126, 95, 175, 1, 200, 108, 136, 250, 21, 192, 235, 34, 240, 165, 147, 166, 139, 120, 184, 40, 225, 177, 48, 17, 50, 210, 184, 252, 203, 147, 71, 54, 171, 61, 114, 19, 148, 26, 243, 108, 10, 212, 7, 7, 93, 53, 212, 95, 82, 146, 29, 238, 248, 139, 96, 65, 48, 16, 200, 21, 119, 56, 124, 103, 1, 32, 37, 26, 147, 95, 29, 155, 82, 79, 58, 85, 86, 164, 118, 66, 17, 54, 233, 101, 148, 235, 77, 49, 144, 133, 208, 177, 33, 123, 116, 128, 242, 148, 241, 202, 34, 30, 145, 70, 156, 177, 153, 113, 178, 111, 216, 65, 88, 69, 213, 36, 243, 10, 214, 235, 218, 166, 225, 14, 133, 223, 23, 212, 29, 24, 5, 176, 65, 142, 205, 161, 228, 244, 106, 151, 165, 14, 197, 11, 235, 145, 241, 92, 69, 120, 255, 132, 137, 57, 204, 164, 156, 128, 30, 187, 28, 140, 144, 120, 123, 28, 29, 223, 143, 1, 199, 221, 19, 203, 241, 228, 6, 170, 190, 136, 234, 141, 160, 85, 0, 194, 124, 121, 218, 190, 57, 58, 103, 103, 96, 104, 7, 223, 131, 42, 193, 16, 129, 58, 85, 16, 231, 140, 169, 66, 45, 178, 154, 229, 192, 195, 236, 186, 104, 175, 12, 134, 48, 200, 139, 106, 3, 177, 240, 247, 216, 25, 175, 200, 54, 178, 22, 77, 217, 3, 252, 221, 124, 137, 135, 47, 151, 148, 142, 182, 252, 192, 114, 229, 216, 225, 75, 59, 64, 69, 160, 115, 111, 66, 212, 204, 118, 214, 2, 208, 246, 150, 76, 202, 154, 224, 209, 98, 37, 197, 147, 204, 70, 154, 206, 111, 249, 169, 211, 143, 63, 173, 36, 53, 247, 222, 179, 131, 13, 52, 71, 223, 231, 234, 54, 138, 8, 164, 84, 185, 200, 107, 117, 60, 208, 159, 175, 236, 57, 31, 141, 55, 178, 41, 92, 123, 178, 67, 130, 141, 78, 111, 89, 202, 181, 68, 224, 155, 8, 43, 251, 95, 176, 141, 215, 13, 203, 81, 189, 226, 238, 214, 89, 0, 62, 90, 33, 211, 203, 203, 36, 0, 174, 48, 230, 145, 28, 22, 99, 113, 208, 43, 61, 138, 74, 36, 75, 209, 249, 233, 169, 137, 107, 200, 139, 244, 18, 21, 147, 148, 165, 134, 72, 72, 154, 20, 54, 231, 134, 75, 182, 231, 199, 190, 180, 217, 241, 185, 247, 132, 116, 220, 103, 94, 234, 109, 163, 201, 75, 167, 154, 76, 184, 134, 101, 123, 148, 66, 141, 182, 159, 83, 90, 203, 89, 10, 121, 230, 57, 125, 127, 42, 14, 195, 26, 105, 59, 251, 170, 146, 235, 148, 133, 75, 144, 251, 42, 34, 35, 81, 212, 58, 32, 42, 235, 39, 157, 185, 49, 55, 45, 110, 245, 154, 109, 58, 173, 206, 194, 17, 233, 207, 56, 205, 61, 247, 112, 125, 252, 183, 155, 206, 233, 142, 248, 91, 66, 254, 120, 6, 213, 250, 202, 147, 30, 195, 84, 227, 139, 164, 148, 143, 67, 140, 231, 100, 200, 42, 215, 5, 232, 224, 183, 59, 59, 148, 16, 28, 55, 166, 198, 132, 22, 173, 73, 135, 152, 74, 48, 10, 106, 159, 161, 60, 249, 185, 137, 232, 196, 212, 92, 28, 213, 208, 2, 121, 134, 92, 107, 5, 149, 15, 241, 55, 130, 108, 223, 80, 214, 8, 43, 172, 153, 174, 85, 49, 149, 199, 31, 214, 14, 19, 1, 132, 110, 61, 117, 165, 98, 236, 176, 44, 152, 19, 66, 112, 255, 114, 97, 211, 56, 242, 9, 248, 54, 131, 161, 66, 254, 246, 210, 101, 149, 177, 95, 113, 75, 176, 67, 7, 77, 54, 90, 17, 160, 189, 178, 90, 39, 108, 110, 136, 86, 44, 5, 1, 201, 244, 91, 211, 132, 168, 229, 55, 219, 153, 211, 222, 118, 40, 16, 212, 229, 192, 154, 236, 247, 186, 133, 51, 225, 114, 117, 99, 153, 150, 182, 69, 235, 0, 135, 176, 13, 122, 55, 154, 74, 249, 212, 238, 230, 131, 179, 38, 17, 118, 255, 214, 192, 11, 57, 204, 108, 64, 29, 186, 139, 125, 149, 203, 1, 79, 76, 48, 179, 164, 25, 238, 229, 167, 159, 133, 62, 178, 241, 224, 174, 33, 223, 45, 1, 56, 172, 241, 4, 56, 242, 209, 232, 80, 150, 228, 106, 240, 245, 86, 93, 83, 237, 152, 203, 162, 130, 171, 139, 199, 81, 108, 239, 123, 209, 57, 152, 134, 73, 70, 238, 199, 94, 102, 73, 248, 236, 187, 252, 27, 25, 214, 47, 96, 45, 99, 189, 150, 26, 59, 95, 167, 33, 12, 50, 27, 139, 232, 153, 111, 73, 7, 206, 194, 106, 70, 81, 146, 123, 245, 74, 48, 193, 29, 151, 237, 12, 7, 77, 177, 25, 122, 38, 102, 167, 244, 211, 23, 13, 59, 43, 205, 124, 114, 78, 80, 171, 111, 91, 194, 35, 75, 239, 75, 121, 75, 160, 118, 15, 24, 100, 231, 68, 162, 68, 229, 156, 56, 147, 25, 155, 102, 157, 141, 216, 184, 128, 33, 141, 252, 111, 210, 254, 246, 111, 214, 112, 64, 25, 52, 14, 170, 30, 150, 85, 145, 243, 138, 211, 209, 37, 223, 233, 44, 188, 13, 169, 165, 253, 89, 44, 227, 126, 48, 253, 58, 195, 206, 73, 43, 83, 122, 113, 81, 62, 172, 37, 165, 165, 175, 234, 181, 63, 195, 3, 29, 246, 107, 195, 74, 78, 239, 136, 240, 158, 136, 228, 29, 218, 253, 209, 27, 96, 83, 65, 84, 238, 223, 54, 173, 55, 61, 161, 242, 52, 233, 235, 35, 165, 197, 81, 232, 108, 184, 167, 111, 14, 155, 74, 192, 115, 164, 7, 167, 3, 92, 197, 206, 132, 171, 232, 24, 15, 184, 78, 54, 235, 208, 214, 140, 241, 5, 133, 226, 140, 255, 13, 209, 186, 103, 16, 180, 88, 233, 142, 241, 6, 55, 168, 94, 49, 112, 205, 44, 143, 246, 43, 123, 6, 127, 209, 46, 161, 216, 19, 203, 17, 191, 132, 216, 218, 222, 228, 147, 158, 5, 27, 57, 164, 2, 44, 38, 69, 146, 89, 9, 57, 78, 225, 109, 86, 136, 15, 18, 237, 137, 208, 130, 131, 218, 96, 1, 58, 252, 239, 230, 194, 145, 154, 199, 182, 106, 122, 201, 226, 50, 141, 254, 144, 18, 58, 158, 49, 17, 176, 41, 152, 81, 51, 8, 242, 60, 250, 243, 224, 254, 15, 187, 93, 252, 122, 49, 106, 96, 138, 59, 51, 100, 254, 42, 67, 128, 128, 9, 112, 24, 142, 235, 219, 90, 164, 40, 111, 157, 210, 181, 56, 205, 26, 115, 117, 59, 7, 87, 254, 38, 60, 210, 83, 150, 116, 236, 206, 9, 198, 14, 12, 101, 61, 189, 220, 190, 69, 56, 168, 123, 236, 143, 128, 133, 212, 33, 240, 97, 88, 56, 164, 22, 168, 11, 230, 75, 1, 27, 76, 184, 36, 140, 236, 83, 233, 192, 176, 29, 61, 69, 202, 155, 125, 103, 230, 44, 151, 25, 74, 34, 219, 160, 224, 35, 14, 173, 135, 45, 52, 0, 172, 228, 210, 253, 125, 120, 140, 7, 249, 211, 105, 254, 148, 107, 120, 139, 53, 138, 101, 72, 228, 4, 19, 202, 208, 146, 60, 118, 187, 112, 52, 104, 158, 235, 13, 211, 61, 157, 168, 197, 230, 103, 117, 22, 50, 28, 203, 218, 72, 237, 153, 189, 61, 36, 203, 19, 150, 175, 136, 72, 110, 37, 221, 228, 101, 183, 127, 71, 135, 26, 235, 26, 138, 10, 0, 31, 134, 69, 22, 49, 184, 255, 187, 74, 119, 124, 74, 10, 119, 101, 58, 194, 146, 32, 90, 223, 220, 195, 187, 191, 11, 245, 199, 104, 215, 168, 192, 95, 95, 180, 113, 76, 236, 39, 252, 72, 235, 48, 239, 163, 15, 161, 66, 131, 83, 165, 177, 113, 90, 37, 23, 157, 144, 210, 193, 92, 183, 50, 158, 65, 73, 46, 10, 236, 181, 24, 226, 214, 33, 27, 241, 138, 127, 21, 55, 192, 231, 223, 152, 163, 234, 64, 59, 172, 20, 75, 216, 136, 187, 53, 53, 173, 104, 11, 57, 24, 250, 49, 247, 123, 229, 254, 235, 187, 156, 243, 89, 193, 140, 129, 109, 97, 73, 98, 12, 9, 64, 253, 111, 138, 208, 201, 106, 111, 79, 89, 161, 115, 138, 214, 175, 146, 151, 195, 89, 14, 189, 41, 17, 120, 22, 68, 230, 194, 3, 41, 4, 97, 4, 191, 128, 171, 194, 169, 138, 171, 40, 79, 120, 58, 246, 162, 159, 131, 138, 132, 240, 47, 182, 137, 166, 197, 176, 116, 156, 99, 210, 194, 90, 112, 46, 85, 149, 199, 103, 34, 9, 212, 74, 15, 24, 250, 203, 74, 115, 54, 41, 89, 100, 224, 132, 117, 195, 147, 153, 154, 71, 125, 46, 106, 150, 177, 103, 165, 75, 5, 65, 219, 235, 148, 251, 51, 152, 188, 215, 100, 55, 156, 109, 84, 30, 157, 99, 56, 85, 81, 204, 85, 223, 142, 33, 217, 64, 15, 238, 149, 39, 39, 179, 81, 13, 185, 18, 125, 40, 83, 163, 161, 141, 168, 213, 244, 146, 158, 111, 103, 37, 45, 208, 92, 27, 236, 214, 246, 237, 15, 198, 191, 159, 91, 129, 12, 21, 164, 143, 25, 84, 199, 4, 157, 87, 157, 154, 255, 241, 196, 218, 142, 18, 21, 223, 222, 197, 171, 163, 46, 70, 55, 5, 237, 118, 129, 96, 181, 73, 218, 8, 239, 177, 204, 71, 143, 210, 175, 115, 129, 71, 179, 108, 37, 93, 94, 9, 52, 203, 163, 32, 6, 166, 29, 148, 125, 125, 61, 244, 12, 3, 150, 217, 108, 12, 176, 170, 103, 221, 210, 205, 91, 51, 51, 222, 22, 35, 70, 18, 202, 43, 113, 37, 247, 138, 196, 233, 12, 163, 188, 247, 4, 102, 255, 158, 106, 192, 224, 31, 65, 49, 223, 76, 174, 10, 149, 10, 122, 252, 168, 1, 67, 242, 167, 10, 104, 247, 8, 112, 144, 106, 34, 15, 33, 142, 91, 139, 226, 173, 118, 115, 232, 12, 88, 58, 169, 237, 129, 202, 246, 101, 63, 112, 84, 237, 164, 189, 18, 156, 227, 130, 25, 171, 118, 8, 150, 136, 99, 212, 70, 147, 4, 181, 22, 8, 46, 209, 174, 224, 90, 246, 246, 230, 56, 11, 34, 40, 244, 34, 197, 28, 185, 246, 182, 116, 9, 185, 212, 245, 175, 121, 133, 44, 106, 157, 41, 115, 79, 195, 73, 161, 122, 128, 47, 207, 190, 88, 216, 104, 133, 9, 122, 127, 248, 148, 10, 211, 114, 93, 148, 224, 29, 229, 192, 215, 155, 185, 11, 113, 142, 25, 206, 187, 35, 153, 168, 11, 151, 51, 48, 196, 210, 244, 125, 51, 116, 114, 37, 68, 227, 103, 217, 39, 160, 5, 62, 126, 190, 202, 222, 11, 57, 137, 231, 238, 48, 212, 42, 44, 119, 115, 148, 95, 157, 99, 238, 166, 80, 81, 34, 148, 4, 28, 244, 116, 229, 133, 241, 239, 241, 172, 175, 250, 202, 239, 160, 77, 39, 204, 178, 187, 252, 244, 241, 201, 17, 67, 234, 88, 136, 199, 118, 15, 153, 131, 133, 124, 87, 191, 95, 155, 168, 98, 85, 216, 150, 79, 144, 99, 18, 147, 103, 147, 79, 138, 73, 11, 15, 69, 240, 68, 186, 33, 190, 58, 16, 74, 134, 45, 213, 54, 180, 125, 49, 104, 52, 84, 69, 232, 215, 66, 243, 252, 181, 119, 194, 22, 223, 1, 4, 26, 213, 61, 186, 214, 195, 66, 187, 224, 121, 12, 193, 64, 188, 195, 198, 104, 13, 59, 243, 77, 118, 116, 137, 28, 122, 106, 150, 141, 63, 230, 201, 72, 15, 221, 123, 93, 147, 209, 84, 73, 233, 160, 49, 138, 32, 244, 203, 62, 165, 99, 124, 191, 141, 214, 38, 60, 137, 231, 216, 175, 111, 177, 176, 59, 189, 39, 54, 25, 85, 169, 13, 234, 201, 193, 80, 26, 201, 230, 12, 250, 115, 223, 86, 82, 204, 30, 0, 80, 230, 88, 255, 160, 212, 43, 121, 96, 86, 249, 36, 200, 94, 83, 158, 221, 63, 139, 26, 31, 247, 94, 119, 161, 175, 22, 120, 254, 29, 27, 122, 128, 5, 151, 61, 251, 141, 91, 96, 72, 166, 32, 171, 156, 53, 224, 238, 109, 74, 31, 167, 96, 56, 133, 52, 240, 125, 104, 253, 41, 219, 165, 75, 151, 145, 42, 66, 111, 82, 110, 92, 149, 2, 236, 155, 139, 161, 221, 61, 117, 76, 151, 248, 170, 27, 76, 173, 252, 186, 153, 23, 5, 155, 61, 11, 168, 190, 65, 237, 125, 134, 59, 68, 98, 147, 209, 35, 107, 84, 101, 240, 104, 83, 52, 245, 18, 137, 128, 119, 49, 33, 212, 217, 248, 104, 49, 222, 150, 87, 74, 91, 50, 155, 194, 211, 89, 193, 81, 228, 94, 248, 150, 72, 81, 181, 235, 126, 49, 91, 172, 179, 153, 17, 234, 231, 15, 143, 217, 156, 169, 116, 67, 79, 104, 211, 154, 76, 224, 31, 8, 22, 227, 219, 119, 131, 207, 99, 162, 179, 169, 153, 212, 104, 75, 211, 51, 36, 223, 31, 188, 2, 214, 84, 217, 44, 23, 132, 7, 243, 237, 104, 175, 133, 10, 189, 202, 153, 107, 32, 207, 202, 145, 44, 214, 236, 105, 118, 219, 41, 98, 95, 198, 51, 216, 28, 9, 170, 53, 81, 29, 229, 5, 115, 64, 194, 168, 243, 207, 117, 234, 145, 110, 119, 17, 231, 172, 195, 99, 160, 231, 189, 16, 251, 92, 61, 7, 6, 224, 85, 94, 146, 81, 252, 9, 219, 165, 72, 66, 221, 193, 153, 53, 119, 170, 215, 160, 74, 245, 194, 241, 200, 219, 220, 136, 244, 227, 145, 148, 252, 9, 163, 30, 87, 73, 200, 88, 67, 112, 162, 105, 33, 21, 209, 208, 21, 214, 208, 240, 186, 31, 238, 169, 27, 8, 49, 109, 231, 105, 4, 228, 183, 136, 26, 118, 159, 116, 153, 86, 17, 193, 82, 228, 248, 9, 150, 208, 116, 14, 46, 255, 117, 136, 144, 171, 68, 45, 142, 172, 125, 41, 104, 56, 153, 66, 221, 168, 16, 174, 15, 29, 216, 248, 141, 234, 182, 39, 107, 62, 239, 0, 34, 167, 89, 98, 177, 88, 252, 152, 217, 147, 251, 21, 154, 156, 168, 146, 87, 164, 57, 182, 75, 147, 181, 202, 122, 171, 100, 53, 78, 56, 88, 204, 125, 190, 78, 176, 83, 119, 189, 18, 109, 134, 190, 137, 163, 251, 112, 229, 96, 68, 156, 129, 7, 67, 44, 47, 52, 254, 67, 76, 66, 130, 133, 222, 160, 35, 64, 157, 174, 80, 182, 129, 100, 240, 50, 175, 156, 46, 219, 213, 251, 243, 159, 192, 11, 65, 129, 26, 213, 159, 54, 96, 184, 17, 129, 117, 239, 227, 26, 119, 111, 118, 133, 243, 224, 103, 132, 168, 228, 202, 107, 181, 225, 13, 214, 103, 161, 210, 143, 77, 57, 170, 147, 54, 8, 75, 78, 39, 38, 19, 25, 123, 161, 9, 193, 16, 83, 114, 33, 242, 156, 18, 29, 194, 161, 168, 16, 120, 160, 198, 107, 32, 232, 45, 20, 57, 143, 45]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eRvwZ5Nf7g6"
      },
      "source": [
        "k = 8       # Number of information bits per message, i.e., M=2**k\n",
        "n = 4       # Number of real channel uses per message\n",
        "seed = 2    # Seed RNG reproduce identical results"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28KCbvYif7hD"
      },
      "source": [
        "#### The Autoencoder Class\n",
        "In order to quickly experiment with different architecture and parameter choices, it is useful to create a Python class that has functions for training and inference. Each autoencoder instance has its own Tensorflow session and graph. Thus, you can have multiple instances running at the same time without interference between them."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######### CREATING A PATH LOSS MODEL OUTSIDE CREATE GRAPH(MAIN LOOP)\n",
        "#IEEE 802.11p is an approved amendment to the IEEE 802.11 standard to add wireless access in vehicular environments (WAVE), a vehicular communication system. \n",
        "#IEEE 802.11p standard typically uses channels of 10 MHz bandwidth in the 5.9 GHz band (5.8505.925 GHz).\n",
        "s = np.floor(np.random.uniform(0,M, batch_size))\n",
        "s = s.astype('int64')\n",
        "f = 5.9*10**9;#in Hz corresponding to IEEE 802.11p\n",
        "lamda = 0.05;  #in metres\n",
        "Pt = 1; #BS transmitted power in watts\n",
        "Lo = 8;   #Total system losses in dB\n",
        "Nf = 5;    #Mobile receiver noise figure in dB\n",
        "T = 290;   #temperature in degree kelvin\n",
        "BW = 10*10**6; #in Hz\n",
        "Gb = 8;  #in dB\n",
        "Gm = 0;   #in dB\n",
        "Hb = 1;  #in metres\n",
        "Hm = 1.1;   #in metres\n",
        "B = 1.38*10**-23; #Boltzmann's constant\n",
        "Te = T*(3.162-1)\n",
        "Pn = B*(Te+T)*BW\n",
        "Free_Lp = {}\n",
        "Pr = {}\n",
        "\n",
        "SNR_var = {}\n",
        "#Calculations&Results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i in range(1000):\n",
        "\n",
        "  Free_Lp[i] = 20*math.log10(Hm*Hb/s[i]**2)\n",
        "  Pr[i] = Free_Lp[i]-Lo+Gm+Gb+Pt  #in dBW\n",
        "  SNR_var[i] = Pr[i]-10*math.log10(Pn) #Provides SNR (stddev) values for all distances spanning s\n",
        "\n",
        "ad_noise_std = []  #The values of the dictionary, finally providing an array of stdevs\n",
        "for i in SNR_var.values():\n",
        "  ad_noise_std.append(i)\n",
        "\n",
        "\n",
        "ad_noise_std = np.tile(ad_noise_std, (2,2,1))\n",
        "ad_noise_std = np.transpose(ad_noise_std)\n",
        "print(np.shape(ad_noise_std)) \n",
        "print((ad_noise_std)) \n",
        "#final_ad_noise_std = {}\n",
        "\n",
        "#for i in range(1000):\n",
        "#  final_ad_noise_std[i] = np.random.normal(0.0, ad_noise_std[i], (2, 2))\n",
        "              #final_ad_noise_std[i] = tf.random_normal([batch_size,2,2], mean=0.0, stddev=ad_noise_std[i])\n",
        "\n",
        "#final_ad_noise_std = final_ad_noise_std.values()\n",
        "            #noise = []\n",
        "            #for i in final_ad_noise_std.values():\n",
        "              #noise.append(i)\n",
        "\n",
        "            #noise = np.asarray(noise, np.float32)\n",
        "            #noise = tf.convert_to_tensor(noise, np.float32)  \n",
        "\n",
        "for j in range(1,M+1):\n",
        "\n",
        "  Free_Lp[j] = 20*math.log10(Hm*Hb/j**2)\n",
        "  Pr[j] = Free_Lp[j]-Lo+Gm+Gb+Pt  #in dBW\n",
        "  SNR_var[j] = Pr[j]-10*math.log10(Pn)\n",
        "\n",
        "const_noise_std = []  #The values of the dictionary, finally providing an array of stdevs\n",
        "for j in SNR_var.values():\n",
        "  const_noise_std.append(i)  \n",
        "const_noise_std = np.transpose(np.tile(const_noise_std, (2,2,1)))\n",
        "\n",
        "for k in range(1000):\n",
        "\n",
        "  Free_Lp[k] = 20*math.log10(Hm*Hb/tr[k]**2)\n",
        "  Pr[k] = Free_Lp[k]-Lo+Gm+Gb+Pt  #in dBW\n",
        "  SNR_var[k] = Pr[k]-10*math.log10(Pn)\n",
        "\n",
        "tr_noise_std = []  #The values of the dictionary, finally providing an array of stdevs\n",
        "for k in SNR_var.values():\n",
        "  tr_noise_std.append(i)  \n",
        "tr_noise_std = np.transpose(np.tile(tr_noise_std, (2,2,1)))\n",
        " #Computations for the constellations!           "
      ],
      "metadata": {
        "id": "mpnHSzmqYXny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bf2fb25-0c90-4c71-b8f4-2256d22c5641"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 2, 2)\n",
            "[[[48.8367433  48.8367433 ]\n",
            "  [48.8367433  48.8367433 ]]\n",
            "\n",
            " [[86.24773011 86.24773011]\n",
            "  [86.24773011 86.24773011]]\n",
            "\n",
            " [[55.3458352  55.3458352 ]\n",
            "  [55.3458352  55.3458352 ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[44.96034278 44.96034278]\n",
            "  [44.96034278 44.96034278]]\n",
            "\n",
            " [[39.65532017 39.65532017]\n",
            "  [39.65532017 39.65532017]]\n",
            "\n",
            " [[42.00114068 42.00114068]\n",
            "  [42.00114068 42.00114068]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: RuntimeWarning: divide by zero encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:73: RuntimeWarning: divide by zero encountered in true_divide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs9Rtd01f7hG"
      },
      "source": [
        "\n",
        "class AE(object):\n",
        "    def __init__(self, k, n, seed=None, filename=None):\n",
        "        assert (n%2 == 0), \"Channel only allows complex symbols -> n must be a multiple of 2\"\n",
        "        self.k = k \n",
        "        self.n = n\n",
        "        self.n_complex = int(self.n/2)\n",
        "        self.bits_per_symbol = self.k/self.n_complex\n",
        "        self.M = 2**self.k\n",
        "        self.seed = seed if (seed is not None) else int(time.time())           \n",
        "        self.graph = None\n",
        "        self.sess = None   \n",
        "        self.vars = None\n",
        "        self.saver = None   \n",
        "        self.constellations = None\n",
        "        self.blers = None\n",
        "        self.create_graph()\n",
        "        self.create_session()\n",
        "        if filename is not None:    \n",
        "            self.load(filename)       \n",
        "        return\n",
        "    \n",
        "    def generate_distances(self,int_batch_size,bins):\n",
        "        # int_batch_size: 1000\n",
        "        # bins = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "\n",
        "        t = np.linspace(0,1,int_batch_size)\n",
        "        triangle1 = signal.sawtooth(2 * np.pi * 5 * t, 0.5)\n",
        "        triangle1 = 10*triangle1\n",
        "        triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "        triangle2 = signal.sawtooth(5 * np.pi * 5 * t, 0.5)\n",
        "        triangle2 = 10*triangle2\n",
        "        triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "        triangle3 = triangle1 + triangle2\n",
        "        triangle3 = triangle3/15*(7)\n",
        "        #print(s)\n",
        "\n",
        "        bins = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "        tr = np.digitize(triangle3, bins, right=True)\n",
        "        replacements = {0:7, 1:6, 2:5, 3:4, 4:3, 5:2, 6:1, 7:0}\n",
        "        replacer = replacements.get\n",
        "        tr = ([replacer(n, n) for n in tr])\n",
        "     \n",
        "        return tr\n",
        "\n",
        "\n",
        "    \n",
        "    def create_graph(self):\n",
        "        '''This function creates the computation graph of the autoencoder'''\n",
        "        self.graph = tf.Graph()        \n",
        "        with self.graph.as_default():    \n",
        "            tf.set_random_seed(self.seed)\n",
        "            batch_size = tf.placeholder(tf.int32, shape=())\n",
        "            \n",
        "            \n",
        "            # Transmitter\n",
        "            #s = tf.random_uniform(shape=[batch_size], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            #s = tf.random_uniform(shape=[1000], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            #s = tf.cast(tf.floor(tf.linspace(0.0, M, batch_size, name=\"linspace\")), tf.int64)\n",
        "            #print(\"Initial shape of S:\",s)\n",
        "            #s = tf.random_uniform(shape=[1000], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            \n",
        "            #plt.plot(t, triangle3, 'o')\n",
        "            #plt.plot(t, triangle3)\n",
        "            #print(s)\n",
        "            #plt.plot(t, s)\n",
        "            #s = tf.convert_to_tensor(s, dtype=tf.int64)\n",
        "            x = self.encoder(s)     \n",
        "            print(\"Initial shape of x:\",x)\n",
        "            \n",
        "            # Channel\n",
        "            noise_std = tf.placeholder(tf.float32, shape=[1000,2,2])\n",
        "            #ad_noise_std = (tf.linspace(0.0, noise_std, tf.shape(x), name=\"linspace\"))\n",
        "            #ad_noise_std = tf.random_normal(tf.shape(x), mean=0.0, stddev=7)\n",
        "            #print(\"Initial shape of ad_noise:\",ad_noise_std)\n",
        "            #noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=ad_SNR)\n",
        "            #x = self.encoder(s)     \n",
        "            \n",
        "            # Channel\n",
        "            #noise_std = tf.placeholder(tf.float32, shape=())\n",
        "            noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=noise_std) \n",
        "\n",
        "            fade = tf.random.normal(shape=tf.shape(x))\n",
        "            sparr1,sparr2 = tf.split(fade,num_or_size_splits=2, axis=2)\n",
        "            complex_fade = tf.complex(sparr1, sparr2)\n",
        "            fade = tf.abs(complex_fade)\n",
        "            #fade = tf.math.sqrt(1/2)*fade\n",
        "            print(\"Initial shape of fade:\",fade)\n",
        "            #print(\"Initial shape of noise:\",noise)\n",
        "            #fade = 1\n",
        "            y = tf.multiply(x,fade) + noise\n",
        "            print(\"Initial shape of y:\",y)\n",
        "           \n",
        "            # Receiver\n",
        "            s_hat = self.decoder(y)\n",
        "            correct_s_hat = tf.argmax(tf.nn.softmax(s_hat), axis=1)\n",
        "\n",
        "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat, weights= (s+1)**(4))\n",
        "\n",
        "\n",
        "            correct_predictions = tf.equal(tf.argmax(tf.nn.softmax(s_hat), axis=1), s)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "            bler = 1-accuracy\n",
        "\n",
        "            # Optimizer\n",
        "            lr = tf.placeholder(tf.float32, shape=()) # We can feed in any desired learning rate for each step     \n",
        "            train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
        "            #train_op_0 = tf.train.AdamOptimizer(lr).minimize(cross_entropy_0)\n",
        "            #lr = tf.train.exponential_decay(1e-10, global_step=cross_entropy, decay_steps=100, decay_rate=1.30)\n",
        "\n",
        "        \n",
        "            # References to graph variables we need to access later \n",
        "            self.vars = {\n",
        "                'accuracy': accuracy,\n",
        "                'batch_size': batch_size,\n",
        "                'bler': bler,\n",
        "                'cross_entropy': cross_entropy,\n",
        "                'init': tf.global_variables_initializer(),\n",
        "                'lr': lr,\n",
        "                'noise_std': noise_std,\n",
        "                'train_op': train_op,\n",
        "                's': s,\n",
        "                's_hat': s_hat,\n",
        "                'correct_s_hat': correct_s_hat,\n",
        "                'x': x,\n",
        "            }            \n",
        "            self.saver = tf.train.Saver()\n",
        "        return\n",
        "    \n",
        "    def create_session(self):\n",
        "        '''Create a session for the autoencoder instance with the compuational graph'''\n",
        "        self.sess = tf.Session(graph=self.graph)        \n",
        "        self.sess.run(self.vars['init'])\n",
        "        return\n",
        "    \n",
        "    def encoder(self, input):\n",
        "        '''The transmitter'''\n",
        "        self.weight_var_rec = self.weight_variable((self.M,self.M)) # shape = (8,8)\n",
        "        self.embedding_lookup_rec = tf.nn.embedding_lookup(self.weight_var_rec, input)\n",
        "        print(self.embedding_lookup_rec)\n",
        "        x = tf.nn.elu(self.embedding_lookup_rec)\n",
        "        #x = tf.layers.dense(self.embedding_lookup_rec, self.M, activation=tf.nn.relu)\n",
        "        x = tf.layers.dense(x, self.M, activation=None)\n",
        "        x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        x = tf.reshape(x, shape=[-1,self.n_complex,2])\n",
        "        print(x);\n",
        "        #Average power normalization\n",
        "        x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x))) \n",
        "        return x\n",
        "    \n",
        "    def decoder(self, input):\n",
        "        '''The Receiver'''\n",
        "        #input = self.flip_decoder(input)\n",
        "        y = tf.reshape(input, shape=[-1,self.n])\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        print(y);\n",
        "        return y\n",
        "    \n",
        "    def EbNo2Sigma(self, ebnodb):\n",
        "        '''Convert Eb/No in dB to noise standard deviation'''\n",
        "        ebno = 10**(ebnodb/10)\n",
        "        return 1/np.sqrt(2*self.bits_per_symbol*ebno)\n",
        "    \n",
        "    def gen_feed_dict(self, batch_size, ebnodb, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['lr']: lr,\n",
        "        }\n",
        "\n",
        "    def gen_e2e_feed_dict(self, batch_size, ebnodb, s_input):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['s']: s_input,\n",
        "        }   \n",
        "    \n",
        "    def load(self, filename):\n",
        "        '''Load an pre_trained model'''\n",
        "        return self.saver.restore(self.sess, filename)\n",
        "        \n",
        "    def plot_constellation(self, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = self.transmit(range(self.M))\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(self.n_complex):\n",
        "            image = plt.figure(figsize=(6,6))\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-maxrange,maxrange)\n",
        "            plt.ylim(-maxrange,maxrange)\n",
        "            for i in range(self.M):       \n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\",marker='x')   \n",
        "            image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            image.suptitle('%d. complex symbol' % (k+1))\n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "        return x, image\n",
        "    \n",
        "    def save(self, filename):\n",
        "        '''Save the current model'''\n",
        "        return self.saver.save(self.sess, filename)  \n",
        "    \n",
        "    def test_step(self, batch_size, ebnodb):\n",
        "        '''Compute the BLER over a single batch and Eb/No'''\n",
        "        bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n",
        "        return bler\n",
        "    \n",
        "    def transmit(self, s, noise_std):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['x'], feed_dict={self.vars['s']: s, self.vars['noise_std']: noise_std})\n",
        "\n",
        "    def end2end(self, batch_size, ebnodb, input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['correct_s_hat'], feed_dict=self.gen_e2e_feed_dict(batch_size, ebnodb, input_s)) \n",
        "        #print(self.sess.run(self.vars['correct_s_hat'], feed_dict={self.vars['s']: input_s}))     \n",
        "\n",
        "    #print(self.sess.run(self.vars['s_hat'], feed_dict={self.vars['s']: s}))\n",
        "\n",
        "    def train(self, training_params, validation_params):  \n",
        "        #s_input = self.generate_distances(100,np.array([0, 1, 2, 3, 4, 5, 6]))\n",
        "        \n",
        "        '''Training and validation loop'''\n",
        "        for index, params in enumerate(training_params):            \n",
        "            batch_size, lr, ebnodb, iterations = params            \n",
        "            print('\\nBatch Size: ' + str(batch_size) +\n",
        "                  ', Learning Rate: ' + str(lr) +\n",
        "                  ', EbNodB: ' + str(ebnodb) +\n",
        "                  ', Iterations: ' + str(iterations))\n",
        "            \n",
        "            val_size, val_ebnodb, val_steps = validation_params[index]\n",
        "            \n",
        "            for i in range(iterations):\n",
        "                self.train_step(batch_size, ebnodb, lr)    \n",
        "                if (i%val_steps==0):\n",
        "                    #bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_new_feed_dict(val_size, val_ebnodb, lr, s_input))\n",
        "                    bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size, val_ebnodb, lr))\n",
        "                    print(bler)                           \n",
        "        return       \n",
        "    \n",
        "    def train_step(self, batch_size, ebnodb, lr):\n",
        "        '''A single training step'''\n",
        "        #self.sess.run(self.vars['train_op'], feed_dict=self.gen_new_feed_dict(batch_size, ebnodb, lr, s_input))\n",
        "        self.sess.run(self.vars['train_op'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr))\n",
        "        return \n",
        "    \n",
        "    def weight_variable(self, shape):\n",
        "        '''Xavier-initialized weights optimized for ReLU Activations'''\n",
        "        (fan_in, fan_out) = shape\n",
        "        low = np.sqrt(6.0/(fan_in + fan_out)) \n",
        "        high = -np.sqrt(6.0/(fan_in + fan_out))\n",
        "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n",
        "    \n",
        "    def bler_sim(self, ebnodbs, batch_size, iterations):\n",
        "        '''Monte Carlo simulations of BLER for a range of Eb/No\n",
        "           Sometimes we to compute statistics for batch sizes that do not fit into the GPUs memory.\n",
        "           You can average over multiple batches with small size instead.           \n",
        "        '''\n",
        "        BLER = np.zeros_like(ebnodbs)\n",
        "        for i in range(iterations):\n",
        "            bler = np.array([self.sess.run(self.vars['bler'],\n",
        "                            feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0)) for ebnodb in ebnodbs])\n",
        "            BLER = BLER + bler/iterations\n",
        "        return BLER\n",
        "    \n",
        "    def plot_bler(self, EbNodB, BLER):\n",
        "        '''Plot a BLER curve'''\n",
        "        image = plt.figure(figsize=(10,8))\n",
        "        plt.plot(EbNodB, BLER, '-r', linewidth=2.0)\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('EbNo (dB)', fontsize=18)\n",
        "        plt.ylabel('Block-error rate', fontsize=18)\n",
        "        plt.grid(True)\n",
        "        plt.ylim([1e-5,1])\n",
        "        return image"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bANUdLIsf7hM"
      },
      "source": [
        "## Training parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeHghWVRf7hO",
        "outputId": "d07d2044-9e0e-4a26-da7b-55fa19b9aff8"
      },
      "source": [
        "train_EbNodB = ad_noise_std\n",
        "val_EbNodB = train_EbNodB\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "\n",
        "#epoch = [10000]\n",
        "\n",
        "#for i in epoch:\n",
        "training_params = [\n",
        "    #batch_size, lr, ebnodb, iterations\n",
        "    [1000 , lr, train_EbNodB, 10000]\n",
        "    ]\n",
        "\n",
        "validation_params = [\n",
        "    #batch_size, ebnodb, val_steps \n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000]\n",
        "    ]\n",
        "\n",
        "model_file_uw = 'models/ae_k_{}_n_{}'.format(k,n)\n",
        "ae = AE(k,n,seed)\n",
        "ae.train(training_params, validation_params)\n",
        "ae.save(model_file_uw);\n",
        "ae = AE(k,n,seed, filename=model_file_uw)\n",
        "  #ae.plot_constellation();\n",
        "  #ae.end2end(tr)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of x: Tensor(\"truediv:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of fade: Tensor(\"Abs:0\", shape=(1000, 2, 1), dtype=float32)\n",
            "Initial shape of y: Tensor(\"add:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n",
            "\n",
            "Batch Size: 1000, Learning Rate: 0.01, EbNodB: [[[38.85134115 38.85134115]\n",
            "  [38.85134115 38.85134115]]\n",
            "\n",
            " [[36.26098501 36.26098501]\n",
            "  [36.26098501 36.26098501]]\n",
            "\n",
            " [[36.18594481 36.18594481]\n",
            "  [36.18594481 36.18594481]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[46.92906368 46.92906368]\n",
            "  [46.92906368 46.92906368]]\n",
            "\n",
            " [[45.21030075 45.21030075]\n",
            "  [45.21030075 45.21030075]]\n",
            "\n",
            " [[40.594564   40.594564  ]\n",
            "  [40.594564   40.594564  ]]], Iterations: 10000\n",
            "0.965\n",
            "0.058000028\n",
            "0.04400003\n",
            "0.036000013\n",
            "0.035000026\n",
            "0.028999984\n",
            "0.038999975\n",
            "0.022000015\n",
            "0.032000005\n",
            "0.029999971\n",
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of x: Tensor(\"truediv:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of fade: Tensor(\"Abs:0\", shape=(1000, 2, 1), dtype=float32)\n",
            "Initial shape of y: Tensor(\"add:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl3hENsvf7hW"
      },
      "source": [
        "## Create and train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0GA7ihjf7hZ"
      },
      "source": [
        "#model_file = 'models/ae_k_{}_n_{}'.format(k,n)\n",
        "#ae = AE(k,n,seed)\n",
        "#ae.train(training_params, validation_params)\n",
        "#ae.save(model_file); # Save the trained autoencoder if you want to reuse it later\n",
        "#sess = tf.Session()\n",
        "#print(sess.run((rmse_uw_0)))"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzqpcwGaf7hm"
      },
      "source": [
        "## Evaluate trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y33vV4xKf7hn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d340f8ad-9ec6-4acc-bf2e-b7ac07321b0e"
      },
      "source": [
        "ae_uw = AE(k,n,seed, filename=model_file_uw) #Load a pretrained model that you have saved if needed"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of x: Tensor(\"truediv:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Initial shape of fade: Tensor(\"Abs:0\", shape=(1000, 2, 1), dtype=float32)\n",
            "Initial shape of y: Tensor(\"add:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fasS-Rz1lapW"
      },
      "source": [
        "\n",
        "class AE_Weighted(object):\n",
        "    def __init__(self, k, n, seed=None, filename=None):\n",
        "        assert (n%2 == 0), \"Channel only allows complex symbols -> n must be a multiple of 2\"\n",
        "        self.k = k \n",
        "        self.n = n\n",
        "        self.n_complex = int(self.n/2)\n",
        "        self.bits_per_symbol = self.k/self.n_complex\n",
        "        self.M = 2**self.k\n",
        "        self.seed = seed if (seed is not None) else int(time.time())           \n",
        "        self.graph = None\n",
        "        self.sess = None   \n",
        "        self.vars = None\n",
        "        self.saver = None   \n",
        "        self.constellations = None\n",
        "        self.blers = None\n",
        "        self.create_graph()\n",
        "        self.create_session()\n",
        "        if filename is not None:    \n",
        "            self.load(filename)       \n",
        "        return\n",
        "    \n",
        "    def generate_distances(self,int_batch_size,bins):\n",
        "        # int_batch_size: 1000\n",
        "        # bins = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "\n",
        "        t = np.linspace(0,1,int_batch_size)\n",
        "        triangle1 = signal.sawtooth(2 * np.pi * 5 * t, 0.5)\n",
        "        triangle1 = 10*triangle1\n",
        "        triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "        triangle2 = signal.sawtooth(5 * np.pi * 5 * t, 0.5)\n",
        "        triangle2 = 10*triangle2\n",
        "        triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "        triangle3 = triangle1 + triangle2\n",
        "        triangle3 = triangle3/15*(7)\n",
        "        #print(s)\n",
        "\n",
        "        bins = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "        tr = np.digitize(triangle3, bins, right=True)\n",
        "        replacements = {0:7, 1:6, 2:5, 3:4, 4:3, 5:2, 6:1, 7:0}\n",
        "        replacer = replacements.get\n",
        "        tr = ([replacer(n, n) for n in tr])\n",
        "     \n",
        "        return tr\n",
        "\n",
        "\n",
        "    \n",
        "    def create_graph(self):\n",
        "        '''This function creates the computation graph of the autoencoder'''\n",
        "        self.graph = tf.Graph()        \n",
        "        with self.graph.as_default():    \n",
        "            tf.set_random_seed(self.seed)\n",
        "            batch_size = tf.placeholder(tf.int32, shape=())\n",
        "            \n",
        "            noise_std = tf.placeholder(tf.float32, shape=[1000,2,2])\n",
        "            #ad_noise_std = (tf.linspace(0.0, noise_std, tf.shape(x), name=\"linspace\"))\n",
        "            #ad_noise_std = tf.random_normal(tf.shape(x), mean=0.0, stddev=7)\n",
        "            #print(\"Initial shape of ad_noise:\",ad_noise_std)\n",
        "            #noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=ad_SNR)\n",
        "            x = self.encoder(s)     \n",
        "            \n",
        "            # Channel\n",
        "            #noise_std = tf.placeholder(tf.float32, shape=())\n",
        "            noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=noise_std) \n",
        "     \n",
        "\n",
        "            fade = tf.random.normal(shape=tf.shape(x))\n",
        "            sparr1,sparr2 = tf.split(fade,num_or_size_splits=2, axis=2)\n",
        "            complex_fade = tf.complex(sparr1, sparr2)\n",
        "            fade = tf.abs(complex_fade)\n",
        "            #fade = tf.math.sqrt(1/2)*fade\n",
        "\n",
        "            #fade = 1\n",
        "            y = tf.multiply(x,fade) + noise\n",
        "            \n",
        "           \n",
        "            # Receiver\n",
        "            s_hat = self.decoder(y)\n",
        "            correct_s_hat = tf.argmax(tf.nn.softmax(s_hat), axis=1)\n",
        "\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat, weights= (s+1)**(4))\n",
        "\n",
        "\n",
        "            correct_predictions = tf.equal(tf.argmax(tf.nn.softmax(s_hat), axis=1), s)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "            bler = 1-accuracy\n",
        "\n",
        "            # Optimizer\n",
        "            lr = tf.placeholder(tf.float32, shape=()) # We can feed in any desired learning rate for each step     \n",
        "            train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
        "            #train_op_0 = tf.train.AdamOptimizer(lr).minimize(cross_entropy_0)\n",
        "            #lr = tf.train.exponential_decay(1e-10, global_step=cross_entropy, decay_steps=100, decay_rate=1.30)\n",
        "\n",
        "        \n",
        "            # References to graph variables we need to access later \n",
        "            self.vars = {\n",
        "                'accuracy': accuracy,\n",
        "                'batch_size': batch_size,\n",
        "                'bler': bler,\n",
        "                'cross_entropy': cross_entropy,\n",
        "                'init': tf.global_variables_initializer(),\n",
        "                'lr': lr,\n",
        "                'noise_std': noise_std,\n",
        "                'train_op': train_op,\n",
        "                's': s,\n",
        "                's_hat': s_hat,\n",
        "                'correct_s_hat': correct_s_hat,\n",
        "                'x': x,\n",
        "            }            \n",
        "            self.saver = tf.train.Saver()\n",
        "        return\n",
        "    \n",
        "    def create_session(self):\n",
        "        '''Create a session for the autoencoder instance with the compuational graph'''\n",
        "        self.sess = tf.Session(graph=self.graph)        \n",
        "        self.sess.run(self.vars['init'])\n",
        "        return\n",
        "    \n",
        "    def encoder(self, input):\n",
        "        '''The transmitter'''\n",
        "        self.weight_var_rec = self.weight_variable((self.M,self.M)) # shape = (8,8)\n",
        "        self.embedding_lookup_rec = tf.nn.embedding_lookup(self.weight_var_rec, input)\n",
        "        print(self.embedding_lookup_rec)\n",
        "        x = tf.nn.elu(self.embedding_lookup_rec)\n",
        "        #x = tf.layers.dense(self.embedding_lookup_rec, self.M, activation=tf.nn.relu)\n",
        "        x = tf.layers.dense(x, self.M, activation=None)\n",
        "        x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        x = tf.reshape(x, shape=[-1,self.n_complex,2])\n",
        "        print(x);\n",
        "        #Average power normalization\n",
        "        x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x))) \n",
        "        return x\n",
        "    \n",
        "    def decoder(self, input):\n",
        "        '''The Receiver'''\n",
        "        #input = self.flip_decoder(input)\n",
        "        y = tf.reshape(input, shape=[-1,self.n])\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        print(y);\n",
        "        return y\n",
        "    \n",
        "    def EbNo2Sigma(self, ebnodb):\n",
        "        '''Convert Eb/No in dB to noise standard deviation'''\n",
        "        ebno = 10**(ebnodb/10)\n",
        "        return 1/np.sqrt(2*self.bits_per_symbol*ebno)\n",
        "    \n",
        "    def gen_feed_dict(self, batch_size, ebnodb, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['lr']: lr,\n",
        "        }\n",
        "\n",
        "    def gen_e2e_feed_dict(self, batch_size, ebnodb, s_input):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['s']: s_input,\n",
        "        }   \n",
        "    \n",
        "    def load(self, filename):\n",
        "        '''Load an pre_trained model'''\n",
        "        return self.saver.restore(self.sess, filename)\n",
        "        \n",
        "    def plot_constellation(self, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = self.transmit(range(self.M))\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(self.n_complex):\n",
        "            image = plt.figure(figsize=(6,6))\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-maxrange,maxrange)\n",
        "            plt.ylim(-maxrange,maxrange)\n",
        "            for i in range(self.M):       \n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\",marker='x')   \n",
        "            image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            image.suptitle('%d. complex symbol' % (k+1))\n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "        return x, image\n",
        "    \n",
        "    def save(self, filename):\n",
        "        '''Save the current model'''\n",
        "        return self.saver.save(self.sess, filename)  \n",
        "    \n",
        "    def test_step(self, batch_size, ebnodb):\n",
        "        '''Compute the BLER over a single batch and Eb/No'''\n",
        "        bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n",
        "        return bler\n",
        "    \n",
        "    def transmit(self, s, noise_std):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['x'], feed_dict={self.vars['s']: s, self.vars['noise_std']: noise_std})\n",
        "\n",
        "    def end2end(self, batch_size, ebnodb, input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['correct_s_hat'], feed_dict=self.gen_e2e_feed_dict(batch_size, ebnodb, input_s)) \n",
        "        #print(self.sess.run(self.vars['correct_s_hat'], feed_dict={self.vars['s']: input_s}))     \n",
        "\n",
        "    #print(self.sess.run(self.vars['s_hat'], feed_dict={self.vars['s']: s}))\n",
        "\n",
        "    def train(self, training_params, validation_params):  \n",
        "        #s_input = self.generate_distances(100,np.array([0, 1, 2, 3, 4, 5, 6]))\n",
        "        \n",
        "        '''Training and validation loop'''\n",
        "        for index, params in enumerate(training_params):            \n",
        "            batch_size, lr, ebnodb, iterations = params            \n",
        "            print('\\nBatch Size: ' + str(batch_size) +\n",
        "                  ', Learning Rate: ' + str(lr) +\n",
        "                  ', EbNodB: ' + str(ebnodb) +\n",
        "                  ', Iterations: ' + str(iterations))\n",
        "            \n",
        "            val_size, val_ebnodb, val_steps = validation_params[index]\n",
        "            \n",
        "            for i in range(iterations):\n",
        "                self.train_step(batch_size, ebnodb, lr)    \n",
        "                if (i%val_steps==0):\n",
        "                    #bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_new_feed_dict(val_size, val_ebnodb, lr, s_input))\n",
        "                    bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size, val_ebnodb, lr))\n",
        "                    print(bler)                           \n",
        "        return       \n",
        "    \n",
        "    def train_step(self, batch_size, ebnodb, lr):\n",
        "        '''A single training step'''\n",
        "        #self.sess.run(self.vars['train_op'], feed_dict=self.gen_new_feed_dict(batch_size, ebnodb, lr, s_input))\n",
        "        self.sess.run(self.vars['train_op'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr))\n",
        "        return \n",
        "    \n",
        "    def weight_variable(self, shape):\n",
        "        '''Xavier-initialized weights optimized for ReLU Activations'''\n",
        "        (fan_in, fan_out) = shape\n",
        "        low = np.sqrt(6.0/(fan_in + fan_out)) \n",
        "        high = -np.sqrt(6.0/(fan_in + fan_out))\n",
        "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n",
        "    \n",
        "    def bler_sim(self, ebnodbs, batch_size, iterations):\n",
        "        '''Monte Carlo simulations of BLER for a range of Eb/No\n",
        "           Sometimes we to compute statistics for batch sizes that do not fit into the GPUs memory.\n",
        "           You can average over multiple batches with small size instead.           \n",
        "        '''\n",
        "        BLER = np.zeros_like(ebnodbs)\n",
        "        for i in range(iterations):\n",
        "            bler = np.array([self.sess.run(self.vars['bler'],\n",
        "                            feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0)) for ebnodb in ebnodbs])\n",
        "            BLER = BLER + bler/iterations\n",
        "        return BLER\n",
        "    \n",
        "    def plot_bler(self, EbNodB, BLER):\n",
        "        '''Plot a BLER curve'''\n",
        "        image = plt.figure(figsize=(10,8))\n",
        "        plt.plot(EbNodB, BLER, '-r', linewidth=2.0)\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('EbNo (dB)', fontsize=18)\n",
        "        plt.ylabel('Block-error rate', fontsize=18)\n",
        "        plt.grid(True)\n",
        "        plt.ylim([1e-5,1])\n",
        "        return image"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjh_J3MIlyrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8838cd71-8058-4fd6-c425-92c708ddf7ac"
      },
      "source": [
        "train_EbNodB = ad_noise_std\n",
        "val_EbNodB = train_EbNodB\n",
        "\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "training_params = [\n",
        "    #batch_size, lr, ebnodb, iterations\n",
        "    [1000 , lr, train_EbNodB, 10000]\n",
        "    ]\n",
        "\n",
        "validation_params = [\n",
        "    #batch_size, ebnodb, val_steps \n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000]\n",
        "    ]\n",
        "\n",
        "model_file = 'models/ae_k_{}_n_{}'.format(k,n)\n",
        "ae_Weighted = AE_Weighted(k,n,seed)\n",
        "ae_Weighted.train(training_params, validation_params)\n",
        "ae_Weighted.save(model_file);\n",
        "ae_Weighted = AE_Weighted(k,n,seed, filename=model_file)"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n",
            "\n",
            "Batch Size: 1000, Learning Rate: 0.01, EbNodB: [[[38.85134115 38.85134115]\n",
            "  [38.85134115 38.85134115]]\n",
            "\n",
            " [[36.26098501 36.26098501]\n",
            "  [36.26098501 36.26098501]]\n",
            "\n",
            " [[36.18594481 36.18594481]\n",
            "  [36.18594481 36.18594481]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[46.92906368 46.92906368]\n",
            "  [46.92906368 46.92906368]]\n",
            "\n",
            " [[45.21030075 45.21030075]\n",
            "  [45.21030075 45.21030075]]\n",
            "\n",
            " [[40.594564   40.594564  ]\n",
            "  [40.594564   40.594564  ]]], Iterations: 10000\n",
            "0.99\n",
            "0.40100002\n",
            "0.338\n",
            "0.31699997\n",
            "0.339\n",
            "0.297\n",
            "0.29299998\n",
            "0.296\n",
            "0.24400002\n",
            "0.232\n",
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsoF-I7Rf7hy"
      },
      "source": [
        "### Plot of learned constellations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjGW8S_df7h0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "64f17eba-66a1-4d6c-99cb-9054c63762ba"
      },
      "source": [
        "import itertools\n",
        "def plot_constellation_2(ae, arr, noise_std, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = ae.transmit(arr, noise_std)\n",
        "        #marker = itertools.cycle(('1', '2', '3', '4', '^', '6', '7', '8'))\n",
        "        weights = [1,4,9,16,25,36,49,64]\n",
        "        #weights = [1,1,1,1,1,1,1,1]\n",
        "        #weights = [1,8,27,64,125,216,343,512]\n",
        "        #weights = [1,2,3,4,5,6,7,8]\n",
        "        #weights = [1,16,81,256,625, 1296, 2401, 4096]\n",
        "        #weights = [1,256,6561,65536, 390625, 1.6, 5.7, 16.7]\n",
        "        #x = ae.transmit(np.ones(ae.M)*n)\n",
        "        #print(ae.M)\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(ae.n_complex):\n",
        "            \n",
        "#            image = plt.figure(figsize=(6,6))\n",
        "            image = plt.plot()\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-2,2)\n",
        "            plt.ylim(-2,2)\n",
        "            #plt.show() \n",
        "            #plt.ion()\n",
        "            xshape = np.shape(x)\n",
        "            for i in range(xshape[0]):      \n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\")\n",
        "                plt.annotate(weights[i], (x[i,k,0],x[i,k,1]))\n",
        "\n",
        "\n",
        "                #plt.show()              \n",
        "                #plt.pause(1)\n",
        "                #plt.hold(True)\n",
        "            #image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            #image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            #image.suptitle('%d. complex symbol' % (k+1))\n",
        "            #image.canvas.draw()\n",
        "            #image.canvas.flush_events()\n",
        "            \n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "            #time.sleep(0.1)\n",
        "        return x, image\n",
        "\n",
        "#plot_constellation_2(ae,range(0,ae.M))\n",
        "plot_constellation_2(ae_Weighted, range(0, ae_Weighted.M), const_noise_std)"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-174-5775f75fcd85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#plot_constellation_2(ae,range(0,ae.M))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mplot_constellation_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mae_Weighted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mae_Weighted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconst_noise_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-174-5775f75fcd85>\u001b[0m in \u001b[0;36mplot_constellation_2\u001b[0;34m(ae, arr, noise_std, maxrange)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_constellation_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxrange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m'''Generate a plot of the current constellation'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;31m#marker = itertools.cycle(('1', '2', '3', '4', '^', '6', '7', '8'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m49\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-172-c982bd59fb15>\u001b[0m in \u001b[0;36mtransmit\u001b[0;34m(self, s, noise_std)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;34m'''Returns the transmitted sigals corresponding to message indices'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'noise_std'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnoise_std\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mend2end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mebnodb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMbROTHNMa79"
      },
      "source": [
        "**THE RMSE Calculations**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(range(0, ae_Weighted.M))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnxLldPLuBuQ",
        "outputId": "c835c6b0-fade-4dc8-cb31-ecf5134034c3"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "range(0, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7-W-Be2auJV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "3073b5dd-72f1-476d-d3da-28f4fb7ffbf9"
      },
      "source": [
        "def rmse(predictions,targets):\n",
        "  return (np.sqrt((np.subtract(predictions,targets) ** 2).mean()))   \n",
        "\n",
        "tr_hat = ae.end2end(len(tr), tr_noise_std, tr)\n",
        "#print(np.shape(tr))\n",
        "#print(np.shape(tr_hat))\n",
        "print(tr_hat)\n",
        "\n",
        "\n",
        "tr_hat_w = ae_Weighted.end2end(len(tr), tr_noise_std, tr)\n",
        "\n",
        "rmse_uw = {}\n",
        "for j in range(M):\n",
        "  rmse_uw[j] = rmse(([tr_hat[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "\n",
        "rmse_w = {}\n",
        "for j in range(M):\n",
        "  rmse_w[j] = rmse(([tr_hat_w[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "\n",
        "\n",
        "\n",
        "#message_factor = [15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,1]\n",
        "message_factor = np.flip(np.arange(M))\n",
        "message_factor[M-1] = 1\n",
        "rmse_uwA = []\n",
        "for i in rmse_uw.values():\n",
        "  rmse_uwA.append(i)\n",
        "\n",
        "rmse_wA = []\n",
        "for i in rmse_w.values():\n",
        "  rmse_wA.append(i)\n",
        "\n",
        "    \n",
        "#rmse_uw = [rmse_uw_0, rmse_uw_1, rmse_uw_2, rmse_uw_3, rmse_uw_4, rmse_uw_5, rmse_uw_6, rmse_uw_7, rmse_uw_8, rmse_uw_9, rmse_uw_10, rmse_uw_11, rmse_uw_12, rmse_uw_13, rmse_uw_14, rmse_uw_15]\n",
        "rmse_uw = (np.divide(rmse_uwA,message_factor))\n",
        "#rmse_w = [rmse_w_0, rmse_w_1, rmse_w_2, rmse_w_3, rmse_w_4, rmse_w_5, rmse_w_6, rmse_w_7, rmse_w_8, rmse_w_9, rmse_w_10, rmse_w_11, rmse_w_12, rmse_w_13, rmse_w_14, rmse_w_15]\n",
        "rmse_w = (np.divide(rmse_wA,message_factor))\n",
        "#message = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
        "message = np.arange(M)\n",
        "\n",
        "print(rmse_uw)\n",
        "plt.plot(message, rmse_uw, '-o');\n",
        "plt.plot(message, rmse_w, '-*');\n",
        "plt.xlabel('Message Bit')\n",
        "plt.ylabel('RMSE deviation log2')\n",
        "plt.legend(['Unweighted', 'Weighted(^2)'])\n",
        "\n",
        "\n",
        "print(np.sum(rmse_uw))\n",
        "print(np.sum(rmse_w))\n",
        "\n",
        "print(np.sum(rmse_uwA))\n",
        "print(np.sum(rmse_wA))"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-180-e84dab1d44c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtr_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend2end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_noise_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(np.shape(tr))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#print(np.shape(tr_hat))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-168-179e6ca2fadc>\u001b[0m in \u001b[0;36mend2end\u001b[0;34m(self, batch_size, ebnodb, input_s)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mend2end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mebnodb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;34m'''Returns the transmitted sigals corresponding to message indices'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'correct_s_hat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_e2e_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mebnodb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;31m#print(self.sess.run(self.vars['correct_s_hat'], feed_dict={self.vars['s']: input_s}))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-168-179e6ca2fadc>\u001b[0m in \u001b[0;36mgen_e2e_feed_dict\u001b[0;34m(self, batch_size, ebnodb, s_input)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'noise_std'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEbNo2Sigma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mebnodb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         }   \n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxostCb-vY1i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "ab171ae0-8e33-4749-91e9-b00cafc1df9b"
      },
      "source": [
        "ae.plot_constellation();"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-181-2d5d5bfc5e9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_constellation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-168-179e6ca2fadc>\u001b[0m in \u001b[0;36mplot_constellation\u001b[0;34m(self, maxrange)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplot_constellation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxrange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;34m'''Generate a plot of the current constellation'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmaxrange\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mmaxrange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: transmit() missing 1 required positional argument: 'noise_std'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtoc7OKCUgko"
      },
      "source": [
        "# 8-PSK Modulation\n",
        "#8PSK constellation \n",
        "#Demodulation matrx\n",
        "#Qfunction \n",
        "\n",
        "import numpy as np\n",
        "from scipy import special\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import subprocess\n",
        "import shlex\n",
        "\n",
        "\n",
        "#Generating constellation points\n",
        "s = np.zeros((8,2))\n",
        "s_comp = np.zeros((8,1))+1j*np.zeros((8,1))\n",
        "for i in range(8):\n",
        "\ts[i,:] = np.array(([np.cos(i*2*np.pi/8),np.sin(i*2*np.pi/8)])) #vector\n",
        "\ts_comp[i] = s[i,0]+1j*s[i,1] #equivalent complex number\n",
        "\n",
        "#Generating demodulation matrix\n",
        "A = np.zeros((8,2,2))\n",
        "A[0,:,:] = np.array(([np.sqrt(2)-1,1],[np.sqrt(2)-1,-1]))\n",
        "A[1,:,:] = np.array(([np.sqrt(2)+1,-1],[-(np.sqrt(2)-1),1]))\n",
        "A[2,:,:] = np.array(([-(np.sqrt(2)+1),1],[np.sqrt(2)+1,1]))\n",
        "A[3,:,:] = np.array(([np.sqrt(2)-1,1],[-(np.sqrt(2)+1),-1]))\n",
        "A[4,:,:] = np.array(([-(np.sqrt(2)-1),-1],[-(np.sqrt(2)-1),1]))\n",
        "A[5,:,:] = np.array(([-(np.sqrt(2)+1),1],[np.sqrt(2)-1,-1]))\n",
        "A[6,:,:] = np.array(([np.sqrt(2)+1,-1],[-(np.sqrt(2)+1),-1]))\n",
        "A[7,:,:] = np.array(([-(np.sqrt(2)-1),-1],[np.sqrt(2)+1,1]))\n",
        "\n",
        "#Gray code\n",
        "gray = np.zeros((8,3))\n",
        "gray[0,:] = np.array(([0,0,0]))\n",
        "gray[1,:] = np.array(([0,0,1]))\n",
        "gray[2,:] = np.array(([0,1,1]))\n",
        "gray[3,:] = np.array(([0,1,0]))\n",
        "gray[4,:] = np.array(([1,1,0]))\n",
        "gray[5,:] = np.array(([1,1,1]))\n",
        "gray[6,:] = np.array(([1,0,1]))\n",
        "gray[7,:] = np.array(([1,0,0]))\n",
        "\n",
        "\n",
        "#Q-function\n",
        "def qfunc(x):\n",
        "\treturn 0.5*special.erfc(x/np.sqrt(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LDujO11Ulo5"
      },
      "source": [
        "def decode(vec_comp):\n",
        "\tvec = np.zeros((2,1))\n",
        "\tvec[0] = np.real(vec_comp)\n",
        "\tvec[1] = np.imag(vec_comp)\n",
        "\tfor i in range(8):\n",
        "\t\ty = A[i,:,:]@vec\n",
        "\t\tif (y [0] >= 0) and (y[1] >= 0):\n",
        "\t\t\treturn s_comp[i]\n",
        "\n",
        "#Extracting bits from demodulated symbols\n",
        "def detect(vec_comp):\n",
        "\tvec = np.zeros((2,1))\n",
        "\tvec[0] = np.real(vec_comp)\n",
        "\tvec[1] = np.imag(vec_comp)\n",
        "\tfor i in range(8):\n",
        "\t\tif s[i,0]==vec[0] and s[i,1] == vec[1]:\n",
        "\t\t\treturn gray[i,:]\n",
        "\n",
        "#Demodulating symbol stream from received noisy  symbols\n",
        "def rx_symb(mat):\n",
        "\tlen = mat.shape[1]\n",
        "\trx_symb_stream = []\n",
        "\tfor i in range(len):\n",
        "\t\trx_symb_stream.append(decode(mat[:,i]))\n",
        "\treturn rx_symb_stream\n",
        "\n",
        "#Getting received bit stream from demodulated symbols\n",
        "def rx_bit(mat):\n",
        "\tlen = mat.shape[1]\n",
        "\trx_bit_stream = []\n",
        "\tfor i in range(len):\n",
        "\t\trx_bit_stream.append(detect(mat[:,i]))\n",
        "\treturn rx_bit_stream"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPu3HX_SUov_"
      },
      "source": [
        "#Generates a bitstream\n",
        "def bitstream(n):\n",
        "\treturn np.random.randint(0,2,n)\n",
        "\n",
        "#Converts bits to 8-PSK symbols using gray code\n",
        "def mapping(b0,b1,b2):\n",
        "\tif (b0 == 0 and b1 == 0 and b2 == 0):\n",
        "\t\treturn s[0,:]\n",
        "\telif (b0 == 0 and b1 == 0 and b2 == 1):\n",
        "\t\treturn s[1,:]\n",
        "\telif (b0 == 0 and b1 == 1 and b2 == 1):\n",
        "\t\treturn s[2,:]\n",
        "\telif (b0 == 0 and b1 == 1 and b2 == 0):\n",
        "\t\treturn s[3,:]\n",
        "\telif( b0 == 1 and b1 == 1 and b2 == 0):\n",
        "\t\treturn s[4,:]\n",
        "\telif(b0==1 and b1 == 1 and b2 == 1):\n",
        "\t\treturn s[5,:]\n",
        "\telif(b0==1 and b1 == 0 and b2 == 1):\n",
        "\t\treturn s[6,:]\n",
        "\telif(b0==1 and b1 == 0 and b2 == 0):\n",
        "\t\treturn s[7,:]\n",
        "\n",
        "\n",
        "#Converts bitstream to 8-PSK symbol stream\n",
        "def symb(bits):\n",
        "\tsymbol =[]\n",
        "\ti = 0\n",
        "\twhile(1):\n",
        "\t\ttry:\n",
        "\t\t\tsymbol.append(mapping(bits[i],bits[i+1],bits[i+2]))\n",
        "\t\t\ti = i+3\n",
        "\t\texcept IndexError:\n",
        "\t\t\treturn symbol\n",
        "\n",
        "#Converts bitstream to 8-PSK complex symbol stream\n",
        "def CompSymb(bits):\n",
        "\tsymbols_lst = symb(bits)\n",
        "\tsymbols = np.array(symbols_lst).T #Symbol vectors\n",
        "\tsymbols_comp = symbols[0,:]+1j*symbols[1,:] #Equivalent complex symbols\n",
        "\treturn symbols_comp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPNtUtWBUryp"
      },
      "source": [
        "\n",
        "#SNR range\n",
        "snrlen=15\n",
        "\n",
        "#SNR in dB and actual per bit \n",
        "#(Check Proakis for factor of 6)\n",
        "snr_db = np.linspace(0,snrlen,snrlen)\n",
        "snr = 6*10**(0.1*snr_db)\n",
        "\n",
        "#Bitstream size\n",
        "bitsimlen = 99999\n",
        "\n",
        "#Symbol stream size\n",
        "simlen = bitsimlen //3\n",
        "\n",
        "#Generating bitstream\n",
        "bits = bitstream(bitsimlen)\n",
        "\n",
        "#Converting bits to Gray coded 8-PSK symbols\n",
        "#Intermediate steps  required for converting list to\n",
        "#numpy matrix\n",
        "symbols_lst = symb(bits)\n",
        "symbols = np.array(symbols_lst).T #Symbol vectors\n",
        "symbols_comp = symbols[0,:]+1j*symbols[1,:] #Equivalent complex symbols\n",
        "\n",
        "ser =[]\n",
        "ser_anal=[]\n",
        "ber = []\n",
        "\n",
        "#SNRloop\n",
        "for k in range(0,snrlen):\n",
        "\treceived = []\n",
        "\tt=0\n",
        "\t#Complex noise\n",
        "\tnoise_comp = np.random.normal(0,1,simlen)+1j*np.random.normal(0,1,simlen)\n",
        "\t#Generating complex received symbols\n",
        "  #fade_comp = np.random.normal(0,1,simlen)+1j*np.random.normal(0,1,simlen)\n",
        "  #fade_comp = np.abs(fade_comp)\n",
        "  #fade_comp = np.math.sqrt(1/2)*fade_comp\n",
        "\ty_comp = np.sqrt(snr[k])*symbols_comp +noise_comp\n",
        "\tbrx = []\n",
        "\tfor i in range(simlen):\n",
        "\t\tsrx_comp = decode(y_comp[i]) #Received Symbol\n",
        "\t\tbrx.append(detect(srx_comp))  #Received Bits\n",
        "\t\tif symbols_comp[i]==srx_comp:\n",
        "\t\t\tt+=1; #Counting symbol errors\n",
        "\t#Evaluating SER\n",
        "\tser.append(1-(t/33334.0))\n",
        "\tser_anal.append(2*qfunc((np.sqrt(snr[k]))*np.sin(np.pi/8)))\n",
        "\t#Received bitstream\n",
        "\tbrx=np.array(brx).flatten()\n",
        "\t#Evaluating BER\n",
        "\tbit_diff = bits-brx\n",
        "\tber.append(1-len(np.where(bit_diff == 0)[0])/bitsimlen)\n",
        "\n",
        "\n",
        "\n",
        "#Plots\n",
        "plt.semilogy(snr_db,ser_anal,label='SER Analysis')\n",
        "plt.semilogy(snr_db,ser,'o',label='SER Sim')\n",
        "plt.semilogy(snr_db,ber,label='BER Sim')\n",
        "plt.xlabel('SNR$\\\\left(\\\\frac{E_b}{N_0}\\\\right)$')\n",
        "plt.ylabel('$P_e$')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFZCQNe9f7iD"
      },
      "source": [
        "### BLER Simulations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wukzCBJff7iE"
      },
      "source": [
        " ebnodbs = np.linspace(0,14,15)\n",
        "BLER_8PSK = [0.3478959, 0.2926128, 0.2378847, 0.1854187, 0.1372344, 0.0953536, 0.0614003, 0.0360195, 0.0185215, 0.0082433, 0.0030178, 0.0008626, 0.0001903, 0.0000289, 0.0000027, ]\n",
        "blers = ae.bler_sim(ebnodbs, 1000000, 1);\n",
        "ae.plot_bler(ebnodbs, blers);\n",
        "blers_w = ae_Weighted.bler_sim(ebnodbs, 1000000, 1);\n",
        "#ae_Weighted.plot_bler(ebnodbs, blers_w);\n",
        "plt.plot(ebnodbs, blers_w)\n",
        "plt.semilogy(snr_db,ser,'o')\n",
        "plt.plot(ebnodbs,BLER_8PSK);\n",
        "plt.legend(['Autoencoder (Rayleigh+AWGN)', 'Weighted Autoencoder(Rayleigh+AWGN)', 'SER Sim(AWGN)', '8PSK(AWGN)'], prop={'size': 16}, loc='lower left');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMl-Rljrf7iR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}