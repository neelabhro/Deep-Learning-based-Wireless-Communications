{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Semantic_PathLoss.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neelabhro/Deep-Learning-based-Wireless-Communications/blob/main/Semantic_PathLoss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIoYASfEf7go"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKl9e4A0H1lk",
        "outputId": "bda70c12-850f-44d4-a025-1fd7146d48c7"
      },
      "source": [
        "# magic command to use TF 1.X in colaboraty when importing tensorflow\n",
        "%tensorflow_version 1.x \n",
        "import tensorflow as tf                       # imports the tensorflow library to the python kernel\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)    # sets the amount of debug information from TF (INFO, WARNING, ERROR)\n",
        "import time\n",
        "import random\n",
        "print(\"Using tensorflow version:\", tf.__version__)\n",
        "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using tensorflow version: 1.15.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiCDuiGqf7gr"
      },
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "import math\n",
        "pi = tf.constant(math.pi)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrLsO1Nxf7g3"
      },
      "source": [
        "#### System parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spN0MeqFD7x-",
        "outputId": "afd55b26-ccdd-494b-b140-f5cd4c77c1b8"
      },
      "source": [
        "batch_size = 1000\n",
        "\n",
        "#print(tr)\n",
        "#plt.plot(t, tr)\n",
        "T1 = random.randint(1, 10)\n",
        "T2 = random.randint(1, 10)\n",
        "M = 256\n",
        "t = np.linspace(0, 1, batch_size)\n",
        "triangle1 = signal.sawtooth(T1 * np.pi * 5 * t, 0.5)\n",
        "triangle1 = M/2*(triangle1)\n",
        "triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "triangle2 = signal.sawtooth(T2 * np.pi * 5 * t, 0.5)\n",
        "triangle2 = M/2*(triangle2)\n",
        "triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "triangle3 = triangle1 + triangle2\n",
        "#triangle3 = triangle3/15*(7)\n",
        "tr = triangle3\n",
        "#bins = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])\n",
        "bins = np.arange(M-1)\n",
        "tr = np.digitize(triangle3, bins, right=True)\n",
        "#plt.plot(t, triangle3)\n",
        "\n",
        "#tr = np.flip(tr)\n",
        "#print(tr)\n",
        "tr = np.floor(np.random.uniform(0,M, 10000))\n",
        "print(tr)\n",
        "#replacements = {0:7, 1:6, 2:5, 3:4, 4:3, 5:2, 6:1, 7:0}\n",
        "#replacements = {0:15, 1:14, 2:13, 3:12, 4:11, 5:10, 6:9, 7:8, 8:7, 9:6, 10:5, 11:4, 12:3, 13:2, 14:1, 15:0}\n",
        "rp1 = np.arange(M)\n",
        "rp2 = np.flip(np.arange(M))\n",
        "replacements = dict(zip(rp1,rp2))\n",
        "#print(rp2)\n",
        "replacer = replacements.get\n",
        "tr = ([replacer(n, n) for n in tr])\n",
        "\n",
        "print(tr)\n",
        "\n",
        "s_ind = {}\n",
        "for j in range(M):\n",
        "  s_ind[j] = [i for i, x in enumerate(tr) if x == j]\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[175. 255.  61. ... 136.  80. 245.]\n",
            "[80, 0, 194, 52, 238, 21, 20, 29, 92, 194, 92, 222, 9, 101, 87, 129, 179, 158, 90, 8, 99, 168, 139, 218, 120, 103, 52, 187, 220, 65, 82, 59, 231, 16, 32, 72, 204, 167, 254, 70, 153, 229, 27, 0, 57, 102, 203, 97, 213, 180, 6, 93, 27, 232, 79, 158, 92, 205, 210, 175, 85, 132, 224, 96, 40, 69, 120, 46, 102, 145, 156, 114, 68, 87, 213, 160, 255, 226, 239, 27, 126, 213, 140, 201, 85, 165, 187, 193, 230, 48, 146, 155, 39, 2, 98, 54, 37, 57, 46, 242, 170, 161, 30, 130, 214, 239, 222, 199, 62, 75, 25, 46, 28, 128, 105, 85, 83, 69, 161, 189, 70, 198, 100, 195, 208, 81, 169, 31, 253, 119, 27, 83, 70, 86, 188, 135, 168, 44, 115, 103, 86, 8, 9, 152, 14, 4, 181, 156, 173, 205, 199, 199, 227, 204, 170, 215, 198, 207, 247, 231, 248, 213, 150, 125, 181, 164, 193, 120, 85, 116, 55, 221, 49, 159, 53, 96, 48, 12, 178, 186, 68, 180, 217, 17, 242, 235, 101, 93, 223, 252, 54, 27, 67, 133, 76, 11, 234, 150, 252, 207, 11, 255, 103, 49, 70, 191, 173, 65, 64, 14, 181, 229, 90, 223, 100, 129, 255, 121, 48, 136, 141, 26, 133, 85, 115, 49, 51, 182, 157, 27, 185, 231, 37, 221, 57, 253, 225, 89, 33, 92, 146, 165, 166, 135, 57, 86, 175, 184, 54, 161, 219, 202, 254, 232, 96, 187, 196, 22, 56, 161, 194, 96, 130, 250, 225, 145, 226, 44, 72, 92, 162, 226, 122, 49, 242, 47, 147, 22, 52, 234, 53, 159, 66, 124, 79, 231, 177, 109, 123, 155, 100, 230, 178, 69, 50, 75, 92, 65, 0, 91, 224, 100, 44, 233, 194, 194, 85, 134, 113, 247, 58, 179, 45, 133, 56, 207, 254, 244, 157, 8, 124, 8, 74, 139, 148, 164, 162, 253, 218, 192, 82, 206, 99, 203, 39, 180, 160, 249, 24, 39, 178, 9, 42, 125, 254, 252, 237, 246, 206, 159, 21, 26, 63, 230, 181, 145, 213, 208, 200, 60, 234, 32, 94, 39, 100, 106, 145, 39, 208, 92, 231, 252, 248, 30, 242, 13, 220, 255, 30, 169, 103, 98, 133, 170, 184, 16, 209, 231, 208, 46, 177, 103, 77, 41, 134, 40, 116, 143, 108, 14, 110, 13, 31, 196, 143, 245, 61, 248, 190, 15, 104, 25, 39, 181, 108, 99, 135, 13, 0, 69, 177, 87, 253, 211, 216, 26, 56, 20, 202, 240, 109, 21, 50, 230, 26, 233, 115, 31, 99, 155, 19, 55, 72, 213, 150, 24, 177, 165, 14, 215, 168, 157, 166, 16, 24, 19, 28, 53, 13, 89, 13, 157, 74, 15, 174, 108, 143, 216, 162, 112, 129, 56, 45, 182, 132, 131, 6, 177, 10, 6, 102, 50, 55, 206, 140, 217, 84, 36, 159, 223, 118, 56, 199, 248, 185, 75, 64, 80, 212, 33, 230, 41, 166, 47, 109, 30, 9, 3, 216, 147, 205, 251, 70, 191, 178, 173, 40, 111, 10, 38, 222, 15, 10, 173, 171, 98, 10, 208, 69, 90, 109, 50, 84, 112, 91, 187, 200, 50, 66, 30, 229, 47, 58, 175, 201, 30, 217, 206, 222, 124, 19, 27, 205, 221, 113, 249, 215, 176, 62, 175, 52, 34, 10, 103, 181, 110, 172, 81, 157, 76, 162, 28, 235, 169, 5, 161, 151, 225, 54, 45, 244, 97, 108, 189, 131, 216, 185, 241, 139, 217, 209, 111, 162, 231, 101, 126, 141, 52, 97, 144, 230, 191, 25, 116, 148, 163, 210, 137, 51, 221, 83, 25, 10, 122, 203, 241, 6, 94, 213, 89, 239, 245, 59, 141, 243, 149, 39, 62, 90, 201, 229, 138, 107, 205, 214, 76, 132, 29, 94, 17, 14, 170, 164, 144, 224, 192, 48, 213, 146, 184, 193, 2, 1, 158, 190, 167, 206, 19, 180, 22, 94, 120, 239, 93, 31, 33, 127, 181, 76, 222, 112, 71, 147, 56, 196, 138, 126, 112, 146, 118, 247, 47, 199, 213, 101, 62, 9, 191, 253, 142, 40, 214, 211, 148, 60, 23, 112, 46, 19, 65, 158, 141, 95, 95, 135, 118, 55, 97, 26, 68, 174, 86, 1, 59, 70, 33, 11, 59, 217, 154, 239, 199, 230, 104, 100, 133, 82, 253, 221, 142, 169, 89, 47, 55, 128, 252, 60, 166, 91, 121, 142, 95, 100, 156, 189, 85, 160, 115, 5, 44, 185, 11, 13, 39, 4, 149, 78, 83, 207, 187, 194, 254, 112, 26, 247, 24, 134, 2, 179, 64, 50, 215, 167, 181, 95, 253, 66, 131, 196, 80, 46, 138, 40, 57, 229, 126, 87, 148, 229, 139, 109, 253, 224, 26, 110, 154, 76, 146, 67, 104, 81, 66, 140, 31, 10, 238, 184, 138, 154, 92, 3, 57, 44, 158, 56, 148, 218, 63, 130, 193, 152, 7, 227, 144, 217, 206, 156, 167, 233, 9, 25, 102, 93, 27, 117, 165, 238, 246, 190, 27, 241, 42, 246, 16, 130, 246, 206, 41, 188, 70, 197, 9, 97, 249, 145, 51, 161, 43, 58, 183, 78, 33, 218, 43, 120, 249, 14, 79, 53, 215, 59, 217, 83, 36, 250, 29, 147, 194, 97, 141, 118, 55, 15, 62, 136, 77, 219, 130, 117, 144, 172, 127, 58, 191, 226, 183, 100, 186, 86, 89, 135, 131, 127, 105, 185, 23, 119, 126, 12, 244, 131, 137, 131, 220, 14, 112, 224, 60, 94, 82, 243, 110, 198, 193, 253, 126, 113, 16, 17, 28, 135, 88, 207, 2, 87, 99, 20, 117, 251, 11, 185, 5, 214, 146, 85, 50, 143, 225, 198, 87, 212, 89, 39, 34, 80, 141, 120, 60, 249, 79, 132, 15, 152, 252, 188, 210, 208, 255, 233, 234, 134, 184, 154, 8, 22, 145, 43, 217, 123, 145, 166, 83, 112, 217, 107, 5, 160, 219, 68, 234, 99, 149, 51, 85, 137, 253, 119, 192, 251, 72, 234, 94, 30, 138, 241, 166, 97, 34, 74, 151, 93, 137, 16, 104, 108, 71, 73, 45, 7, 152, 246, 76, 72, 23, 98, 236, 104, 254, 47, 17, 101, 2, 33, 174, 145, 184, 149, 21, 69, 22, 209, 176, 107, 226, 65, 169, 82, 222, 163, 218, 19, 55, 31, 207, 194, 195, 151, 54, 122, 127, 245, 56, 12, 150, 253, 164, 26, 249, 54, 79, 167, 32, 81, 160, 138, 57, 209, 32, 170, 7, 18, 39, 245, 24, 220, 36, 211, 16, 111, 18, 25, 186, 153, 164, 185, 209, 46, 140, 2, 12, 103, 83, 30, 107, 100, 183, 97, 176, 35, 54, 140, 47, 197, 17, 15, 117, 249, 248, 245, 112, 46, 107, 21, 198, 183, 221, 164, 87, 212, 125, 96, 25, 87, 216, 207, 145, 98, 126, 37, 205, 168, 163, 2, 159, 238, 74, 60, 61, 63, 252, 102, 230, 208, 130, 114, 29, 107, 33, 241, 108, 192, 63, 61, 197, 16, 48, 69, 135, 131, 76, 132, 5, 36, 133, 230, 81, 85, 191, 233, 139, 76, 238, 69, 75, 212, 88, 65, 9, 93, 250, 44, 34, 44, 14, 225, 164, 22, 44, 227, 72, 133, 84, 37, 119, 96, 123, 75, 249, 230, 96, 225, 63, 158, 115, 253, 14, 236, 124, 145, 152, 126, 79, 207, 247, 108, 76, 177, 78, 19, 143, 139, 26, 206, 207, 9, 201, 3, 204, 229, 198, 238, 212, 143, 153, 93, 173, 231, 116, 181, 153, 189, 202, 98, 123, 216, 170, 239, 166, 220, 98, 230, 134, 136, 228, 253, 116, 255, 229, 195, 138, 34, 218, 90, 245, 119, 169, 16, 132, 214, 143, 178, 67, 150, 85, 223, 236, 48, 192, 27, 12, 239, 231, 59, 148, 200, 157, 232, 80, 144, 8, 162, 63, 173, 29, 67, 108, 24, 46, 188, 48, 166, 187, 125, 103, 51, 122, 42, 140, 210, 34, 118, 133, 121, 237, 26, 52, 79, 118, 11, 127, 227, 234, 59, 143, 19, 140, 109, 2, 147, 173, 37, 113, 56, 90, 203, 108, 100, 32, 49, 241, 138, 158, 105, 209, 144, 215, 62, 57, 96, 140, 151, 2, 55, 193, 114, 117, 21, 104, 201, 57, 193, 87, 140, 212, 116, 199, 28, 254, 219, 46, 170, 214, 152, 199, 165, 227, 12, 161, 253, 251, 53, 208, 117, 108, 221, 19, 196, 72, 229, 138, 9, 24, 250, 43, 110, 65, 55, 231, 218, 25, 67, 251, 58, 216, 228, 184, 104, 231, 3, 103, 252, 190, 21, 11, 237, 13, 134, 217, 121, 200, 54, 207, 197, 200, 45, 248, 183, 57, 157, 62, 72, 149, 210, 79, 82, 163, 201, 160, 181, 238, 213, 52, 138, 107, 114, 55, 3, 140, 223, 59, 95, 140, 186, 175, 46, 157, 120, 153, 170, 77, 170, 245, 117, 58, 231, 142, 105, 164, 156, 136, 118, 82, 92, 53, 202, 106, 179, 12, 12, 140, 71, 30, 61, 102, 44, 157, 221, 144, 121, 126, 241, 247, 21, 45, 193, 4, 195, 154, 91, 239, 202, 241, 158, 126, 35, 66, 56, 34, 215, 62, 251, 143, 217, 118, 40, 243, 250, 233, 155, 106, 108, 112, 109, 170, 232, 175, 233, 206, 111, 143, 202, 95, 231, 66, 211, 185, 160, 148, 186, 207, 64, 111, 71, 6, 91, 240, 96, 139, 110, 241, 89, 56, 86, 19, 61, 23, 134, 229, 213, 66, 205, 100, 17, 175, 129, 148, 199, 238, 22, 102, 135, 60, 113, 9, 235, 39, 20, 43, 117, 159, 17, 50, 2, 214, 13, 124, 56, 248, 21, 95, 244, 120, 143, 2, 191, 109, 103, 190, 111, 77, 171, 177, 145, 232, 40, 122, 12, 33, 102, 167, 46, 111, 120, 117, 202, 24, 32, 12, 9, 18, 46, 45, 80, 172, 58, 96, 114, 226, 213, 245, 32, 20, 238, 26, 198, 120, 146, 50, 233, 245, 167, 213, 112, 140, 121, 200, 148, 23, 191, 37, 216, 101, 102, 59, 231, 123, 168, 46, 170, 1, 222, 245, 126, 16, 8, 110, 120, 244, 70, 77, 16, 48, 234, 94, 251, 105, 97, 93, 210, 65, 204, 192, 103, 175, 213, 208, 99, 5, 72, 60, 53, 147, 83, 189, 45, 114, 161, 34, 157, 74, 117, 251, 167, 24, 57, 95, 0, 135, 188, 128, 141, 86, 93, 162, 220, 58, 198, 166, 111, 32, 90, 3, 172, 85, 68, 6, 81, 31, 168, 160, 89, 131, 194, 164, 199, 11, 78, 17, 189, 91, 195, 13, 165, 122, 237, 95, 61, 245, 135, 115, 153, 131, 79, 1, 242, 175, 252, 168, 250, 189, 74, 212, 169, 38, 15, 56, 208, 63, 146, 200, 91, 127, 68, 14, 204, 92, 157, 122, 187, 132, 223, 64, 173, 13, 92, 124, 24, 216, 29, 154, 89, 56, 102, 36, 156, 168, 243, 229, 91, 233, 94, 120, 207, 1, 86, 222, 138, 255, 178, 132, 64, 132, 248, 236, 212, 186, 184, 23, 140, 49, 69, 200, 183, 30, 117, 147, 36, 53, 161, 247, 8, 61, 43, 18, 55, 139, 72, 250, 218, 178, 31, 177, 126, 4, 189, 41, 221, 191, 207, 232, 51, 110, 164, 153, 120, 47, 45, 148, 29, 218, 198, 81, 104, 205, 221, 189, 86, 91, 145, 24, 119, 177, 63, 168, 74, 18, 221, 106, 219, 225, 34, 212, 225, 95, 228, 193, 109, 22, 120, 118, 2, 59, 225, 91, 14, 48, 42, 234, 127, 175, 45, 92, 39, 162, 211, 59, 164, 250, 1, 23, 91, 181, 13, 114, 114, 48, 62, 142, 185, 31, 128, 178, 100, 159, 76, 250, 49, 193, 73, 255, 104, 58, 100, 91, 170, 237, 212, 68, 181, 18, 81, 81, 61, 220, 131, 37, 241, 198, 221, 217, 5, 34, 49, 58, 234, 82, 54, 138, 151, 28, 186, 208, 207, 48, 14, 96, 181, 50, 197, 94, 6, 203, 37, 213, 127, 2, 93, 29, 229, 169, 241, 52, 207, 215, 191, 119, 132, 211, 99, 69, 143, 12, 28, 98, 148, 239, 7, 96, 249, 154, 216, 254, 180, 8, 234, 26, 228, 175, 28, 123, 112, 61, 73, 178, 193, 184, 243, 66, 31, 96, 195, 244, 3, 87, 81, 231, 220, 85, 77, 82, 253, 176, 195, 179, 159, 45, 199, 203, 29, 246, 252, 178, 47, 138, 61, 175, 45, 196, 250, 157, 63, 144, 93, 70, 149, 146, 49, 40, 138, 143, 6, 83, 109, 154, 29, 46, 255, 86, 165, 161, 48, 68, 92, 211, 49, 121, 10, 14, 221, 235, 60, 248, 61, 165, 217, 99, 145, 102, 34, 195, 133, 172, 33, 85, 32, 190, 217, 87, 126, 205, 233, 65, 208, 74, 90, 4, 56, 47, 62, 43, 72, 248, 37, 183, 208, 125, 229, 70, 128, 2, 23, 134, 218, 203, 116, 16, 164, 186, 35, 73, 5, 205, 105, 147, 11, 56, 172, 57, 194, 56, 243, 43, 16, 152, 20, 216, 67, 62, 162, 40, 245, 90, 70, 87, 213, 213, 202, 202, 42, 251, 49, 30, 166, 252, 124, 22, 102, 83, 44, 24, 188, 94, 40, 220, 141, 186, 94, 114, 66, 89, 158, 60, 191, 79, 65, 193, 160, 198, 82, 222, 225, 127, 89, 193, 88, 217, 104, 109, 121, 26, 143, 219, 255, 90, 65, 94, 192, 202, 93, 116, 127, 236, 34, 32, 52, 20, 170, 42, 12, 214, 114, 57, 20, 202, 5, 64, 143, 11, 113, 145, 194, 67, 127, 255, 188, 50, 173, 71, 49, 230, 40, 186, 28, 253, 86, 87, 227, 111, 175, 102, 169, 186, 186, 169, 179, 193, 217, 201, 72, 199, 189, 68, 230, 104, 21, 10, 70, 243, 160, 2, 151, 197, 187, 194, 156, 213, 107, 76, 4, 100, 240, 30, 95, 184, 79, 193, 37, 243, 252, 139, 168, 202, 228, 50, 191, 202, 208, 188, 10, 55, 118, 232, 8, 5, 43, 57, 129, 93, 221, 116, 237, 239, 177, 125, 1, 27, 193, 55, 68, 164, 255, 89, 146, 168, 196, 224, 217, 81, 159, 44, 36, 89, 239, 28, 193, 224, 123, 214, 31, 185, 84, 180, 125, 163, 128, 219, 130, 186, 110, 1, 11, 229, 22, 51, 24, 33, 199, 209, 151, 79, 244, 123, 36, 206, 118, 62, 107, 224, 170, 4, 153, 236, 0, 238, 90, 34, 93, 58, 68, 11, 88, 121, 201, 26, 86, 242, 247, 197, 178, 58, 138, 92, 251, 0, 125, 53, 70, 154, 192, 200, 19, 181, 190, 203, 215, 201, 60, 57, 141, 82, 40, 161, 246, 6, 154, 70, 33, 193, 222, 138, 32, 87, 152, 115, 50, 216, 209, 114, 24, 153, 70, 4, 172, 88, 235, 8, 129, 48, 37, 75, 124, 161, 158, 77, 190, 219, 85, 137, 98, 231, 132, 162, 0, 236, 222, 86, 19, 145, 64, 10, 84, 162, 240, 158, 192, 82, 47, 115, 30, 180, 247, 30, 142, 99, 36, 59, 156, 159, 232, 203, 113, 140, 202, 206, 245, 24, 77, 131, 108, 206, 108, 4, 150, 174, 110, 55, 56, 193, 65, 47, 21, 125, 18, 132, 98, 39, 20, 63, 103, 194, 237, 138, 10, 238, 134, 91, 12, 19, 42, 67, 230, 37, 81, 30, 148, 163, 130, 251, 73, 68, 158, 91, 82, 75, 239, 134, 127, 59, 156, 254, 225, 152, 87, 180, 165, 248, 44, 39, 146, 1, 184, 189, 212, 78, 58, 173, 84, 239, 94, 222, 176, 126, 102, 240, 94, 240, 148, 158, 213, 45, 134, 104, 236, 147, 81, 212, 95, 121, 114, 16, 71, 96, 183, 113, 204, 137, 201, 239, 85, 222, 215, 15, 109, 226, 20, 232, 92, 197, 230, 206, 184, 178, 150, 51, 244, 152, 16, 8, 213, 206, 148, 135, 107, 106, 170, 171, 11, 243, 133, 73, 144, 115, 21, 180, 247, 107, 191, 226, 251, 101, 252, 3, 154, 237, 209, 20, 111, 246, 126, 179, 255, 102, 132, 11, 170, 243, 208, 206, 76, 152, 216, 88, 233, 195, 14, 89, 173, 124, 193, 97, 218, 156, 26, 217, 52, 185, 240, 81, 163, 12, 79, 8, 2, 83, 161, 35, 37, 99, 155, 126, 7, 2, 44, 152, 105, 110, 205, 83, 152, 38, 112, 205, 196, 46, 145, 45, 187, 40, 162, 153, 187, 81, 28, 244, 55, 248, 110, 92, 118, 11, 253, 241, 238, 181, 97, 121, 45, 55, 48, 223, 40, 93, 212, 77, 246, 168, 75, 242, 43, 184, 241, 0, 110, 141, 246, 97, 184, 215, 169, 115, 171, 101, 21, 11, 20, 175, 211, 105, 144, 237, 20, 211, 104, 80, 221, 112, 197, 143, 4, 155, 174, 122, 217, 202, 158, 105, 41, 212, 43, 228, 31, 186, 83, 20, 222, 215, 141, 163, 172, 142, 251, 87, 93, 219, 208, 32, 242, 32, 208, 106, 154, 117, 118, 64, 46, 32, 89, 38, 82, 213, 147, 157, 22, 115, 238, 57, 252, 98, 153, 255, 212, 27, 218, 243, 210, 32, 79, 71, 105, 65, 43, 255, 142, 54, 216, 210, 213, 103, 144, 10, 157, 175, 83, 46, 223, 248, 56, 51, 94, 114, 52, 122, 181, 136, 31, 160, 126, 165, 53, 254, 151, 230, 59, 77, 134, 204, 210, 120, 59, 92, 19, 83, 142, 226, 65, 194, 15, 196, 93, 31, 10, 238, 44, 6, 33, 181, 16, 114, 138, 195, 233, 48, 45, 143, 255, 22, 87, 100, 24, 211, 105, 116, 17, 98, 200, 91, 196, 239, 203, 176, 219, 80, 3, 208, 151, 163, 69, 97, 247, 47, 178, 16, 242, 186, 190, 60, 33, 73, 113, 100, 181, 240, 244, 78, 19, 131, 6, 204, 57, 193, 99, 43, 73, 50, 120, 103, 224, 236, 54, 208, 40, 160, 18, 213, 141, 202, 77, 55, 78, 211, 171, 248, 3, 184, 223, 5, 137, 145, 170, 178, 243, 96, 112, 64, 165, 103, 231, 36, 254, 156, 136, 109, 137, 247, 188, 201, 33, 107, 16, 205, 220, 163, 43, 17, 135, 175, 160, 227, 6, 68, 75, 62, 211, 32, 216, 125, 208, 241, 184, 12, 239, 118, 141, 60, 223, 107, 83, 37, 224, 215, 10, 42, 161, 104, 168, 247, 20, 47, 217, 143, 144, 211, 82, 77, 151, 89, 117, 5, 73, 182, 77, 213, 147, 112, 210, 34, 161, 203, 20, 118, 110, 125, 43, 189, 232, 129, 82, 59, 135, 150, 141, 108, 205, 28, 254, 182, 193, 174, 172, 237, 231, 253, 111, 36, 237, 182, 44, 134, 201, 108, 5, 37, 152, 17, 207, 69, 221, 118, 46, 88, 250, 149, 211, 3, 0, 231, 50, 123, 146, 224, 6, 204, 174, 35, 172, 32, 109, 50, 21, 164, 228, 89, 37, 185, 191, 66, 112, 168, 3, 194, 173, 50, 198, 116, 31, 95, 35, 42, 142, 1, 108, 72, 88, 246, 90, 160, 165, 103, 146, 171, 62, 19, 75, 140, 169, 189, 214, 249, 89, 105, 109, 46, 121, 79, 143, 98, 20, 166, 35, 101, 59, 60, 142, 211, 168, 218, 103, 44, 225, 167, 168, 226, 119, 144, 251, 191, 194, 64, 59, 152, 127, 50, 220, 38, 91, 149, 253, 196, 52, 66, 55, 208, 221, 220, 231, 25, 16, 230, 96, 225, 64, 64, 93, 156, 100, 152, 116, 178, 128, 97, 122, 249, 131, 107, 41, 44, 225, 247, 28, 198, 167, 206, 120, 123, 106, 31, 9, 213, 221, 57, 221, 90, 73, 88, 125, 24, 228, 238, 150, 66, 63, 60, 30, 230, 207, 124, 243, 196, 8, 31, 56, 236, 192, 50, 136, 123, 205, 112, 172, 41, 172, 153, 236, 99, 135, 182, 24, 69, 139, 37, 232, 78, 234, 14, 192, 192, 3, 21, 48, 175, 50, 222, 226, 185, 227, 51, 85, 165, 109, 79, 88, 124, 234, 51, 118, 208, 242, 244, 78, 198, 76, 227, 187, 37, 163, 15, 202, 133, 240, 72, 5, 132, 136, 206, 126, 174, 90, 101, 39, 88, 196, 64, 159, 99, 104, 161, 212, 153, 70, 226, 245, 35, 220, 194, 190, 58, 79, 122, 144, 142, 157, 235, 22, 1, 41, 147, 96, 82, 253, 72, 85, 72, 222, 189, 222, 39, 188, 52, 231, 10, 225, 38, 68, 169, 85, 57, 227, 8, 179, 215, 241, 22, 5, 40, 223, 4, 52, 158, 192, 154, 50, 220, 91, 191, 183, 218, 238, 28, 146, 91, 250, 49, 117, 182, 77, 63, 148, 195, 91, 113, 141, 108, 242, 4, 225, 69, 99, 194, 238, 147, 110, 29, 247, 112, 162, 234, 46, 87, 6, 41, 83, 0, 57, 29, 45, 145, 236, 225, 85, 203, 167, 31, 164, 239, 112, 95, 232, 39, 167, 142, 44, 183, 238, 179, 85, 84, 60, 119, 148, 80, 37, 213, 131, 130, 138, 89, 34, 129, 124, 210, 98, 201, 109, 62, 249, 125, 221, 253, 86, 211, 231, 46, 183, 38, 255, 103, 156, 87, 106, 238, 126, 96, 74, 14, 127, 226, 216, 48, 156, 46, 195, 141, 100, 226, 176, 134, 124, 253, 44, 56, 118, 143, 220, 49, 84, 75, 0, 230, 63, 151, 214, 137, 85, 164, 76, 116, 68, 12, 137, 20, 138, 145, 65, 206, 1, 3, 159, 88, 3, 197, 100, 249, 168, 98, 146, 19, 32, 10, 186, 24, 201, 6, 175, 144, 107, 1, 117, 220, 191, 24, 92, 227, 69, 142, 146, 123, 207, 71, 217, 190, 29, 49, 217, 10, 92, 131, 231, 157, 66, 157, 150, 231, 57, 183, 130, 184, 20, 140, 112, 67, 28, 2, 115, 187, 165, 39, 188, 70, 136, 139, 106, 110, 166, 122, 187, 120, 100, 127, 189, 54, 252, 219, 0, 153, 195, 49, 50, 177, 46, 152, 102, 5, 226, 191, 228, 230, 142, 219, 169, 34, 141, 158, 1, 0, 2, 139, 110, 190, 19, 228, 87, 86, 236, 93, 48, 194, 156, 38, 58, 203, 249, 193, 101, 192, 63, 103, 116, 170, 55, 41, 173, 94, 169, 234, 112, 105, 44, 93, 81, 235, 40, 149, 57, 167, 215, 68, 90, 216, 10, 181, 212, 187, 38, 233, 116, 191, 223, 72, 14, 238, 20, 244, 92, 53, 61, 7, 55, 155, 177, 99, 104, 192, 186, 92, 54, 107, 19, 133, 26, 73, 160, 5, 39, 99, 195, 248, 20, 242, 165, 155, 214, 127, 160, 78, 63, 116, 130, 16, 122, 79, 125, 38, 153, 140, 170, 136, 80, 5, 245, 188, 73, 87, 106, 19, 53, 101, 143, 165, 26, 122, 23, 100, 197, 222, 22, 10, 213, 229, 207, 2, 169, 235, 198, 20, 199, 61, 50, 168, 201, 86, 126, 112, 175, 152, 36, 186, 199, 85, 174, 53, 137, 104, 230, 110, 11, 25, 180, 95, 99, 252, 88, 12, 150, 19, 223, 63, 195, 240, 99, 39, 26, 156, 230, 230, 42, 98, 234, 131, 125, 207, 29, 53, 167, 2, 47, 212, 222, 51, 31, 85, 167, 203, 145, 191, 39, 92, 253, 203, 114, 10, 151, 251, 120, 182, 32, 187, 100, 230, 160, 130, 31, 61, 5, 197, 34, 159, 239, 163, 36, 63, 27, 63, 250, 116, 103, 98, 65, 169, 150, 52, 244, 129, 51, 109, 183, 247, 20, 218, 130, 81, 5, 126, 196, 202, 91, 79, 93, 13, 18, 65, 85, 147, 168, 97, 126, 148, 201, 22, 238, 125, 216, 151, 75, 237, 37, 72, 148, 23, 90, 201, 56, 173, 52, 36, 229, 45, 36, 62, 99, 225, 101, 211, 125, 237, 177, 68, 159, 171, 89, 210, 168, 96, 239, 248, 54, 131, 199, 191, 27, 250, 60, 47, 43, 166, 126, 128, 92, 76, 122, 151, 56, 159, 62, 86, 28, 221, 39, 130, 47, 83, 185, 53, 129, 195, 60, 31, 55, 59, 114, 24, 136, 117, 116, 38, 148, 1, 116, 148, 212, 91, 165, 133, 70, 14, 136, 79, 92, 232, 24, 33, 185, 226, 173, 93, 204, 144, 192, 250, 248, 0, 2, 243, 106, 107, 104, 126, 168, 199, 28, 216, 232, 71, 246, 158, 15, 233, 162, 179, 81, 93, 150, 147, 48, 207, 239, 112, 103, 197, 29, 32, 187, 161, 211, 232, 196, 6, 165, 88, 141, 19, 82, 195, 245, 132, 123, 252, 38, 183, 46, 213, 210, 107, 112, 149, 87, 37, 27, 102, 103, 234, 157, 28, 190, 109, 118, 78, 184, 144, 169, 123, 39, 158, 145, 140, 251, 50, 87, 246, 233, 74, 114, 238, 139, 214, 71, 200, 51, 177, 217, 80, 93, 191, 20, 186, 34, 44, 123, 218, 243, 229, 198, 87, 98, 89, 199, 180, 145, 28, 239, 148, 66, 84, 7, 161, 188, 127, 167, 60, 74, 176, 77, 145, 192, 172, 70, 30, 93, 203, 254, 204, 15, 208, 20, 33, 186, 92, 200, 39, 111, 78, 0, 121, 121, 241, 14, 58, 11, 205, 211, 163, 58, 96, 10, 181, 53, 124, 156, 133, 179, 189, 68, 187, 58, 88, 24, 116, 19, 114, 26, 23, 204, 197, 69, 55, 175, 242, 127, 235, 77, 76, 4, 85, 255, 252, 175, 29, 75, 103, 225, 69, 16, 181, 90, 17, 87, 204, 79, 118, 189, 27, 69, 39, 132, 240, 209, 86, 167, 67, 251, 190, 84, 202, 50, 228, 30, 115, 183, 177, 174, 165, 207, 44, 23, 241, 65, 109, 45, 219, 214, 13, 130, 0, 83, 229, 19, 157, 95, 7, 179, 46, 139, 66, 146, 145, 185, 17, 188, 204, 13, 203, 122, 28, 57, 1, 50, 224, 21, 127, 169, 152, 82, 243, 31, 85, 222, 126, 84, 59, 85, 255, 32, 111, 210, 90, 168, 196, 131, 37, 172, 45, 23, 67, 154, 74, 185, 213, 166, 70, 159, 207, 92, 5, 49, 65, 28, 218, 82, 74, 231, 35, 23, 254, 122, 27, 75, 203, 11, 217, 242, 162, 177, 30, 140, 181, 230, 88, 255, 216, 84, 26, 171, 106, 106, 60, 186, 53, 55, 136, 155, 141, 244, 112, 255, 144, 14, 27, 239, 65, 5, 232, 101, 7, 91, 218, 10, 18, 56, 162, 244, 227, 107, 212, 35, 19, 213, 47, 78, 195, 127, 211, 209, 161, 164, 43, 192, 136, 67, 63, 37, 88, 100, 64, 184, 231, 239, 171, 180, 7, 225, 217, 12, 85, 65, 58, 42, 181, 37, 174, 160, 61, 123, 207, 91, 221, 81, 68, 80, 138, 127, 117, 182, 127, 166, 93, 143, 84, 8, 166, 1, 166, 232, 250, 52, 22, 211, 166, 208, 194, 163, 109, 189, 209, 193, 142, 232, 12, 121, 17, 70, 85, 39, 169, 212, 188, 203, 96, 194, 87, 69, 48, 63, 95, 254, 18, 33, 149, 9, 242, 232, 235, 112, 22, 125, 216, 86, 78, 154, 97, 49, 76, 200, 42, 213, 10, 169, 191, 252, 215, 232, 106, 152, 206, 4, 43, 109, 13, 176, 151, 151, 112, 233, 14, 158, 5, 91, 14, 194, 52, 173, 131, 196, 147, 95, 82, 138, 234, 53, 173, 186, 237, 125, 237, 152, 106, 65, 133, 32, 160, 120, 44, 246, 171, 154, 244, 146, 19, 214, 118, 43, 163, 50, 150, 154, 214, 106, 176, 140, 68, 49, 212, 244, 206, 21, 81, 54, 10, 109, 181, 222, 61, 235, 113, 59, 22, 171, 243, 149, 81, 119, 46, 185, 228, 131, 151, 48, 217, 179, 133, 161, 41, 242, 231, 203, 217, 238, 110, 55, 106, 123, 47, 185, 8, 214, 213, 138, 53, 124, 1, 247, 125, 62, 196, 243, 124, 255, 5, 197, 29, 129, 179, 202, 240, 227, 118, 229, 254, 214, 193, 160, 139, 1, 20, 62, 142, 252, 241, 35, 94, 174, 77, 122, 249, 242, 248, 100, 107, 79, 188, 181, 91, 1, 181, 129, 54, 24, 36, 44, 21, 109, 244, 55, 41, 177, 112, 33, 101, 13, 119, 130, 240, 37, 82, 51, 93, 135, 157, 85, 31, 163, 184, 116, 232, 67, 21, 129, 20, 98, 218, 170, 130, 204, 39, 116, 109, 78, 132, 130, 81, 206, 93, 244, 228, 108, 22, 252, 99, 125, 57, 230, 145, 229, 114, 188, 171, 114, 159, 8, 11, 254, 33, 216, 85, 37, 23, 118, 217, 125, 127, 185, 47, 104, 52, 43, 150, 225, 192, 142, 190, 101, 252, 62, 83, 45, 79, 30, 13, 227, 11, 30, 53, 220, 202, 208, 248, 100, 45, 145, 90, 129, 68, 236, 129, 237, 112, 48, 229, 179, 219, 195, 153, 23, 144, 242, 122, 17, 99, 171, 24, 36, 227, 66, 32, 121, 84, 207, 207, 69, 250, 65, 56, 1, 190, 200, 240, 41, 204, 239, 108, 158, 204, 79, 194, 134, 77, 34, 213, 61, 241, 53, 204, 174, 118, 119, 99, 246, 186, 128, 7, 150, 66, 42, 38, 90, 247, 165, 97, 179, 54, 96, 158, 122, 247, 33, 184, 115, 200, 231, 234, 114, 35, 114, 50, 168, 66, 166, 196, 158, 161, 122, 234, 74, 211, 228, 203, 24, 230, 84, 38, 182, 27, 29, 205, 56, 5, 242, 246, 87, 130, 253, 110, 185, 234, 36, 177, 69, 181, 86, 160, 201, 119, 147, 73, 110, 69, 214, 240, 63, 80, 221, 77, 234, 196, 165, 54, 131, 12, 197, 2, 207, 147, 198, 117, 76, 128, 208, 188, 171, 144, 148, 44, 53, 239, 169, 182, 96, 169, 106, 190, 177, 1, 68, 229, 61, 82, 162, 139, 199, 136, 230, 145, 214, 46, 84, 152, 133, 5, 47, 84, 17, 7, 149, 4, 187, 115, 122, 245, 84, 3, 99, 245, 155, 4, 188, 24, 155, 163, 103, 109, 169, 114, 248, 145, 213, 204, 236, 192, 161, 155, 145, 110, 2, 115, 141, 210, 242, 245, 15, 46, 246, 208, 64, 202, 60, 46, 13, 79, 85, 143, 106, 100, 165, 77, 16, 251, 187, 9, 69, 161, 85, 140, 86, 111, 222, 138, 75, 38, 170, 0, 20, 177, 98, 73, 220, 122, 103, 31, 123, 250, 13, 182, 91, 39, 230, 140, 0, 31, 140, 170, 20, 237, 108, 217, 215, 60, 209, 31, 242, 162, 147, 63, 184, 100, 224, 129, 111, 120, 171, 137, 57, 83, 152, 132, 183, 233, 138, 233, 98, 104, 8, 25, 178, 118, 188, 9, 65, 211, 216, 55, 174, 48, 231, 44, 153, 218, 39, 89, 123, 179, 232, 200, 53, 19, 64, 49, 76, 172, 206, 83, 30, 55, 51, 45, 4, 215, 224, 182, 172, 202, 201, 246, 245, 38, 223, 58, 207, 195, 134, 204, 63, 9, 51, 53, 177, 154, 233, 183, 106, 79, 0, 12, 227, 246, 221, 183, 33, 146, 183, 205, 102, 234, 134, 192, 54, 44, 64, 61, 191, 48, 3, 142, 87, 225, 154, 154, 140, 76, 196, 17, 112, 162, 10, 180, 23, 157, 90, 179, 75, 13, 100, 32, 54, 126, 206, 70, 151, 214, 142, 175, 166, 47, 9, 100, 252, 186, 71, 4, 144, 225, 198, 0, 143, 200, 102, 6, 50, 125, 16, 140, 103, 254, 136, 165, 203, 82, 234, 181, 73, 244, 213, 92, 104, 5, 164, 227, 244, 215, 18, 108, 94, 184, 215, 144, 74, 5, 100, 76, 190, 61, 79, 244, 117, 79, 56, 193, 222, 126, 66, 228, 79, 6, 50, 21, 116, 166, 220, 85, 126, 114, 70, 253, 91, 223, 21, 196, 236, 109, 72, 46, 27, 222, 62, 40, 49, 139, 202, 224, 18, 147, 87, 41, 65, 156, 237, 130, 245, 114, 188, 72, 86, 250, 91, 91, 205, 176, 131, 173, 181, 9, 180, 31, 218, 188, 106, 37, 103, 136, 247, 157, 138, 123, 254, 1, 48, 33, 23, 30, 189, 83, 183, 21, 181, 43, 229, 80, 51, 121, 124, 133, 195, 177, 17, 57, 47, 230, 143, 201, 127, 191, 174, 149, 10, 106, 27, 187, 10, 176, 64, 197, 206, 132, 86, 136, 22, 195, 51, 208, 51, 19, 107, 253, 142, 99, 170, 186, 177, 226, 141, 210, 99, 187, 105, 135, 148, 253, 67, 246, 51, 152, 202, 204, 249, 148, 121, 30, 162, 196, 160, 99, 102, 88, 53, 61, 5, 131, 49, 144, 144, 183, 197, 10, 216, 99, 214, 171, 164, 233, 101, 89, 59, 189, 84, 173, 51, 7, 120, 75, 247, 42, 210, 250, 221, 252, 244, 150, 209, 98, 165, 238, 161, 246, 43, 233, 174, 76, 63, 141, 49, 161, 183, 49, 226, 60, 93, 56, 248, 86, 122, 195, 45, 224, 9, 52, 46, 58, 118, 9, 75, 90, 150, 30, 117, 156, 240, 230, 86, 154, 8, 170, 115, 234, 162, 193, 98, 88, 229, 209, 105, 207, 124, 15, 241, 13, 68, 210, 207, 103, 110, 189, 103, 168, 252, 159, 86, 189, 146, 41, 119, 209, 161, 163, 180, 119, 86, 87, 55, 32, 128, 138, 137, 181, 142, 147, 53, 116, 126, 87, 154, 156, 120, 68, 113, 25, 183, 76, 143, 154, 112, 142, 47, 180, 176, 111, 251, 196, 63, 241, 55, 134, 135, 76, 100, 68, 255, 200, 125, 103, 202, 200, 237, 142, 198, 72, 230, 184, 221, 34, 101, 244, 108, 201, 67, 169, 207, 244, 23, 41, 173, 171, 66, 240, 196, 103, 164, 198, 61, 237, 255, 217, 22, 225, 87, 117, 86, 68, 214, 92, 48, 166, 125, 21, 138, 166, 240, 73, 206, 42, 97, 136, 90, 172, 44, 102, 138, 238, 167, 15, 24, 38, 129, 23, 44, 225, 58, 121, 107, 137, 206, 26, 6, 228, 215, 111, 207, 21, 109, 93, 163, 145, 68, 183, 119, 106, 247, 212, 131, 189, 200, 32, 118, 43, 184, 224, 237, 253, 29, 150, 118, 86, 172, 240, 64, 215, 6, 255, 216, 209, 251, 253, 85, 118, 30, 246, 173, 217, 143, 28, 29, 126, 11, 4, 71, 124, 67, 212, 108, 113, 142, 170, 149, 242, 93, 55, 235, 192, 146, 65, 190, 114, 156, 166, 161, 121, 60, 199, 129, 182, 114, 197, 205, 164, 66, 30, 57, 130, 178, 89, 246, 22, 29, 216, 179, 237, 192, 135, 124, 178, 12, 3, 209, 173, 66, 173, 92, 255, 79, 154, 127, 114, 170, 155, 60, 149, 57, 203, 61, 131, 149, 189, 220, 190, 158, 164, 179, 200, 231, 111, 152, 114, 182, 142, 51, 199, 233, 112, 77, 11, 13, 19, 149, 51, 212, 120, 205, 238, 47, 46, 162, 75, 80, 227, 88, 201, 110, 221, 152, 239, 147, 68, 128, 43, 44, 25, 209, 203, 242, 146, 202, 156, 75, 213, 136, 246, 50, 189, 149, 77, 164, 225, 139, 230, 171, 186, 213, 110, 139, 173, 41, 82, 198, 165, 150, 26, 52, 212, 0, 249, 222, 218, 40, 239, 103, 2, 3, 113, 172, 101, 69, 14, 203, 142, 7, 224, 10, 58, 154, 171, 73, 153, 134, 140, 233, 235, 93, 8, 191, 238, 168, 171, 35, 218, 222, 50, 38, 217, 38, 155, 182, 7, 72, 223, 117, 9, 10, 74, 206, 119, 188, 190, 66, 193, 106, 89, 55, 78, 104, 242, 59, 84, 169, 79, 236, 241, 80, 215, 143, 119, 186, 162, 238, 203, 117, 174, 46, 46, 120, 72, 131, 71, 127, 94, 197, 81, 162, 66, 230, 206, 81, 31, 29, 202, 138, 141, 135, 225, 0, 106, 250, 122, 228, 27, 31, 126, 164, 114, 40, 71, 72, 80, 25, 111, 66, 164, 66, 28, 59, 57, 60, 185, 122, 120, 242, 200, 143, 177, 216, 86, 39, 43, 168, 91, 68, 223, 97, 245, 124, 222, 197, 187, 173, 195, 1, 68, 243, 110, 58, 6, 188, 177, 201, 93, 147, 243, 19, 45, 134, 224, 214, 71, 207, 136, 173, 76, 141, 60, 255, 32, 58, 134, 177, 4, 50, 159, 17, 130, 195, 126, 118, 202, 211, 113, 146, 154, 62, 226, 128, 192, 155, 55, 213, 33, 104, 132, 80, 118, 146, 214, 187, 47, 211, 244, 159, 1, 12, 216, 9, 56, 226, 129, 17, 236, 11, 111, 33, 20, 47, 192, 89, 53, 238, 113, 123, 155, 86, 223, 151, 66, 191, 151, 52, 184, 132, 233, 137, 122, 29, 52, 2, 173, 46, 64, 40, 191, 236, 182, 114, 16, 238, 104, 208, 56, 169, 51, 106, 181, 191, 211, 246, 172, 186, 116, 216, 19, 195, 76, 107, 41, 131, 213, 94, 171, 134, 23, 6, 86, 161, 92, 191, 56, 134, 244, 171, 31, 123, 188, 100, 78, 54, 197, 168, 7, 99, 59, 137, 124, 186, 154, 173, 10, 60, 22, 2, 62, 156, 226, 245, 135, 165, 89, 83, 207, 25, 44, 193, 198, 152, 156, 32, 255, 191, 117, 48, 51, 116, 206, 157, 254, 15, 36, 181, 163, 66, 144, 57, 76, 105, 216, 192, 140, 22, 106, 3, 109, 72, 3, 89, 36, 6, 21, 241, 104, 166, 196, 23, 61, 204, 160, 194, 249, 184, 93, 241, 32, 96, 222, 32, 243, 219, 19, 193, 118, 28, 51, 127, 72, 76, 242, 199, 124, 193, 38, 111, 40, 197, 225, 27, 113, 145, 74, 200, 46, 67, 158, 215, 124, 109, 53, 201, 199, 43, 49, 24, 115, 86, 33, 225, 163, 117, 54, 55, 233, 11, 16, 184, 162, 48, 45, 101, 208, 142, 227, 40, 78, 89, 7, 228, 233, 11, 50, 253, 34, 77, 144, 169, 165, 9, 62, 70, 53, 243, 140, 130, 136, 40, 35, 236, 218, 146, 92, 105, 130, 110, 176, 123, 228, 177, 5, 26, 160, 85, 149, 205, 77, 139, 231, 238, 207, 25, 150, 55, 208, 29, 147, 104, 227, 160, 250, 149, 189, 2, 198, 151, 33, 24, 227, 204, 102, 60, 158, 8, 161, 154, 56, 105, 54, 180, 160, 26, 122, 118, 82, 151, 166, 165, 113, 100, 4, 5, 59, 95, 170, 75, 241, 24, 249, 176, 182, 253, 194, 214, 249, 119, 141, 14, 85, 136, 224, 80, 197, 10, 124, 226, 105, 51, 218, 38, 114, 117, 129, 117, 47, 6, 208, 197, 80, 151, 88, 46, 95, 117, 166, 8, 103, 220, 15, 191, 221, 98, 44, 175, 80, 92, 0, 164, 159, 182, 104, 109, 193, 117, 231, 202, 91, 223, 4, 55, 150, 21, 116, 24, 87, 228, 50, 218, 212, 50, 98, 109, 89, 64, 208, 111, 52, 75, 115, 5, 250, 39, 108, 37, 130, 92, 56, 30, 177, 32, 148, 210, 15, 188, 149, 3, 135, 182, 203, 25, 82, 178, 102, 45, 211, 254, 243, 92, 214, 143, 170, 140, 47, 153, 138, 237, 20, 88, 57, 246, 117, 50, 185, 213, 103, 142, 49, 125, 206, 104, 199, 160, 223, 104, 145, 138, 177, 161, 137, 82, 239, 65, 68, 133, 64, 171, 138, 131, 254, 104, 0, 193, 216, 77, 7, 46, 0, 97, 175, 106, 196, 3, 244, 89, 22, 28, 252, 152, 197, 213, 240, 97, 76, 25, 246, 241, 121, 93, 166, 184, 214, 31, 89, 54, 33, 238, 218, 146, 207, 64, 248, 29, 225, 24, 236, 21, 159, 59, 6, 109, 217, 12, 130, 142, 223, 186, 220, 173, 35, 121, 36, 172, 161, 35, 100, 181, 104, 219, 13, 34, 80, 72, 57, 18, 172, 193, 59, 212, 51, 224, 206, 11, 170, 94, 105, 51, 113, 139, 171, 32, 27, 119, 188, 23, 253, 216, 88, 192, 153, 45, 139, 141, 211, 74, 33, 57, 64, 117, 205, 12, 255, 39, 184, 92, 135, 134, 144, 61, 238, 57, 94, 131, 4, 236, 60, 122, 235, 132, 134, 138, 32, 208, 32, 172, 194, 223, 140, 224, 39, 90, 149, 92, 83, 116, 236, 180, 16, 180, 108, 215, 217, 62, 168, 200, 227, 25, 128, 11, 40, 210, 47, 87, 93, 221, 152, 37, 107, 21, 88, 105, 45, 112, 94, 173, 206, 212, 203, 59, 88, 88, 67, 63, 249, 252, 117, 238, 247, 113, 159, 209, 118, 88, 150, 187, 202, 46, 189, 213, 130, 234, 147, 105, 253, 117, 148, 235, 135, 118, 94, 188, 146, 38, 50, 151, 249, 201, 48, 55, 126, 97, 5, 49, 255, 29, 89, 245, 222, 254, 75, 88, 245, 84, 240, 63, 98, 214, 89, 81, 82, 20, 3, 24, 50, 125, 137, 247, 38, 81, 232, 245, 198, 232, 108, 139, 215, 183, 199, 91, 203, 58, 231, 242, 79, 235, 220, 105, 70, 107, 142, 69, 152, 239, 193, 196, 117, 131, 184, 172, 178, 242, 110, 246, 14, 127, 141, 248, 223, 194, 225, 139, 178, 84, 82, 178, 128, 185, 245, 153, 80, 185, 216, 15, 80, 190, 145, 188, 146, 143, 246, 176, 2, 192, 56, 44, 242, 142, 14, 54, 6, 128, 91, 68, 114, 118, 7, 152, 228, 181, 145, 132, 151, 176, 61, 116, 91, 97, 9, 186, 149, 83, 124, 128, 6, 210, 133, 226, 9, 77, 78, 208, 43, 231, 96, 129, 172, 1, 220, 201, 53, 9, 239, 179, 22, 239, 139, 88, 83, 66, 64, 58, 49, 200, 213, 143, 149, 4, 189, 6, 144, 117, 181, 255, 58, 55, 190, 117, 122, 52, 77, 48, 255, 85, 66, 150, 238, 222, 119, 230, 145, 186, 121, 123, 99, 135, 26, 141, 185, 136, 178, 47, 50, 85, 250, 146, 212, 81, 63, 165, 34, 236, 219, 119, 204, 142, 219, 185, 112, 102, 11, 5, 169, 191, 250, 61, 105, 29, 180, 129, 204, 24, 115, 224, 30, 30, 70, 36, 20, 186, 15, 42, 219, 176, 89, 62, 37, 207, 242, 19, 201, 45, 37, 26, 49, 16, 255, 102, 209, 48, 231, 55, 4, 31, 33, 186, 181, 106, 71, 191, 142, 89, 182, 222, 171, 80, 182, 27, 133, 18, 107, 63, 93, 105, 102, 160, 113, 74, 165, 3, 97, 78, 213, 86, 61, 255, 159, 63, 54, 90, 114, 172, 63, 196, 236, 102, 67, 236, 254, 171, 177, 88, 123, 159, 70, 42, 104, 0, 239, 86, 64, 197, 182, 234, 87, 128, 86, 77, 120, 221, 207, 36, 24, 54, 218, 179, 13, 152, 158, 128, 18, 161, 127, 128, 198, 95, 250, 9, 185, 73, 218, 213, 237, 196, 229, 55, 72, 167, 49, 61, 209, 17, 9, 209, 168, 207, 51, 232, 82, 198, 186, 104, 209, 16, 11, 148, 163, 91, 212, 196, 15, 104, 88, 238, 187, 100, 184, 110, 214, 149, 97, 52, 174, 195, 200, 174, 213, 6, 18, 136, 170, 155, 9, 234, 228, 138, 204, 74, 25, 181, 60, 219, 240, 16, 147, 139, 250, 87, 248, 164, 16, 15, 70, 107, 125, 151, 225, 118, 159, 159, 189, 149, 223, 181, 47, 230, 39, 127, 59, 56, 95, 254, 77, 131, 216, 38, 69, 141, 39, 253, 61, 21, 98, 76, 241, 183, 34, 48, 237, 33, 207, 8, 193, 153, 220, 152, 163, 5, 116, 117, 91, 159, 91, 217, 169, 226, 156, 118, 168, 152, 95, 247, 197, 5, 164, 15, 177, 221, 164, 193, 43, 18, 199, 218, 141, 103, 177, 241, 56, 203, 241, 136, 183, 31, 5, 20, 235, 68, 78, 65, 239, 69, 233, 240, 148, 167, 98, 193, 71, 149, 27, 188, 200, 242, 95, 71, 186, 132, 107, 162, 101, 35, 240, 173, 50, 91, 104, 207, 144, 243, 123, 168, 13, 9, 221, 178, 50, 123, 3, 238, 171, 222, 54, 96, 148, 38, 102, 28, 154, 79, 200, 194, 19, 62, 4, 106, 26, 52, 233, 150, 197, 68, 87, 134, 172, 183, 30, 3, 127, 195, 152, 150, 20, 63, 32, 70, 161, 155, 123, 151, 55, 155, 184, 130, 6, 67, 235, 59, 121, 93, 192, 191, 194, 166, 199, 245, 108, 225, 237, 55, 187, 238, 123, 115, 110, 141, 41, 240, 34, 28, 157, 225, 169, 59, 71, 199, 217, 128, 203, 206, 59, 220, 244, 73, 76, 250, 33, 107, 240, 120, 254, 42, 114, 213, 125, 1, 62, 160, 78, 211, 14, 235, 178, 180, 197, 5, 208, 157, 210, 27, 35, 40, 207, 20, 178, 5, 137, 233, 93, 116, 148, 25, 201, 84, 171, 109, 113, 168, 77, 69, 200, 86, 104, 205, 113, 36, 84, 249, 228, 140, 92, 58, 161, 121, 150, 14, 132, 36, 220, 57, 145, 88, 211, 172, 161, 50, 241, 16, 88, 16, 179, 70, 208, 27, 25, 254, 248, 156, 96, 24, 67, 248, 19, 66, 182, 88, 172, 170, 110, 102, 176, 248, 177, 96, 165, 64, 35, 90, 43, 101, 1, 62, 30, 54, 191, 82, 155, 22, 89, 62, 111, 216, 115, 180, 189, 194, 235, 17, 211, 244, 45, 180, 120, 136, 21, 230, 7, 93, 252, 2, 212, 200, 250, 240, 155, 156, 33, 186, 236, 35, 29, 43, 31, 204, 109, 52, 80, 175, 179, 96, 229, 247, 134, 25, 2, 91, 219, 26, 88, 184, 171, 173, 103, 213, 223, 163, 240, 120, 195, 143, 241, 219, 184, 13, 133, 101, 53, 154, 48, 199, 38, 209, 73, 121, 91, 95, 72, 169, 114, 19, 213, 250, 231, 228, 202, 0, 208, 26, 21, 153, 4, 112, 5, 99, 66, 246, 62, 248, 105, 29, 184, 195, 179, 20, 107, 200, 45, 170, 19, 207, 61, 210, 232, 23, 218, 7, 111, 127, 144, 129, 215, 205, 188, 38, 244, 176, 119, 98, 192, 94, 57, 164, 120, 89, 58, 122, 142, 0, 35, 163, 18, 84, 68, 194, 237, 244, 57, 225, 207, 61, 126, 76, 34, 232, 228, 121, 25, 13, 229, 25, 41, 173, 246, 53, 182, 137, 179, 199, 139, 5, 144, 52, 185, 34, 207, 196, 88, 248, 61, 28, 187, 173, 239, 214, 9, 241, 138, 133, 124, 228, 50, 195, 142, 128, 123, 253, 13, 124, 253, 174, 144, 81, 220, 90, 228, 118, 169, 109, 235, 215, 255, 96, 179, 247, 221, 133, 112, 250, 247, 254, 207, 254, 77, 187, 206, 180, 81, 197, 148, 104, 116, 106, 110, 229, 175, 157, 242, 194, 84, 52, 161, 215, 101, 202, 173, 182, 235, 130, 221, 142, 15, 76, 140, 60, 198, 121, 236, 165, 73, 252, 195, 162, 81, 182, 143, 7, 42, 9, 6, 148, 89, 230, 249, 251, 58, 126, 99, 30, 59, 3, 157, 51, 215, 215, 24, 95, 124, 184, 99, 2, 170, 0, 96, 59, 95, 226, 42, 231, 125, 77, 38, 78, 118, 145, 169, 134, 171, 140, 15, 162, 56, 254, 58, 82, 217, 90, 231, 245, 244, 34, 189, 118, 222, 193, 146, 101, 234, 65, 62, 148, 52, 179, 98, 126, 149, 182, 36, 69, 139, 170, 44, 144, 122, 74, 184, 207, 197, 33, 159, 61, 184, 180, 238, 203, 33, 47, 124, 37, 51, 126, 80, 169, 75, 116, 64, 170, 204, 66, 137, 35, 56, 10, 45, 177, 220, 2, 187, 152, 151, 231, 188, 163, 186, 100, 172, 129, 18, 120, 136, 78, 40, 221, 97, 74, 6, 90, 252, 255, 122, 68, 71, 7, 179, 210, 43, 94, 19, 87, 109, 36, 130, 139, 165, 161, 135, 70, 107, 34, 22, 220, 150, 189, 99, 180, 84, 193, 186, 49, 152, 59, 169, 22, 74, 233, 173, 190, 182, 126, 77, 161, 104, 252, 245, 88, 136, 71, 143, 37, 206, 146, 203, 93, 129, 234, 50, 44, 168, 69, 178, 38, 98, 92, 128, 133, 20, 138, 132, 206, 220, 114, 94, 255, 225, 138, 221, 104, 86, 203, 194, 218, 89, 108, 112, 82, 48, 186, 135, 20, 192, 60, 4, 40, 219, 230, 200, 250, 203, 1, 200, 10, 39, 56, 63, 159, 193, 233, 241, 42, 148, 19, 134, 138, 108, 241, 90, 252, 131, 55, 75, 164, 129, 250, 27, 188, 47, 27, 71, 12, 118, 232, 109, 7, 154, 134, 235, 104, 143, 220, 234, 58, 155, 135, 92, 240, 90, 72, 48, 6, 72, 154, 141, 74, 22, 0, 203, 61, 241, 244, 59, 244, 170, 124, 167, 89, 247, 165, 232, 203, 177, 22, 161, 64, 102, 100, 79, 158, 214, 214, 218, 20, 58, 90, 4, 179, 7, 145, 87, 118, 32, 5, 140, 210, 189, 185, 100, 199, 41, 163, 159, 24, 54, 119, 61, 239, 130, 109, 27, 164, 63, 210, 184, 100, 213, 110, 32, 149, 110, 64, 143, 113, 219, 221, 133, 43, 60, 156, 161, 181, 205, 165, 243, 174, 187, 15, 196, 243, 238, 185, 252, 228, 166, 214, 224, 37, 187, 162, 114, 117, 121, 213, 155, 176, 8, 73, 87, 217, 106, 16, 135, 152, 125, 204, 171, 246, 151, 68, 166, 237, 218, 123, 123, 80, 98, 69, 100, 202, 86, 4, 117, 88, 119, 136, 120, 36, 112, 137, 236, 97, 166, 77, 174, 234, 106, 23, 94, 59, 28, 56, 220, 207, 225, 136, 208, 229, 55, 59, 172, 228, 69, 62, 122, 171, 255, 70, 159, 26, 173, 143, 214, 63, 231, 146, 222, 144, 8, 126, 119, 117, 145, 243, 165, 96, 253, 86, 131, 121, 232, 188, 232, 228, 36, 193, 12, 206, 241, 130, 20, 10, 135, 108, 195, 209, 145, 197, 152, 21, 143, 222, 185, 93, 15, 0, 232, 40, 216, 118, 231, 144, 85, 117, 35, 251, 171, 210, 178, 24, 80, 31, 95, 226, 75, 2, 43, 142, 76, 150, 66, 175, 65, 130, 33, 144, 243, 225, 131, 84, 65, 206, 114, 240, 161, 74, 172, 206, 249, 56, 19, 38, 85, 36, 75, 131, 208, 85, 185, 125, 63, 132, 8, 230, 210, 133, 242, 216, 233, 95, 74, 171, 102, 211, 26, 34, 140, 19, 108, 205, 119, 172, 79, 245, 167, 168, 99, 99, 114, 113, 81, 40, 98, 119, 91, 149, 245, 37, 41, 147, 66, 122, 113, 207, 141, 90, 57, 202, 33, 101, 101, 128, 91, 243, 30, 202, 191, 101, 98, 46, 151, 98, 222, 219, 148, 113, 98, 125, 130, 148, 44, 105, 245, 89, 212, 0, 198, 16, 120, 35, 62, 149, 58, 164, 130, 56, 36, 179, 170, 235, 147, 176, 250, 200, 154, 170, 110, 182, 72, 42, 56, 207, 187, 125, 106, 71, 98, 52, 249, 206, 36, 23, 170, 131, 232, 176, 44, 18, 25, 189, 181, 60, 24, 153, 172, 252, 99, 202, 103, 149, 17, 72, 73, 151, 109, 217, 237, 135, 146, 161, 227, 46, 30, 151, 206, 206, 63, 177, 40, 81, 220, 178, 83, 84, 101, 222, 101, 115, 109, 91, 191, 69, 173, 134, 244, 213, 177, 38, 63, 229, 160, 45, 175, 74, 15, 215, 211, 248, 110, 49, 120, 170, 11, 44, 6, 100, 150, 81, 35, 156, 57, 115, 127, 179, 21, 124, 150, 46, 80, 225, 156, 185, 155, 68, 51, 172, 7, 249, 67, 39, 206, 175, 20, 168, 219, 53, 245, 151, 160, 232, 62, 189, 210, 54, 33, 243, 121, 131, 129, 147, 171, 160, 166, 91, 161, 180, 26, 133, 226, 26, 255, 27, 15, 165, 240, 223, 65, 232, 26, 245, 64, 175, 189, 134, 199, 76, 153, 25, 147, 197, 165, 209, 186, 43, 22, 249, 49, 206, 86, 67, 109, 95, 42, 106, 175, 230, 121, 145, 172, 241, 144, 94, 213, 178, 170, 52, 157, 97, 152, 0, 153, 205, 76, 148, 170, 51, 8, 55, 175, 101, 81, 164, 211, 220, 9, 227, 249, 180, 138, 216, 163, 201, 179, 46, 102, 24, 195, 131, 218, 201, 28, 31, 205, 241, 233, 173, 7, 56, 57, 42, 49, 153, 160, 223, 111, 95, 153, 149, 176, 68, 78, 218, 48, 252, 197, 114, 140, 150, 33, 27, 159, 162, 209, 132, 185, 115, 243, 48, 127, 175, 218, 118, 140, 63, 127, 165, 32, 67, 193, 32, 85, 44, 186, 108, 172, 154, 28, 246, 203, 225, 11, 102, 236, 163, 4, 58, 202, 65, 1, 241, 204, 176, 205, 244, 161, 42, 11, 29, 123, 211, 218, 214, 77, 239, 149, 92, 34, 82, 176, 234, 185, 253, 1, 140, 224, 216, 39, 115, 194, 165, 19, 110, 207, 115, 22, 242, 195, 84, 48, 77, 214, 29, 58, 135, 94, 215, 186, 195, 45, 207, 58, 181, 111, 204, 158, 113, 98, 64, 65, 252, 38, 191, 88, 105, 50, 27, 117, 188, 160, 80, 226, 190, 250, 87, 58, 195, 165, 79, 32, 199, 186, 173, 131, 34, 250, 245, 66, 10, 216, 58, 212, 101, 25, 172, 72, 116, 7, 147, 241, 33, 221, 239, 122, 77, 208, 4, 86, 59, 103, 96, 18, 73, 5, 142, 182, 136, 153, 255, 75, 150, 230, 50, 230, 186, 222, 108, 106, 103, 228, 171, 224, 252, 88, 192, 236, 43, 14, 52, 46, 71, 63, 234, 211, 81, 246, 86, 243, 230, 51, 54, 98, 151, 224, 40, 94, 248, 198, 186, 2, 218, 125, 134, 35, 180, 17, 115, 37, 138, 166, 10, 85, 11, 200, 99, 186, 168, 105, 40, 129, 160, 203, 243, 202, 42, 88, 110, 13, 4, 195, 120, 84, 49, 111, 209, 7, 56, 222, 89, 4, 67, 31, 122, 238, 167, 164, 213, 150, 144, 35, 125, 88, 77, 66, 204, 11, 99, 150, 87, 149, 105, 248, 27, 90, 243, 186, 221, 196, 215, 176, 214, 31, 64, 39, 25, 139, 246, 6, 162, 7, 76, 207, 178, 173, 91, 248, 89, 235, 113, 214, 135, 74, 114, 68, 158, 136, 127, 162, 14, 44, 242, 123, 251, 137, 59, 196, 113, 90, 151, 150, 144, 3, 244, 13, 50, 50, 204, 206, 246, 94, 249, 38, 215, 5, 42, 131, 111, 6, 196, 194, 99, 72, 106, 96, 84, 202, 98, 189, 1, 203, 202, 97, 202, 177, 212, 165, 231, 41, 131, 214, 200, 184, 11, 66, 81, 86, 205, 203, 115, 242, 84, 140, 80, 191, 109, 86, 41, 26, 32, 38, 106, 242, 5, 196, 148, 192, 184, 12, 241, 235, 17, 61, 197, 247, 79, 226, 245, 166, 204, 7, 228, 223, 37, 75, 184, 169, 59, 181, 54, 25, 136, 147, 114, 196, 17, 225, 73, 50, 119, 253, 221, 222, 111, 224, 86, 9, 139, 107, 35, 236, 245, 31, 31, 15, 97, 194, 84, 182, 152, 143, 11, 219, 0, 117, 214, 36, 70, 158, 223, 186, 236, 119, 170, 253, 140, 129, 208, 218, 73, 35, 189, 151, 228, 118, 206, 32, 129, 187, 39, 86, 55, 16, 233, 133, 5, 34, 29, 78, 1, 199, 89, 255, 53, 142, 9, 60, 151, 170, 150, 105, 16, 56, 76, 183, 210, 119, 191, 247, 166, 50, 241, 105, 203, 101, 17, 72, 240, 107, 71, 195, 115, 141, 53, 109, 7, 92, 249, 190, 119, 249, 131, 28, 28, 52, 72, 63, 234, 229, 59, 230, 166, 8, 44, 131, 17, 191, 218, 2, 222, 174, 40, 206, 21, 237, 196, 234, 188, 33, 72, 177, 160, 138, 178, 18, 251, 246, 39, 244, 209, 6, 43, 169, 70, 136, 46, 16, 115, 154, 93, 67, 3, 47, 101, 227, 36, 76, 200, 83, 78, 108, 129, 230, 60, 117, 9, 162, 33, 97, 178, 253, 60, 58, 57, 95, 56, 6, 94, 3, 117, 162, 181, 241, 168, 215, 149, 137, 21, 28, 71, 241, 123, 242, 167, 129, 75, 90, 55, 150, 175, 179, 188, 10, 49, 37, 44, 122, 119, 243, 172, 136, 199, 206, 55, 167, 42, 82, 147, 178, 198, 96, 0, 80, 153, 130, 129, 102, 81, 91, 3, 199, 134, 249, 200, 15, 93, 51, 130, 141, 51, 75, 58, 93, 122, 22, 123, 239, 134, 186, 144, 194, 255, 208, 244, 248, 151, 151, 85, 72, 151, 171, 171, 234, 130, 93, 110, 254, 51, 72, 26, 41, 48, 223, 253, 19, 101, 61, 157, 125, 98, 1, 15, 67, 236, 54, 113, 28, 32, 60, 161, 115, 71, 128, 148, 98, 251, 112, 183, 23, 119, 117, 135, 92, 150, 201, 98, 201, 83, 156, 255, 120, 77, 27, 192, 102, 116, 186, 46, 68, 145, 9, 120, 17, 242, 154, 18, 161, 108, 252, 81, 251, 217, 202, 191, 218, 166, 108, 15, 22, 0, 41, 137, 134, 190, 193, 53, 60, 233, 31, 52, 186, 11, 38, 148, 249, 185, 54, 151, 181, 8, 32, 161, 52, 124, 29, 206, 103, 205, 139, 190, 14, 7, 192, 251, 248, 6, 14, 82, 129, 194, 205, 164, 94, 7, 13, 188, 65, 7, 254, 145, 15, 65, 4, 58, 120, 222, 13, 34, 121, 184, 10, 21, 245, 106, 91, 249, 38, 171, 198, 129, 83, 197, 188, 22, 255, 75, 250, 79, 27, 166, 236, 221, 209, 0, 231, 139, 59, 108, 255, 197, 178, 108, 133, 223, 84, 100, 182, 143, 213, 222, 19, 181, 193, 157, 245, 195, 194, 212, 38, 59, 50, 85, 106, 192, 115, 77, 215, 15, 253, 58, 195, 70, 213, 76, 33, 126, 54, 180, 79, 143, 88, 8, 156, 82, 233, 243, 3, 72, 155, 43, 110, 116, 238, 181, 129, 106, 151, 58, 32, 190, 27, 127, 96, 229, 10, 35, 30, 233, 227, 123, 34, 228, 112, 88, 119, 31, 105, 67, 41, 55, 184, 176, 242, 214, 68, 242, 125, 71, 226, 141, 62, 46, 2, 181, 63, 147, 10, 82, 48, 144, 111, 18, 92, 0, 49, 78, 44, 217, 31, 60, 169, 180, 187, 209, 36, 198, 176, 96, 64, 212, 151, 95, 86, 152, 23, 219, 12, 239, 80, 209, 211, 234, 84, 119, 104, 149, 180, 41, 128, 166, 224, 194, 82, 83, 110, 163, 193, 14, 232, 138, 177, 232, 25, 250, 98, 38, 73, 54, 162, 89, 104, 95, 127, 192, 52, 136, 79, 241, 96, 38, 115, 226, 18, 228, 18, 12, 80, 90, 201, 73, 246, 53, 21, 91, 185, 230, 100, 219, 170, 241, 218, 218, 148, 85, 10, 29, 233, 209, 132, 234, 46, 43, 246, 113, 162, 124, 54, 125, 92, 116, 199, 103, 179, 224, 187, 176, 54, 72, 154, 67, 173, 179, 9, 147, 178, 166, 94, 49, 213, 75, 233, 213, 125, 36, 109, 190, 225, 23, 150, 170, 217, 53, 108, 51, 226, 83, 198, 213, 92, 16, 199, 164, 150, 181, 196, 201, 207, 104, 218, 203, 43, 35, 66, 212, 22, 141, 170, 66, 255, 227, 196, 208, 123, 253, 173, 52, 90, 230, 55, 4, 34, 37, 202, 247, 10, 125, 210, 39, 31, 74, 162, 79, 245, 87, 181, 139, 42, 17, 140, 188, 15, 177, 214, 147, 67, 168, 219, 214, 166, 44, 243, 85, 184, 173, 96, 67, 251, 78, 167, 31, 150, 184, 154, 170, 145, 163, 245, 14, 20, 107, 184, 180, 117, 242, 229, 130, 18, 64, 151, 104, 189, 228, 121, 203, 132, 73, 129, 116, 14, 29, 64, 141, 145, 252, 148, 169, 43, 23, 85, 20, 207, 46, 12, 50, 116, 184, 169, 185, 222, 121, 115, 154, 48, 219, 5, 129, 141, 19, 66, 120, 56, 146, 233, 143, 128, 227, 136, 1, 242, 228, 95, 118, 0, 136, 84, 16, 149, 245, 48, 35, 230, 4, 192, 27, 188, 58, 38, 222, 87, 163, 228, 246, 109, 13, 194, 126, 232, 230, 30, 77, 121, 211, 54, 45, 42, 221, 12, 86, 160, 175, 179, 23, 101, 61, 26, 200, 52, 90, 184, 177, 95, 67, 222, 127, 164, 244, 167, 29, 108, 157, 73, 133, 91, 21, 123, 20, 210, 77, 138, 116, 112, 245, 156, 222, 193, 54, 76, 198, 32, 142, 46, 35, 71, 225, 230, 223, 109, 191, 101, 94, 167, 210, 159, 129, 14, 116, 74, 202, 1, 24, 17, 189, 37, 208, 128, 8, 51, 197, 146, 99, 78, 146, 15, 160, 78, 184, 190, 129, 54, 66, 16, 14, 46, 181, 70, 121, 196, 5, 156, 138, 231, 76, 195, 220, 26, 224, 244, 192, 176, 185, 9, 179, 29, 192, 209, 19, 71, 6, 38, 224, 129, 155, 66, 110, 242, 89, 148, 208, 4, 142, 107, 144, 76, 14, 103, 25, 172, 169, 49, 205, 48, 164, 175, 83, 43, 126, 185, 217, 67, 161, 215, 250, 245, 159, 198, 216, 110, 81, 201, 74, 86, 13, 178, 199, 40, 144, 10, 46, 150, 190, 49, 204, 236, 69, 177, 221, 244, 100, 34, 141, 255, 132, 241, 159, 172, 64, 111, 161, 131, 190, 226, 53, 76, 154, 113, 178, 158, 14, 11, 163, 119, 175, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eRvwZ5Nf7g6"
      },
      "source": [
        "k = 8       # Number of information bits per message, i.e., M=2**k\n",
        "n = 4       # Number of real channel uses per message\n",
        "seed = 2    # Seed RNG reproduce identical results"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28KCbvYif7hD"
      },
      "source": [
        "#### The Autoencoder Class\n",
        "In order to quickly experiment with different architecture and parameter choices, it is useful to create a Python class that has functions for training and inference. Each autoencoder instance has its own Tensorflow session and graph. Thus, you can have multiple instances running at the same time without interference between them."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = 5.9*10**9;#in Hz corresponding to IEEE 802.11p\n",
        "lamda = 0.05;  #in metres\n",
        "Pt = 1; #BS transmitted power in watts\n",
        "Lo = 8;   #Total system losses in dB\n",
        "Nf = 5;    #Mobile receiver noise figure in dB\n",
        "T = 290;   #temperature in degree kelvin\n",
        "BW = 10*10**6; #in Hz\n",
        "Gb = 8;  #in dB\n",
        "Gm = 0;   #in dB\n",
        "Hb = 1;  #in metres\n",
        "Hm = 1.1;   #in metres\n",
        "B = 1.38*10**-23; #Boltzmann's constant\n",
        "Te = T*(3.162-1)\n",
        "Pn = B*(Te+T)*BW\n",
        "Free_Lp = {}\n",
        "Pr = {}\n",
        "SNR_var = {}\n",
        "ad_noise_std = []  #The values of the dictionary, finally providing an array of stdevs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "s = np.floor(np.random.uniform(0,256, 1000))\n",
        "s = s.astype('int64')            \n",
        "for i in range(1000):\n",
        "  Free_Lp[i] = 20*math.log10(Hm*Hb/s[i]**2)\n",
        "  Pr[i] = Free_Lp[i]-Lo+Gm+Gb+Pt  #in dBW\n",
        "  SNR_var[i] = Pr[i]-10*math.log10(Pn) #Provides SNR (stddev) values for all distances spanning s\n",
        "\n",
        "for i in SNR_var.values():\n",
        "  ad_noise_std.append(i)\n",
        "\n",
        "ad_noise_std = np.transpose(np.tile(ad_noise_std, (2,2,1)))\n",
        "#s = s.tolist()\n",
        "#ad_noise_std = tuple(map(tuple, ad_noise_std))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for j in range(1,M+1):\n",
        "\n",
        "  Free_Lp[j] = 20*math.log10(Hm*Hb/j**2)\n",
        "  Pr[j] = Free_Lp[j]-Lo+Gm+Gb+Pt  #in dBW\n",
        "  SNR_var[j] = Pr[j]-10*math.log10(Pn)\n",
        "\n",
        "const_noise_std = []  #The values of the dictionary, finally providing an array of stdevs\n",
        "for j in SNR_var.values():\n",
        "  const_noise_std.append(j)  \n",
        "const_noise_std = np.transpose(np.tile(const_noise_std, (2,2,1)))\n",
        "\n",
        "const_noise_std = tuple(map(tuple, const_noise_std))\n",
        "\n",
        "\n",
        "print(np.shape(ad_noise_std))\n",
        "\n",
        " #Computations for the constellations!           \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0hIseee8ira",
        "outputId": "19565c06-872c-454e-d365-cdd65c6dc5a5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 2, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: divide by zero encountered in true_divide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs9Rtd01f7hG"
      },
      "source": [
        "\n",
        "class AE(object):\n",
        "    def __init__(self, k, n, seed=None, filename=None):\n",
        "        assert (n%2 == 0), \"Channel only allows complex symbols -> n must be a multiple of 2\"\n",
        "        self.k = k \n",
        "        self.n = n\n",
        "        self.n_complex = int(self.n/2)\n",
        "        self.bits_per_symbol = self.k/self.n_complex\n",
        "        self.M = 2**self.k\n",
        "        self.seed = seed if (seed is not None) else int(time.time())           \n",
        "        self.graph = None\n",
        "        self.sess = None   \n",
        "        self.vars = None\n",
        "        self.saver = None   \n",
        "        self.constellations = None\n",
        "        self.blers = None\n",
        "        self.create_graph()\n",
        "        self.create_session()\n",
        "        if filename is not None:    \n",
        "            self.load(filename)       \n",
        "        return\n",
        "    \n",
        "    def generate_distances(self,int_batch_size,bins):\n",
        "        # int_batch_size: 1000\n",
        "        # bins = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "\n",
        "        t = np.linspace(0,1,int_batch_size)\n",
        "        triangle1 = signal.sawtooth(2 * np.pi * 5 * t, 0.5)\n",
        "        triangle1 = M/2(*triangle1)\n",
        "        triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "        triangle2 = signal.sawtooth(5 * np.pi * 5 * t, 0.5)\n",
        "        triangle2 = M/2*(triangle2)\n",
        "        triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "        triangle3 = triangle1 + triangle2\n",
        "        bins = np.arange(M-1)\n",
        "        tr = np.digitize(triangle3, bins, right=True)\n",
        "\n",
        "        print(tr)\n",
        "        rp1 = np.arange(M)\n",
        "        rp2 = np.flip(np.arange(M))\n",
        "        replacements = dict(zip(rp1,rp2))\n",
        "        replacer = replacements.get\n",
        "        tr = ([replacer(n, n) for n in tr])\n",
        "\n",
        "\n",
        "    \n",
        "    def create_graph(self):\n",
        "        '''This function creates the computation graph of the autoencoder'''\n",
        "        self.graph = tf.Graph()        \n",
        "        with self.graph.as_default():    \n",
        "            tf.set_random_seed(self.seed)\n",
        "            batch_size = tf.placeholder(tf.int32, shape=())\n",
        "            \n",
        "            # Transmitter\n",
        "            #s = tf.random_uniform(shape=[batch_size], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            #s = tf.random_uniform(shape=[1000], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            #s = tf.convert_to_tensor(s, np.int64)\n",
        "            #s = tf.convert_to_tensor(s, dtype=tf.int64)\n",
        "            x = self.encoder(s) \n",
        "\n",
        "                         \n",
        "            \n",
        "            # Channel\n",
        "            #noise_std = tf.placeholder(tf.float32, shape=())\n",
        "            noise_std = tf.placeholder(tf.float32, shape=[1000,2,2])\n",
        "            noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=noise_std) \n",
        "\n",
        "            fade = tf.random.normal(shape=tf.shape(x))\n",
        "            sparr1,sparr2 = tf.split(fade,num_or_size_splits=2, axis=2)\n",
        "            complex_fade = tf.complex(sparr1, sparr2)\n",
        "            fade = tf.abs(complex_fade)\n",
        "            #fade = tf.math.sqrt(1/2)*fade\n",
        "\n",
        "            #fade = 1\n",
        "            y = tf.multiply(x,fade) + noise\n",
        "            #y = x + noise\n",
        "            #fade = 1            \n",
        "           \n",
        "            # Receiver\n",
        "            s_hat = self.decoder(y)\n",
        "            correct_s_hat = tf.argmax(tf.nn.softmax(s_hat), axis=1)\n",
        "\n",
        "            \n",
        "            # Loss function\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            #cross_entropy_0 = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat, weights= (s+1)**(4))\n",
        "      \n",
        "           # Performance metrics\n",
        "            #correct_predictions_0 = tf.equal(tf.argmax(tf.nn.softmax([s_hat[x] for x in s_ind_0]), axis=1), [s[x] for x in s_ind_0])\n",
        "\n",
        "            correct_predictions = tf.equal(tf.argmax(tf.nn.softmax(s_hat), axis=1), s)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "            bler = 1-accuracy\n",
        "\n",
        "            lr = tf.placeholder(tf.float32, shape=()) # We can feed in any desired learning rate for each step     \n",
        "            train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
        "            #train_op_0 = tf.train.AdamOptimizer(lr).minimize(cross_entropy_0)\n",
        "            #lr = tf.train.exponential_decay(1e-10, global_step=cross_entropy, decay_steps=100, decay_rate=1.30)\n",
        "\n",
        "            # References to graph variables we need to access later \n",
        "            self.vars = {\n",
        "                'accuracy': accuracy,\n",
        "                'batch_size': batch_size,\n",
        "                'bler': bler,\n",
        "                'cross_entropy': cross_entropy,\n",
        "                'init': tf.global_variables_initializer(),\n",
        "                'lr': lr,\n",
        "                'noise_std': noise_std,\n",
        "                'train_op': train_op,\n",
        "                's': s,\n",
        "                's_hat': s_hat,\n",
        "                'correct_s_hat': correct_s_hat,\n",
        "                'x': x,\n",
        "            }            \n",
        "            self.saver = tf.train.Saver()\n",
        "        return\n",
        "    \n",
        "    def create_session(self):\n",
        "        '''Create a session for the autoencoder instance with the compuational graph'''\n",
        "        self.sess = tf.Session(graph=self.graph)        \n",
        "        self.sess.run(self.vars['init'])\n",
        "        return\n",
        "    \n",
        "    def encoder(self, input):\n",
        "        '''The transmitter'''\n",
        "        self.weight_var_rec = self.weight_variable((self.M,self.M)) # shape = (8,8)\n",
        "        self.embedding_lookup_rec = tf.nn.embedding_lookup(self.weight_var_rec, input)\n",
        "        print(self.embedding_lookup_rec)\n",
        "        x = tf.nn.elu(self.embedding_lookup_rec)\n",
        "        #x = tf.layers.dense(self.embedding_lookup_rec, self.M, activation=tf.nn.relu)\n",
        "        x = tf.layers.dense(x, self.M, activation=None)\n",
        "        x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        x = tf.reshape(x, shape=[-1,self.n_complex,2])\n",
        "        print(x);\n",
        "        #Average power normalization\n",
        "        x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x))) \n",
        "        return x\n",
        "    \n",
        "    def decoder(self, input):\n",
        "        '''The Receiver'''\n",
        "        #input = self.flip_decoder(input)\n",
        "        y = tf.reshape(input, shape=[-1,self.n])\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        print(y);\n",
        "        return y\n",
        "    \n",
        "    def EbNo2Sigma(self, ebnodb):\n",
        "        '''Convert Eb/No in dB to noise standard deviation'''\n",
        "        ebno = 10**(ebnodb/10)\n",
        "        return 1/np.sqrt(2*self.bits_per_symbol*ebno)\n",
        "    \n",
        "    def gen_feed_dict(self, batch_size, ebnodb, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['lr']: lr,\n",
        "        }\n",
        "\n",
        "    def gen_e2e_feed_dict(self, batch_size, ebnodb, s_input):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['s']: s_input,\n",
        "        }   \n",
        "    \n",
        "    def load(self, filename):\n",
        "        '''Load an pre_trained model'''\n",
        "        return self.saver.restore(self.sess, filename)\n",
        "        \n",
        "    def plot_constellation(self, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = self.transmit(range(self.M))\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(self.n_complex):\n",
        "            image = plt.figure(figsize=(6,6))\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-maxrange,maxrange)\n",
        "            plt.ylim(-maxrange,maxrange)\n",
        "            for i in range(self.M):       \n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\",marker='x')   \n",
        "            image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            image.suptitle('%d. complex symbol' % (k+1))\n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "        return x, image\n",
        "    \n",
        "    def save(self, filename):\n",
        "        '''Save the current model'''\n",
        "        return self.saver.save(self.sess, filename)  \n",
        "    \n",
        "    def test_step(self, batch_size, ebnodb):\n",
        "        '''Compute the BLER over a single batch and Eb/No'''\n",
        "        bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n",
        "        return bler\n",
        "    \n",
        "    def transmit(self, s, noise_std):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['x'], feed_dict={self.vars['s']: s, self.vars['noise_std']: noise_std})\n",
        "\n",
        "    def end2end(self, batch_size, ebnodb, input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['correct_s_hat'], feed_dict=self.gen_e2e_feed_dict(batch_size, ebnodb, input_s)) \n",
        "        #print(self.sess.run(self.vars['correct_s_hat'], feed_dict={self.vars['s']: input_s}))     \n",
        "\n",
        "    #print(self.sess.run(self.vars['s_hat'], feed_dict={self.vars['s']: s}))\n",
        "\n",
        "    def train(self, training_params, validation_params):  \n",
        "        #s_input = self.generate_distances(100,np.array([0, 1, 2, 3, 4, 5, 6]))\n",
        "        \n",
        "        '''Training and validation loop'''\n",
        "        for index, params in enumerate(training_params):            \n",
        "            batch_size, lr, ebnodb, iterations = params            \n",
        "            print('\\nBatch Size: ' + str(batch_size) +\n",
        "                  ', Learning Rate: ' + str(lr) +\n",
        "                  ', EbNodB: ' + str(ebnodb) +\n",
        "                  ', Iterations: ' + str(iterations))\n",
        "            \n",
        "            val_size, val_ebnodb, val_steps = validation_params[index]\n",
        "            \n",
        "            for i in range(iterations):\n",
        "                self.train_step(batch_size, ebnodb, lr)    \n",
        "                if (i%val_steps==0):\n",
        "                    #bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_new_feed_dict(val_size, val_ebnodb, lr, s_input))\n",
        "                    bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size, val_ebnodb, lr))\n",
        "                    print(bler)                           \n",
        "        return       \n",
        "    \n",
        "    def train_step(self, batch_size, ebnodb, lr):\n",
        "        '''A single training step'''\n",
        "        #self.sess.run(self.vars['train_op'], feed_dict=self.gen_new_feed_dict(batch_size, ebnodb, lr, s_input))\n",
        "        self.sess.run(self.vars['train_op'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr))\n",
        "        return \n",
        "    \n",
        "    def weight_variable(self, shape):\n",
        "        '''Xavier-initialized weights optimized for ReLU Activations'''\n",
        "        (fan_in, fan_out) = shape\n",
        "        low = np.sqrt(6.0/(fan_in + fan_out)) \n",
        "        high = -np.sqrt(6.0/(fan_in + fan_out))\n",
        "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n",
        "    \n",
        "    def bler_sim(self, ebnodbs, batch_size, iterations):\n",
        "        '''Monte Carlo simulations of BLER for a range of Eb/No\n",
        "           Sometimes we to compute statistics for batch sizes that do not fit into the GPUs memory.\n",
        "           You can average over multiple batches with small size instead.           \n",
        "        '''\n",
        "        BLER = np.zeros_like(ebnodbs)\n",
        "        for i in range(iterations):\n",
        "            bler = np.array([self.sess.run(self.vars['bler'],\n",
        "                            feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0)) for ebnodb in ebnodbs])\n",
        "            BLER = BLER + bler/iterations\n",
        "        return BLER\n",
        "    \n",
        "    def plot_bler(self, EbNodB, BLER):\n",
        "        '''Plot a BLER curve'''\n",
        "        image = plt.figure(figsize=(10,8))\n",
        "        plt.plot(EbNodB, BLER, '-r', linewidth=2.0)\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('EbNo (dB)', fontsize=18)\n",
        "        plt.ylabel('Block-error rate', fontsize=18)\n",
        "        plt.grid(True)\n",
        "        plt.ylim([1e-5,1])\n",
        "        return image"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bANUdLIsf7hM"
      },
      "source": [
        "## Training parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeHghWVRf7hO",
        "outputId": "beacb86f-01d1-4bc8-a410-6f84364ecf16"
      },
      "source": [
        "train_EbNodB = ad_noise_std\n",
        "val_EbNodB = train_EbNodB\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "epoch = [10000]\n",
        "\n",
        "training_params = [\n",
        "    #batch_size, lr, ebnodb, iterations\n",
        "    [1000 , 0.01, train_EbNodB, 10000]\n",
        "#    [10000 , 0.001, train_EbNodB, 10000]    \n",
        "    ]\n",
        "\n",
        "validation_params = [\n",
        "    #batch_size, ebnodb, val_steps \n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000]\n",
        "    ]\n",
        "\n",
        "model_file_uw = 'models/ae_k_{}_n_{}'.format(k,n)\n",
        "ae = AE(k,n,seed)\n",
        "ae.train(training_params, validation_params)\n",
        "ae.save(model_file_uw);\n",
        "ae = AE(k,n,seed, filename=model_file_uw)\n",
        "  #ae.plot_constellation();\n",
        "  #ae.end2end(tr)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n",
            "\n",
            "Batch Size: 1000, Learning Rate: 0.01, EbNodB: [[[38.25077539 38.25077539]\n",
            "  [38.25077539 38.25077539]]\n",
            "\n",
            " [[41.68999602 41.68999602]\n",
            "  [41.68999602 41.68999602]]\n",
            "\n",
            " [[45.59211346 45.59211346]\n",
            "  [45.59211346 45.59211346]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[58.28892994 58.28892994]\n",
            "  [58.28892994 58.28892994]]\n",
            "\n",
            " [[41.68999602 41.68999602]\n",
            "  [41.68999602 41.68999602]]\n",
            "\n",
            " [[44.71393043 44.71393043]\n",
            "  [44.71393043 44.71393043]]], Iterations: 10000\n",
            "0.94200003\n",
            "0.05400002\n",
            "0.05400002\n",
            "0.036000013\n",
            "0.055000007\n",
            "0.035000026\n",
            "0.029999971\n",
            "0.029999971\n",
            "0.032999992\n",
            "0.007000029\n",
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VKoeXIMOcpbJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl3hENsvf7hW"
      },
      "source": [
        "## Create and train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzqpcwGaf7hm"
      },
      "source": [
        "## Evaluate trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y33vV4xKf7hn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bbec7a4-6b46-4795-dbcf-e0e6e93b1445"
      },
      "source": [
        "ae_uw = AE(k,n,seed, filename=model_file_uw) #Load a pretrained model that you have saved if needed"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fasS-Rz1lapW"
      },
      "source": [
        "\n",
        "class AE_Weighted(object):\n",
        "    def __init__(self, k, n, seed=None, filename=None):\n",
        "        assert (n%2 == 0), \"Channel only allows complex symbols -> n must be a multiple of 2\"\n",
        "        self.k = k \n",
        "        self.n = n\n",
        "        self.n_complex = int(self.n/2)\n",
        "        self.bits_per_symbol = self.k/self.n_complex\n",
        "        self.M = 2**self.k\n",
        "        self.seed = seed if (seed is not None) else int(time.time())           \n",
        "        self.graph = None\n",
        "        self.sess = None   \n",
        "        self.vars = None\n",
        "        self.saver = None   \n",
        "        self.constellations = None\n",
        "        self.blers = None\n",
        "        self.create_graph()\n",
        "        self.create_session()\n",
        "        if filename is not None:    \n",
        "            self.load(filename)       \n",
        "        return\n",
        "    \n",
        "    def generate_distances(self,int_batch_size,bins):\n",
        "        # int_batch_size: 1000\n",
        "        # bins = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "\n",
        "        t = np.linspace(0,1,int_batch_size)\n",
        "        triangle1 = signal.sawtooth(2 * np.pi * 5 * t, 0.5)\n",
        "        triangle1 = M/2(*triangle1)\n",
        "        triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "        triangle2 = signal.sawtooth(5 * np.pi * 5 * t, 0.5)\n",
        "        triangle2 = M/2*(triangle2)\n",
        "        triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "        triangle3 = triangle1 + triangle2\n",
        "        bins = np.arange(M-1)\n",
        "        tr = np.digitize(triangle3, bins, right=True)\n",
        "\n",
        "        print(tr)\n",
        "        rp1 = np.arange(M)\n",
        "        rp2 = np.flip(np.arange(M))\n",
        "        replacements = dict(zip(rp1,rp2))\n",
        "        replacer = replacements.get\n",
        "        tr = ([replacer(n, n) for n in tr])\n",
        "\n",
        "    \n",
        "    def create_graph(self):\n",
        "        '''This function creates the computation graph of the autoencoder'''\n",
        "        self.graph = tf.Graph()        \n",
        "        with self.graph.as_default():    \n",
        "            tf.set_random_seed(self.seed)\n",
        "            batch_size = tf.placeholder(tf.int32, shape=())\n",
        "            \n",
        "            # Transmitter\n",
        "            #s = tf.random_uniform(shape=[batch_size], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "            #s = tf.random_uniform(shape=[1000], minval=0, maxval=self.M, dtype=tf.int64)\n",
        "\n",
        "            #s = tf.convert_to_tensor(s, dtype=tf.int64)\n",
        "            x = self.encoder(s) \n",
        "\n",
        "                         \n",
        "            \n",
        "            # Channel\n",
        "            #noise_std = tf.placeholder(tf.float32, shape=())\n",
        "            noise_std = tf.placeholder(tf.float32, shape=[1000,2,2])\n",
        "            noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=noise_std) \n",
        "\n",
        "            fade = tf.random.normal(shape=tf.shape(x))\n",
        "            sparr1,sparr2 = tf.split(fade,num_or_size_splits=2, axis=2)\n",
        "            complex_fade = tf.complex(sparr1, sparr2)\n",
        "            fade = tf.abs(complex_fade)\n",
        "            #fade = tf.math.sqrt(1/2)*fade\n",
        "\n",
        "            #fade = 1\n",
        "            y = tf.multiply(x,fade) + noise\n",
        "            #y = x + noise\n",
        "            #fade = 1            \n",
        "           \n",
        "            # Receiver\n",
        "            s_hat = self.decoder(y)\n",
        "            correct_s_hat = tf.argmax(tf.nn.softmax(s_hat), axis=1)\n",
        "\n",
        "            \n",
        "            # Loss function\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            #cross_entropy_0 = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat, weights= (s+1)**(2))\n",
        "      \n",
        "           # Performance metrics\n",
        "            #correct_predictions_0 = tf.equal(tf.argmax(tf.nn.softmax([s_hat[x] for x in s_ind_0]), axis=1), [s[x] for x in s_ind_0])\n",
        "\n",
        "            correct_predictions = tf.equal(tf.argmax(tf.nn.softmax(s_hat), axis=1), s)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "            bler = 1-accuracy\n",
        "\n",
        "            lr = tf.placeholder(tf.float32, shape=()) # We can feed in any desired learning rate for each step     \n",
        "            train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
        "            #train_op_0 = tf.train.AdamOptimizer(lr).minimize(cross_entropy_0)\n",
        "            #lr = tf.train.exponential_decay(1e-10, global_step=cross_entropy, decay_steps=100, decay_rate=1.30)\n",
        "\n",
        "            # References to graph variables we need to access later \n",
        "            self.vars = {\n",
        "                'accuracy': accuracy,\n",
        "                'batch_size': batch_size,\n",
        "                'bler': bler,\n",
        "                'cross_entropy': cross_entropy,\n",
        "                'init': tf.global_variables_initializer(),\n",
        "                'lr': lr,\n",
        "                'noise_std': noise_std,\n",
        "                'train_op': train_op,\n",
        "                's': s,\n",
        "                's_hat': s_hat,\n",
        "                'correct_s_hat': correct_s_hat,\n",
        "                'x': x,\n",
        "            }            \n",
        "            self.saver = tf.train.Saver()\n",
        "        return\n",
        "    \n",
        "    \n",
        "    def create_session(self):\n",
        "        '''Create a session for the autoencoder instance with the compuational graph'''\n",
        "        self.sess = tf.Session(graph=self.graph)        \n",
        "        self.sess.run(self.vars['init'])\n",
        "        return\n",
        "    \n",
        "    def encoder(self, input):\n",
        "        '''The transmitter'''\n",
        "        self.weight_var_rec = self.weight_variable((self.M,self.M)) # shape = (8,8)\n",
        "        self.embedding_lookup_rec = tf.nn.embedding_lookup(self.weight_var_rec, input)\n",
        "        print(self.embedding_lookup_rec)\n",
        "        x = tf.nn.elu(self.embedding_lookup_rec)\n",
        "        #x = tf.layers.dense(self.embedding_lookup_rec, self.M, activation=tf.nn.relu)\n",
        "        x = tf.layers.dense(x, self.M, activation=None)\n",
        "        x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        x = tf.reshape(x, shape=[-1,self.n_complex,2])\n",
        "        print(x);\n",
        "        #Average power normalization\n",
        "        x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x))) \n",
        "        return x\n",
        "    \n",
        "    def decoder(self, input):\n",
        "        '''The Receiver'''\n",
        "        #input = self.flip_decoder(input)\n",
        "        y = tf.reshape(input, shape=[-1,self.n])\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        print(y);\n",
        "        return y\n",
        "    \n",
        "    def EbNo2Sigma(self, ebnodb):\n",
        "        '''Convert Eb/No in dB to noise standard deviation'''\n",
        "        ebno = 10**(ebnodb/10)\n",
        "        return 1/np.sqrt(2*self.bits_per_symbol*ebno)\n",
        "    \n",
        "    def gen_feed_dict(self, batch_size, ebnodb, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['lr']: lr,\n",
        "        }\n",
        "\n",
        "    def gen_e2e_feed_dict(self, batch_size, ebnodb, s_input):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['s']: s_input,\n",
        "        }   \n",
        "    \n",
        "    def load(self, filename):\n",
        "        '''Load an pre_trained model'''\n",
        "        return self.saver.restore(self.sess, filename)\n",
        "        \n",
        "    def plot_constellation(self, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = self.transmit(range(self.M))\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(self.n_complex):\n",
        "            image = plt.figure(figsize=(6,6))\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-maxrange,maxrange)\n",
        "            plt.ylim(-maxrange,maxrange)\n",
        "            for i in range(self.M):       \n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\",marker='x')   \n",
        "            image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            image.suptitle('%d. complex symbol' % (k+1))\n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "        return x, image\n",
        "    \n",
        "    def save(self, filename):\n",
        "        '''Save the current model'''\n",
        "        return self.saver.save(self.sess, filename)  \n",
        "    \n",
        "    def test_step(self, batch_size, ebnodb):\n",
        "        '''Compute the BLER over a single batch and Eb/No'''\n",
        "        bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n",
        "        return bler\n",
        "    \n",
        "    def transmit(self, s, noise_std):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['x'], feed_dict={self.vars['s']: s, self.vars['noise_std']: noise_std})\n",
        "\n",
        "    def end2end(self, batch_size, ebnodb, input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['correct_s_hat'], feed_dict=self.gen_e2e_feed_dict(batch_size, ebnodb, input_s)) \n",
        "        #print(self.sess.run(self.vars['correct_s_hat'], feed_dict={self.vars['s']: input_s}))     \n",
        "\n",
        "    #print(self.sess.run(self.vars['s_hat'], feed_dict={self.vars['s']: s}))\n",
        "\n",
        "    def train(self, training_params, validation_params):  \n",
        "        #s_input = self.generate_distances(100,np.array([0, 1, 2, 3, 4, 5, 6]))\n",
        "        \n",
        "        '''Training and validation loop'''\n",
        "        for index, params in enumerate(training_params):            \n",
        "            batch_size, lr, ebnodb, iterations = params            \n",
        "            print('\\nBatch Size: ' + str(batch_size) +\n",
        "                  ', Learning Rate: ' + str(lr) +\n",
        "                  ', EbNodB: ' + str(ebnodb) +\n",
        "                  ', Iterations: ' + str(iterations))\n",
        "            \n",
        "            val_size, val_ebnodb, val_steps = validation_params[index]\n",
        "            \n",
        "            for i in range(iterations):\n",
        "                self.train_step(batch_size, ebnodb, lr)    \n",
        "                if (i%val_steps==0):\n",
        "                    #bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_new_feed_dict(val_size, val_ebnodb, lr, s_input))\n",
        "                    bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size, val_ebnodb, lr))\n",
        "                    print(bler)                           \n",
        "        return       \n",
        "    \n",
        "    def train_step(self, batch_size, ebnodb, lr):\n",
        "        '''A single training step'''\n",
        "        #self.sess.run(self.vars['train_op'], feed_dict=self.gen_new_feed_dict(batch_size, ebnodb, lr, s_input))\n",
        "        self.sess.run(self.vars['train_op'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr))\n",
        "        return \n",
        "    \n",
        "    def weight_variable(self, shape):\n",
        "        '''Xavier-initialized weights optimized for ReLU Activations'''\n",
        "        (fan_in, fan_out) = shape\n",
        "        low = np.sqrt(6.0/(fan_in + fan_out)) \n",
        "        high = -np.sqrt(6.0/(fan_in + fan_out))\n",
        "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n",
        "    \n",
        "    def bler_sim(self, ebnodbs, batch_size, iterations):\n",
        "        '''Monte Carlo simulations of BLER for a range of Eb/No\n",
        "           Sometimes we to compute statistics for batch sizes that do not fit into the GPUs memory.\n",
        "           You can average over multiple batches with small size instead.           \n",
        "        '''\n",
        "        BLER = np.zeros_like(ebnodbs)\n",
        "        for i in range(iterations):\n",
        "            bler = np.array([self.sess.run(self.vars['bler'],\n",
        "                            feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0)) for ebnodb in ebnodbs])\n",
        "            BLER = BLER + bler/iterations\n",
        "        return BLER\n",
        "    \n",
        "    def plot_bler(self, EbNodB, BLER):\n",
        "        '''Plot a BLER curve'''\n",
        "        image = plt.figure(figsize=(10,8))\n",
        "        plt.plot(EbNodB, BLER, '-r', linewidth=2.0)\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('EbNo (dB)', fontsize=18)\n",
        "        plt.ylabel('Block-error rate', fontsize=18)\n",
        "        plt.grid(True)\n",
        "        plt.ylim([1e-5,1])\n",
        "        return image"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjh_J3MIlyrF",
        "outputId": "c11e45c4-36bb-4822-ede9-d9187a1bf2b6"
      },
      "source": [
        "train_EbNodB = ad_noise_std\n",
        "val_EbNodB = train_EbNodB\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "epoch = [10000]\n",
        "\n",
        "#for i in epoch:\n",
        "training_params = [\n",
        "    #batch_size, lr, ebnodb, iterations\n",
        "    [1000 , 0.01, train_EbNodB, 10000]\n",
        "#    [50000 , 0.001, train_EbNodB, 10000]    \n",
        "    ]\n",
        "\n",
        "validation_params = [\n",
        "    #batch_size, ebnodb, val_steps \n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000],\n",
        "    [10000, val_EbNodB, 1000]\n",
        "    ]\n",
        "\n",
        "model_file = 'models/ae_k_{}_n_{}'.format(k,n)\n",
        "ae_Weighted = AE_Weighted(k,n,seed)\n",
        "ae_Weighted.train(training_params, validation_params)\n",
        "ae_Weighted.save(model_file);\n",
        "ae_Weighted = AE_Weighted(k,n,seed, filename=model_file)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n",
            "\n",
            "Batch Size: 1000, Learning Rate: 0.01, EbNodB: [[[38.25077539 38.25077539]\n",
            "  [38.25077539 38.25077539]]\n",
            "\n",
            " [[41.68999602 41.68999602]\n",
            "  [41.68999602 41.68999602]]\n",
            "\n",
            " [[45.59211346 45.59211346]\n",
            "  [45.59211346 45.59211346]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[58.28892994 58.28892994]\n",
            "  [58.28892994 58.28892994]]\n",
            "\n",
            " [[41.68999602 41.68999602]\n",
            "  [41.68999602 41.68999602]]\n",
            "\n",
            " [[44.71393043 44.71393043]\n",
            "  [44.71393043 44.71393043]]], Iterations: 10000\n",
            "0.957\n",
            "0.236\n",
            "0.16900003\n",
            "0.18400002\n",
            "0.15899998\n",
            "0.15899998\n",
            "0.12900001\n",
            "0.148\n",
            "0.12900001\n",
            "0.12699997\n",
            "Tensor(\"embedding_lookup/Identity:0\", shape=(1000, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(1000, 2, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(1000, 256), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsoF-I7Rf7hy"
      },
      "source": [
        "### Plot of learned constellations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjGW8S_df7h0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "09dfca3e-0297-443f-fe61-045a826513c7"
      },
      "source": [
        "import itertools\n",
        "def plot_constellation_2(ae, arr, noise_std,  maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = ae.transmit(arr, noise_std)\n",
        "        #marker = itertools.cycle(('1', '2', '3', '4', '^', '6', '7', '8'))\n",
        "        #weights = [1,4,9,16,25,36,49,64]\n",
        "        #weights = [1,4,9,16,25,36,49,64,81,100,121,144,169,196,225,256]\n",
        "        #weights = [1,1,1,1,1,1,1,1]\n",
        "        #weights = [1,8,27,64,125,216,343,512]\n",
        "        #weights = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
        "        weights = np.arange(M)\n",
        "        #weights = [1,16,81,256,625, 1296, 2401, 4096]\n",
        "        #weights = [1,256,6561,65536, 390625, 1.6, 5.7, 16.7]\n",
        "        #x = ae.transmit(np.ones(ae.M)*n)\n",
        "        #print(ae.M)\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(ae.n_complex):\n",
        "            \n",
        "#            image = plt.figure(figsize=(6,6))\n",
        "            image = plt.plot()\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-2,2)\n",
        "            plt.ylim(-2,2)\n",
        "            #plt.show() \n",
        "            #plt.ion()\n",
        "            xshape = np.shape(x)\n",
        "            for i in range(xshape[0]):      \n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\")\n",
        "                plt.annotate(weights[i], (x[i,k,0],x[i,k,1]))\n",
        "\n",
        "\n",
        "                #plt.show()              \n",
        "                #plt.pause(1)\n",
        "                #plt.hold(True)\n",
        "            #image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            #image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            #image.suptitle('%d. complex symbol' % (k+1))\n",
        "            #image.canvas.draw()\n",
        "            #image.canvas.flush_events()\n",
        "            \n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "            #time.sleep(0.1)\n",
        "        return x, image\n",
        "\n",
        "plot_constellation_2(ae,range(0,ae.M), const_noise_std)\n",
        "#plot_constellation_2(ae_Weighted, range(0, ae_Weighted.M))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-2f476ba0a2db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mplot_constellation_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mae\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconst_noise_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;31m#plot_constellation_2(ae_Weighted, range(0, ae_Weighted.M))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-2f476ba0a2db>\u001b[0m in \u001b[0;36mplot_constellation_2\u001b[0;34m(ae, arr, noise_std, maxrange)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_constellation_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_std\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmaxrange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m'''Generate a plot of the current constellation'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;31m#marker = itertools.cycle(('1', '2', '3', '4', '^', '6', '7', '8'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m#weights = [1,4,9,16,25,36,49,64]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-cc208ff61940>\u001b[0m in \u001b[0;36mtransmit\u001b[0;34m(self, s, noise_std)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;34m'''Returns the transmitted sigals corresponding to message indices'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'noise_std'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnoise_std\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mend2end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mebnodb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMbROTHNMa79"
      },
      "source": [
        "**THE RMSE Calculations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7-W-Be2auJV"
      },
      "source": [
        "def rmse(predictions,targets):\n",
        "  return (np.sqrt((np.subtract(predictions,targets) ** 2).mean()))   \n",
        "\n",
        "tr_hat = ae.end2end(len(tr), tr_noise_std, tr)\n",
        "tr_hat_w = ae_Weighted.end2end(len(tr), tr_noise_std, tr)\n",
        "#print(np.shape(tr))\n",
        "#print(np.shape(tr_hat))\n",
        "print(tr)\n",
        "print(tr_hat_w)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rmse_uw = {}\n",
        "for j in range(M):\n",
        "  rmse_uw[j] = rmse(([tr_hat[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "\n",
        "rmse_w = {}\n",
        "for j in range(M):\n",
        "  rmse_w[j] = rmse(([tr_hat_w[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "\n",
        "\n",
        "\n",
        "#message_factor = [15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,1]\n",
        "message_factor = np.flip(np.arange(M))\n",
        "message_factor[M-1] = 1\n",
        "rmse_uwA = []\n",
        "for i in rmse_uw.values():\n",
        "  rmse_uwA.append(i)\n",
        "\n",
        "rmse_wA = []\n",
        "for i in rmse_w.values():\n",
        "  rmse_wA.append(i)\n",
        "\n",
        "    \n",
        "#rmse_uw = [rmse_uw_0, rmse_uw_1, rmse_uw_2, rmse_uw_3, rmse_uw_4, rmse_uw_5, rmse_uw_6, rmse_uw_7, rmse_uw_8, rmse_uw_9, rmse_uw_10, rmse_uw_11, rmse_uw_12, rmse_uw_13, rmse_uw_14, rmse_uw_15]\n",
        "rmse_uw = (np.divide(rmse_uwA,message_factor))\n",
        "#rmse_w = [rmse_w_0, rmse_w_1, rmse_w_2, rmse_w_3, rmse_w_4, rmse_w_5, rmse_w_6, rmse_w_7, rmse_w_8, rmse_w_9, rmse_w_10, rmse_w_11, rmse_w_12, rmse_w_13, rmse_w_14, rmse_w_15]\n",
        "rmse_w = (np.divide(rmse_wA,message_factor))\n",
        "#message = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
        "message = np.arange(M)\n",
        "\n",
        "print(rmse_uw)\n",
        "plt.plot(message, rmse_uw, '-o');\n",
        "plt.plot(message, rmse_w, '-*');\n",
        "plt.xlabel('Message Bit')\n",
        "plt.ylabel('RMSE deviation log2')\n",
        "plt.legend(['Unweighted', 'Weighted(^2)'])\n",
        "\n",
        "\n",
        "print(np.sum(rmse_uw))\n",
        "print(np.sum(rmse_w))\n",
        "\n",
        "print(np.sum(rmse_uwA))\n",
        "print(np.sum(rmse_wA))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxostCb-vY1i"
      },
      "source": [
        "ae.plot_constellation();\n",
        "#plot_constellation_2(ae,range(0,ae.M))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75eUVLFnq1JO"
      },
      "source": [
        "ae_Weighted.plot_constellation();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtoc7OKCUgko"
      },
      "source": [
        "# 8-PSK Modulation\n",
        "#8PSK constellation \n",
        "#Demodulation matrx\n",
        "#Qfunction \n",
        "\n",
        "import numpy as np\n",
        "from scipy import special\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import subprocess\n",
        "import shlex\n",
        "\n",
        "\n",
        "#Generating constellation points\n",
        "s = np.zeros((8,2))\n",
        "s_comp = np.zeros((8,1))+1j*np.zeros((8,1))\n",
        "for i in range(8):\n",
        "\ts[i,:] = np.array(([np.cos(i*2*np.pi/8),np.sin(i*2*np.pi/8)])) #vector\n",
        "\ts_comp[i] = s[i,0]+1j*s[i,1] #equivalent complex number\n",
        "\n",
        "#Generating demodulation matrix\n",
        "A = np.zeros((8,2,2))\n",
        "A[0,:,:] = np.array(([np.sqrt(2)-1,1],[np.sqrt(2)-1,-1]))\n",
        "A[1,:,:] = np.array(([np.sqrt(2)+1,-1],[-(np.sqrt(2)-1),1]))\n",
        "A[2,:,:] = np.array(([-(np.sqrt(2)+1),1],[np.sqrt(2)+1,1]))\n",
        "A[3,:,:] = np.array(([np.sqrt(2)-1,1],[-(np.sqrt(2)+1),-1]))\n",
        "A[4,:,:] = np.array(([-(np.sqrt(2)-1),-1],[-(np.sqrt(2)-1),1]))\n",
        "A[5,:,:] = np.array(([-(np.sqrt(2)+1),1],[np.sqrt(2)-1,-1]))\n",
        "A[6,:,:] = np.array(([np.sqrt(2)+1,-1],[-(np.sqrt(2)+1),-1]))\n",
        "A[7,:,:] = np.array(([-(np.sqrt(2)-1),-1],[np.sqrt(2)+1,1]))\n",
        "\n",
        "#Gray code\n",
        "gray = np.zeros((8,3))\n",
        "gray[0,:] = np.array(([0,0,0]))\n",
        "gray[1,:] = np.array(([0,0,1]))\n",
        "gray[2,:] = np.array(([0,1,1]))\n",
        "gray[3,:] = np.array(([0,1,0]))\n",
        "gray[4,:] = np.array(([1,1,0]))\n",
        "gray[5,:] = np.array(([1,1,1]))\n",
        "gray[6,:] = np.array(([1,0,1]))\n",
        "gray[7,:] = np.array(([1,0,0]))\n",
        "\n",
        "\n",
        "#Q-function\n",
        "def qfunc(x):\n",
        "\treturn 0.5*special.erfc(x/np.sqrt(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LDujO11Ulo5"
      },
      "source": [
        "def decode(vec_comp):\n",
        "\tvec = np.zeros((2,1))\n",
        "\tvec[0] = np.real(vec_comp)\n",
        "\tvec[1] = np.imag(vec_comp)\n",
        "\tfor i in range(8):\n",
        "\t\ty = A[i,:,:]@vec\n",
        "\t\tif (y [0] >= 0) and (y[1] >= 0):\n",
        "\t\t\treturn s_comp[i]\n",
        "\n",
        "#Extracting bits from demodulated symbols\n",
        "def detect(vec_comp):\n",
        "\tvec = np.zeros((2,1))\n",
        "\tvec[0] = np.real(vec_comp)\n",
        "\tvec[1] = np.imag(vec_comp)\n",
        "\tfor i in range(8):\n",
        "\t\tif s[i,0]==vec[0] and s[i,1] == vec[1]:\n",
        "\t\t\treturn gray[i,:]\n",
        "\n",
        "#Demodulating symbol stream from received noisy  symbols\n",
        "def rx_symb(mat):\n",
        "\tlen = mat.shape[1]\n",
        "\trx_symb_stream = []\n",
        "\tfor i in range(len):\n",
        "\t\trx_symb_stream.append(decode(mat[:,i]))\n",
        "\treturn rx_symb_stream\n",
        "\n",
        "#Getting received bit stream from demodulated symbols\n",
        "def rx_bit(mat):\n",
        "\tlen = mat.shape[1]\n",
        "\trx_bit_stream = []\n",
        "\tfor i in range(len):\n",
        "\t\trx_bit_stream.append(detect(mat[:,i]))\n",
        "\treturn rx_bit_stream"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPu3HX_SUov_"
      },
      "source": [
        "#Generates a bitstream\n",
        "def bitstream(n):\n",
        "\treturn np.random.randint(0,2,n)\n",
        "\n",
        "#Converts bits to 8-PSK symbols using gray code\n",
        "def mapping(b0,b1,b2):\n",
        "\tif (b0 == 0 and b1 == 0 and b2 == 0):\n",
        "\t\treturn s[0,:]\n",
        "\telif (b0 == 0 and b1 == 0 and b2 == 1):\n",
        "\t\treturn s[1,:]\n",
        "\telif (b0 == 0 and b1 == 1 and b2 == 1):\n",
        "\t\treturn s[2,:]\n",
        "\telif (b0 == 0 and b1 == 1 and b2 == 0):\n",
        "\t\treturn s[3,:]\n",
        "\telif( b0 == 1 and b1 == 1 and b2 == 0):\n",
        "\t\treturn s[4,:]\n",
        "\telif(b0==1 and b1 == 1 and b2 == 1):\n",
        "\t\treturn s[5,:]\n",
        "\telif(b0==1 and b1 == 0 and b2 == 1):\n",
        "\t\treturn s[6,:]\n",
        "\telif(b0==1 and b1 == 0 and b2 == 0):\n",
        "\t\treturn s[7,:]\n",
        "\n",
        "\n",
        "#Converts bitstream to 8-PSK symbol stream\n",
        "def symb(bits):\n",
        "\tsymbol =[]\n",
        "\ti = 0\n",
        "\twhile(1):\n",
        "\t\ttry:\n",
        "\t\t\tsymbol.append(mapping(bits[i],bits[i+1],bits[i+2]))\n",
        "\t\t\ti = i+3\n",
        "\t\texcept IndexError:\n",
        "\t\t\treturn symbol\n",
        "\n",
        "#Converts bitstream to 8-PSK complex symbol stream\n",
        "def CompSymb(bits):\n",
        "\tsymbols_lst = symb(bits)\n",
        "\tsymbols = np.array(symbols_lst).T #Symbol vectors\n",
        "\tsymbols_comp = symbols[0,:]+1j*symbols[1,:] #Equivalent complex symbols\n",
        "\treturn symbols_comp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPNtUtWBUryp"
      },
      "source": [
        "\n",
        "#SNR range\n",
        "snrlen=15\n",
        "\n",
        "#SNR in dB and actual per bit \n",
        "#(Check Proakis for factor of 6)\n",
        "snr_db = np.linspace(0,snrlen,snrlen)\n",
        "snr = 6*10**(0.1*snr_db)\n",
        "\n",
        "#Bitstream size\n",
        "bitsimlen = 99999\n",
        "\n",
        "#Symbol stream size\n",
        "simlen = bitsimlen //3\n",
        "\n",
        "#Generating bitstream\n",
        "bits = bitstream(bitsimlen)\n",
        "\n",
        "#Converting bits to Gray coded 8-PSK symbols\n",
        "#Intermediate steps  required for converting list to\n",
        "#numpy matrix\n",
        "symbols_lst = symb(bits)\n",
        "symbols = np.array(symbols_lst).T #Symbol vectors\n",
        "symbols_comp = symbols[0,:]+1j*symbols[1,:] #Equivalent complex symbols\n",
        "\n",
        "ser =[]\n",
        "ser_anal=[]\n",
        "ber = []\n",
        "\n",
        "#SNRloop\n",
        "for k in range(0,snrlen):\n",
        "\treceived = []\n",
        "\tt=0\n",
        "\t#Complex noise\n",
        "\tnoise_comp = np.random.normal(0,1,simlen)+1j*np.random.normal(0,1,simlen)\n",
        "\t#Generating complex received symbols\n",
        "  #fade_comp = np.random.normal(0,1,simlen)+1j*np.random.normal(0,1,simlen)\n",
        "  #fade_comp = np.abs(fade_comp)\n",
        "  #fade_comp = np.math.sqrt(1/2)*fade_comp\n",
        "\ty_comp = np.sqrt(snr[k])*symbols_comp +noise_comp\n",
        "\tbrx = []\n",
        "\tfor i in range(simlen):\n",
        "\t\tsrx_comp = decode(y_comp[i]) #Received Symbol\n",
        "\t\tbrx.append(detect(srx_comp))  #Received Bits\n",
        "\t\tif symbols_comp[i]==srx_comp:\n",
        "\t\t\tt+=1; #Counting symbol errors\n",
        "\t#Evaluating SER\n",
        "\tser.append(1-(t/33334.0))\n",
        "\tser_anal.append(2*qfunc((np.sqrt(snr[k]))*np.sin(np.pi/8)))\n",
        "\t#Received bitstream\n",
        "\tbrx=np.array(brx).flatten()\n",
        "\t#Evaluating BER\n",
        "\tbit_diff = bits-brx\n",
        "\tber.append(1-len(np.where(bit_diff == 0)[0])/bitsimlen)\n",
        "\n",
        "\n",
        "\n",
        "#Plots\n",
        "plt.semilogy(snr_db,ser_anal,label='SER Analysis')\n",
        "plt.semilogy(snr_db,ser,'o',label='SER Sim')\n",
        "plt.semilogy(snr_db,ber,label='BER Sim')\n",
        "plt.xlabel('SNR$\\\\left(\\\\frac{E_b}{N_0}\\\\right)$')\n",
        "plt.ylabel('$P_e$')\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFZCQNe9f7iD"
      },
      "source": [
        "### BLER Simulations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wukzCBJff7iE"
      },
      "source": [
        " ebnodbs = np.linspace(0,14,15)\n",
        "BLER_8PSK = [0.3478959, 0.2926128, 0.2378847, 0.1854187, 0.1372344, 0.0953536, 0.0614003, 0.0360195, 0.0185215, 0.0082433, 0.0030178, 0.0008626, 0.0001903, 0.0000289, 0.0000027, ]\n",
        "blers = ae.bler_sim(ebnodbs, 1000000, 1);\n",
        "ae.plot_bler(ebnodbs, blers);\n",
        "blers_w = ae_Weighted.bler_sim(ebnodbs, 1000000, 1);\n",
        "#ae_Weighted.plot_bler(ebnodbs, blers_w);\n",
        "plt.plot(ebnodbs, blers_w)\n",
        "plt.semilogy(snr_db,ser,'o')\n",
        "plt.plot(ebnodbs,BLER_8PSK);\n",
        "plt.legend(['Autoencoder (Rayleigh+AWGN)', 'Weighted Autoencoder(Rayleigh+AWGN)', 'SER Sim(AWGN)', '8PSK(AWGN)'], prop={'size': 16}, loc='lower left');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMl-Rljrf7iR"
      },
      "source": [
        "np.divide(10,2)\n",
        "#IEEE 802.11p Max TX Power 30dBm\n",
        "#Path loss model corresponding: 20log10(d(m))  (We take n=2, FSPL)\n",
        "#0.1W corresponds to 20dBm or -10dB\n",
        "#7dB corresponds to 5W\n",
        "#IEEE 802.11p is an approved amendment to the IEEE 802.11 standard to add wireless access in vehicular environments (WAVE), a vehicular communication system. \n",
        "#IEEE 802.11p standard typically uses channels of 10 MHz bandwidth in the 5.9 GHz band (5.8505.925 GHz).\n",
        "# Noise Power -100dBm, TX Power is 20dBm and that \\\n",
        "#256m- -94dBm, 128m- -88dBm, 1m"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}