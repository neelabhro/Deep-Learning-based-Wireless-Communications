{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIoYASfEf7go"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKl9e4A0H1lk",
        "outputId": "b87c32ac-5b35-4bf4-d815-5da78cab3bd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using tensorflow version: 2.5.0\n",
            "WARNING:tensorflow:From /home/neelabhro/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "# magic command to use TF 1.X in colaboraty when importing tensorflow\n",
        "#%tensorflow_version 1.x \n",
        "import tensorflow as tf                       # imports the tensorflow library to the python kernel\n",
        "#tf.logging.set_verbosity(tf.logging.ERROR)    # sets the amount of debug information from TF (INFO, WARNING, ERROR)\n",
        "import time\n",
        "import random\n",
        "print(\"Using tensorflow version:\", tf.__version__)\n",
        "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "tf.disable_v2_behavior() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QiCDuiGqf7gr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "import math\n",
        "pi = tf.constant(math.pi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrLsO1Nxf7g3"
      },
      "source": [
        "#### System parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<module 'numpy.version' from '/home/neelabhro/.local/lib/python3.8/site-packages/numpy/version.py'>\n",
            "Using tensorflow version: 2.5.0\n",
            "1.19.5\n"
          ]
        }
      ],
      "source": [
        "print(np.version)\n",
        "print(\"Using tensorflow version:\", tf.__version__)\n",
        "print(np.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spN0MeqFD7x-",
        "outputId": "3d876d4e-dc96-462c-bb45-f34b05554bc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200000\n"
          ]
        }
      ],
      "source": [
        "batch_size = 200000\n",
        "########## BIT FLIPPING OFF\n",
        "#print(tr)\n",
        "#plt.plot(t, tr)\n",
        "T1 = random.randint(1, 10)\n",
        "T2 = random.randint(1, 10)\n",
        "M = 256\n",
        "t = np.linspace(0, 1, batch_size)\n",
        "triangle1 = signal.sawtooth(T1 * np.pi * 5 * t, 0.5)\n",
        "triangle1 = M/2*(triangle1)\n",
        "triangle1 = triangle1.clip(min=0)\n",
        "\n",
        "triangle2 = signal.sawtooth(T2 * np.pi * 5 * t, 0.5)\n",
        "triangle2 = M/2*(triangle2)\n",
        "triangle2 = triangle2.clip(min=0)\n",
        "\n",
        "triangle3 = triangle1 + triangle2\n",
        "#triangle3 = triangle3/15*(7)\n",
        "tr = triangle3\n",
        "bins = np.arange(M-1)\n",
        "tr = np.digitize(triangle3, bins, right=True)\n",
        "#plt.plot(t, triangle3)\n",
        "\n",
        "#tr = np.flip(tr)\n",
        "#print(tr)\n",
        "\n",
        "# tr is a random uniform distribution of numbers between 1 and M, with the length of the vector being the batch size\n",
        "\n",
        "\n",
        "tr = np.floor(np.random.uniform(1,M, batch_size))\n",
        "#print(tr)\n",
        "print(np.size(tr))\n",
        "#replacements = {0:7, 1:6, 2:5, 3:4, 4:3, 5:2, 6:1, 7:0}\n",
        "#replacements = {0:15, 1:14, 2:13, 3:12, 4:11, 5:10, 6:9, 7:8, 8:7, 9:6, 10:5, 11:4, 12:3, 13:2, 14:1, 15:0}\n",
        "rp1 = np.arange(1,M)\n",
        "rp2 = np.flip(np.arange(1,M))\n",
        "replacements = dict(zip(rp1,rp2))\n",
        "#print(rp2)\n",
        "replacer = replacements.get\n",
        "#tr = ([replacer(n, n) for n in tr])\n",
        "\n",
        "#print(tr)\n",
        "\n",
        "s_ind = {}\n",
        "for j in range(1,M):\n",
        "  s_ind[j] = [i for i, x in enumerate(tr) if x == j]\n",
        "#print(s_ind)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4eRvwZ5Nf7g6"
      },
      "outputs": [],
      "source": [
        "k = 8       # Number of information bits per message, i.e., M=2**k\n",
        "n = 8       # Number of real channel uses per message\n",
        "seed = 2    # Seed RNG reproduce identical results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28KCbvYif7hD"
      },
      "source": [
        "#### The Autoencoder Class\n",
        "In order to quickly experiment with different architecture and parameter choices, it is useful to create a Python class that has functions for training and inference. Each autoencoder instance has its own Tensorflow session and graph. Thus, you can have multiple instances running at the same time without interference between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0hIseee8ira",
        "outputId": "f3a18270-991a-455d-c37b-186d10a410ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(200000,)\n",
            "(200000, 4, 2)\n"
          ]
        }
      ],
      "source": [
        "f = 5.9*10**9;#in Hz corresponding to IEEE 802.11p\n",
        "lamda = 0.05;  #in metres\n",
        "Pt = 1; #BS transmitted power in watts\n",
        "\n",
        "BW = 10*10**6; #in Hz\n",
        "PLE = 2.8; #Path Loss exponent\n",
        "Beta = {}\n",
        "Pr = {}\n",
        "SNR_var = {}\n",
        "ad_noise_SNR = []  #The values of the dictionary, finally providing an array of stdevs\n",
        "NoW = 0.02\n",
        "\n",
        "Transmitted_Power = 20 #in dBm\n",
        "Noise_val = 95 #in -dBm\n",
        "\n",
        "noise_factor = Transmitted_Power + Noise_val\n",
        "#|β| =λ/(4πD)2.31 \n",
        "\n",
        "\n",
        "s = np.floor(np.random.uniform(1,M, batch_size))\n",
        "s = s.astype('int64')\n",
        "#s = ([replacer(n, n) for n in s])  \n",
        "std_shadow = 3          \n",
        "rand1 = np.random.normal(0,std_shadow,batch_size)\n",
        "for i in range(batch_size):\n",
        "  Beta[i] = 10*(np.log10(lamda/((4*np.pi*s[i])**PLE)))\n",
        "  SNR_var[i] = Beta[i] + noise_factor + rand1[i]\n",
        "#SNR_var = SNR_var + np.random.normal(0, 3, batch_size) \n",
        "  #SNR_var[i] = 10*(np.log10(np.divide((Pt*[(Beta[i])**2]),NoW)))\n",
        "  #SNR_var[i] = ((M)/((s[i]+1)*4))*25   #For M=8\n",
        "  #SNR_var[i] = ((M)/((s[i]+1)*4))      #For M=256\n",
        "\n",
        "for i in SNR_var.values():\n",
        "  ad_noise_SNR.append(i)\n",
        "print(np.shape(ad_noise_SNR))\n",
        "ad_noise_SNR = np.transpose(np.tile(ad_noise_SNR, (2,4,1)))\n",
        "print(np.shape(ad_noise_SNR))\n",
        "\n",
        "#print(np.shape(np.tile(ad_noise_SNR[7], (1000, 1, 1))))\n",
        "#print(np.shape(tr[7]*np.ones(batch_size)))\n",
        "rand2 = np.random.normal(0,std_shadow,batch_size)\n",
        "\n",
        "for j in range(1,M):\n",
        "\n",
        "\n",
        "  #SNR_var[j] = ((M)/((j+1)*4))*25\n",
        "  #SNR_var[j] = ((M)/((j+1)*4))\n",
        "  Beta[j] = 10*(np.log10(lamda/((4*np.pi*j)**2.8)))\n",
        "  SNR_var[j] = Beta[j] + noise_factor + rand2[j]\n",
        "\n",
        "const_noise_SNR = []  #The values of the dictionary, finally providing an array of stdevs\n",
        "for j in SNR_var.values():\n",
        "  const_noise_SNR.append(j)  \n",
        "const_noise_SNR = np.transpose(np.tile(const_noise_SNR, (2,4,1)))\n",
        "#print(const_noise_std)\n",
        "\n",
        "#Choosing 15dB SNR as the constant SNR for the TB Approach\n",
        "seven_noise_SNR = np.add(np.ones(batch_size)*7, np.random.normal(0,3,batch_size))\n",
        "\n",
        "seven_noise_SNR = np.transpose(np.tile(seven_noise_SNR, (2,4,1)))\n",
        "#print(np.random.normal(0,std_shadow,batch_size))\n",
        "\n",
        "tr_noise_SNR = []\n",
        "rand3 = np.random.normal(0,std_shadow,batch_size)\n",
        "\n",
        "for i in range(batch_size):\n",
        "\n",
        "  #SNR_var[i] = ((M)/((tr[i]+1)*4))*25\n",
        "  #SNR_var[i] = ((M)/((tr[i]+1)*4))\n",
        "\n",
        "  Beta[i] = 10*(np.log10(lamda/((4*np.pi*tr[i])**2.8)))\n",
        "  SNR_var[i] = Beta[i] + noise_factor + rand3[i]\n",
        "for i in SNR_var.values():\n",
        "  tr_noise_SNR.append(i)\n",
        "\n",
        "tr_noise_SNR = np.transpose(np.tile(tr_noise_SNR, (2,4,1)))\n",
        "#print(s)\n",
        "\n",
        "#print(ad_noise_SNR[7])\n",
        "#print(np.tile(ad_noise_SNR[7], (1000, 1, 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hs9Rtd01f7hG"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AE(object):\n",
        "    def __init__(self, k, n, seed=None, filename=None):\n",
        "        assert (n%2 == 0), \"Channel only allows complex symbols -> n must be a multiple of 2\"\n",
        "        self.k = k \n",
        "        self.n = n\n",
        "        self.n_complex = int(self.n/2)\n",
        "        self.bits_per_symbol = self.k/self.n_complex\n",
        "        self.M = 2**self.k\n",
        "        self.seed = seed if (seed is not None) else int(time.time())           \n",
        "        self.graph = None\n",
        "        self.sess = None   \n",
        "        self.vars = None\n",
        "        self.saver = None   \n",
        "        self.constellations = None\n",
        "        self.blers = None\n",
        "        self.create_graph()\n",
        "        self.create_session()\n",
        "        if filename is not None:    \n",
        "            self.load(filename)       \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def create_graph(self):\n",
        "        '''This function creates the computation graph of the autoencoder'''\n",
        "        self.graph = tf.Graph()        \n",
        "        with self.graph.as_default():    \n",
        "            tf.set_random_seed(self.seed)\n",
        "            batch_size = tf.placeholder(tf.int32, shape=())\n",
        "            \n",
        "            # Transmitter\n",
        "            s = (tf.random_uniform(shape=[batch_size], minval=0, maxval=self.M, dtype=tf.int64))\n",
        "\n",
        "            ad_noise_std = tf.random_uniform(shape=[batch_size], minval=0, maxval=M, dtype=tf.int64)\n",
        "            ones = tf.ones([2,4,1],dtype=tf.int64)\n",
        "            ad_noise_std = tf.transpose(ad_noise_std*ones)\n",
        "            #ad_noise_std = tf.transpose(tf.tile(ad_noise_std, [2,2,1]))           \n",
        "            x = self.encoder(s)                         \n",
        "            \n",
        "            # Channel\n",
        "            #noise_std = tf.placeholder(tf.float32, shape=())\n",
        "            noise_std = tf.placeholder(tf.float32, shape=[200000,4,2])\n",
        "            noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=noise_std) \n",
        "\n",
        "            fade = tf.random.normal(shape=tf.shape(x))\n",
        "            sparr1,sparr2 = tf.split(fade,num_or_size_splits=2, axis=2)\n",
        "            complex_fade = tf.complex(sparr1, sparr2)\n",
        "            fade = tf.abs(complex_fade)\n",
        "            #fade = tf.math.sqrt(1/2)*fade\n",
        "\n",
        "            fade = 1\n",
        "            y = tf.multiply(x,fade) + noise\n",
        "            #y = x + noise\n",
        "            #fade = 1            \n",
        "           \n",
        "            # Receiver\n",
        "            s_hat = self.decoder(y)\n",
        "            correct_s_hat = tf.argmax(tf.nn.softmax(s_hat), axis=1)\n",
        "\n",
        "            \n",
        "            # Loss function\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat, weights= (s+1)**(0))\n",
        "\n",
        "            #Defining the i_matrix\n",
        "            limit = 256\n",
        "            i = tf.range(limit)\n",
        "            multiply = tf.constant([200000])\n",
        "            i_matrix = tf.reshape(tf.tile(i, multiply), [ multiply[0], tf.shape(i)[0]])\n",
        "            print(i_matrix)\n",
        "            #e_si = (tf.cast(s[:, tf.newaxis], tf.float32)) - (tf.cast(i_matrix,tf.float32))\n",
        "            #e_si = tf.math.square(tf.math.divide((tf.cast(s, tf.float32)) - tf.transpose(tf.cast(i_matrix,tf.float32)), tf.transpose(tf.cast(i_matrix,tf.float32))))\n",
        "            e_si = tf.math.square((tf.cast(s, tf.float32)) - tf.transpose(tf.cast(i_matrix,tf.float32)))\n",
        "            e_si = tf.transpose(e_si)\n",
        "            b = tf.nn.softmax(s_hat)\n",
        "\n",
        "            cost_func = tf.math.multiply(b,e_si)\n",
        "\n",
        "            sum_cf = tf.math.multiply( tf.math.sqrt(tf.reduce_sum(cost_func, axis = 1) + 0.000001 ), (tf.cast(1/(s+ 1), tf.float32)))            \n",
        "            #cross_entropy_0 = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat, weights= (s+1)**(4))\n",
        "      \n",
        "           # Performance metrics\n",
        "            #correct_predictions_0 = tf.equal(tf.argmax(tf.nn.softmax([s_hat[x] for x in s_ind_0]), axis=1), [s[x] for x in s_ind_0])\n",
        "\n",
        "            correct_predictions = tf.equal(tf.argmax(tf.nn.softmax(s_hat), axis=1), s)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "            bler = 1-accuracy\n",
        "\n",
        "            lr = tf.placeholder(tf.float32, shape=()) # We can feed in any desired learning rate for each step     \n",
        "            train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
        "            #train_op_0 = tf.train.AdamOptimizer(lr).minimize(cross_entropy_0)\n",
        "            #lr = tf.train.exponential_decay(1e-10, global_step=cross_entropy, decay_steps=100, decay_rate=1.30)\n",
        "\n",
        "            # References to graph variables we need to access later \n",
        "            self.vars = {\n",
        "                'accuracy': accuracy,\n",
        "                'batch_size': batch_size,\n",
        "                'bler': bler,\n",
        "                'cross_entropy': cross_entropy,\n",
        "                'init': tf.global_variables_initializer(),\n",
        "                'lr': lr,\n",
        "                'noise_std': noise_std,\n",
        "                'train_op': train_op,\n",
        "                #'s': (np.floor(np.random.uniform(0,256, 1000))).astype('int64'),\n",
        "                's': s,\n",
        "                's_hat': s_hat,\n",
        "                'correct_s_hat': correct_s_hat,\n",
        "                'x': x,\n",
        "            }            \n",
        "            self.saver = tf.train.Saver()\n",
        "        return\n",
        "    \n",
        "    def create_session(self):\n",
        "        '''Create a session for the autoencoder instance with the compuational graph'''\n",
        "        self.sess = tf.Session(graph=self.graph)        \n",
        "        self.sess.run(self.vars['init'])\n",
        "        return\n",
        "    \n",
        "    def encoder(self, input):\n",
        "        '''The transmitter'''\n",
        "        self.weight_var_rec = self.weight_variable((self.M,self.M)) # shape = (8,8)\n",
        "        self.embedding_lookup_rec = tf.nn.embedding_lookup(self.weight_var_rec, input)\n",
        "        print(self.embedding_lookup_rec)\n",
        "        x = tf.nn.elu(self.embedding_lookup_rec)\n",
        "        #x = tf.layers.dense(self.embedding_lookup_rec, self.M, activation=tf.nn.relu)\n",
        "        x = tf.layers.dense(x, self.M, activation=None)\n",
        "        x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        x = tf.reshape(x, shape=[-1,self.n_complex,2])\n",
        "        print(x);\n",
        "        #Average power normalization\n",
        "        x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x))) \n",
        "        return x\n",
        "    \n",
        "    def decoder(self, input):\n",
        "        '''The Receiver'''\n",
        "        #input = self.flip_decoder(input)\n",
        "        y = tf.reshape(input, shape=[-1,self.n])\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        print(y);\n",
        "        return y\n",
        "    \n",
        "    def EbNo2Sigma(self, ebnodb):\n",
        "        '''Convert Eb/No in dB to noise standard deviation'''\n",
        "        ebno = 10**(ebnodb/10)\n",
        "        return 1/np.sqrt(2*self.bits_per_symbol*ebno)\n",
        "    \n",
        "    def gen_feed_dict(self, batch_size, ebnodb, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['lr']: lr,\n",
        "        }\n",
        "\n",
        "    def gen_e2e_feed_dict(self, batch_size, ebnodb, s_input, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['s']: s_input,\n",
        "            self.vars['lr']: lr,\n",
        "        }    \n",
        "    \n",
        "    def load(self, filename):\n",
        "        '''Load an pre_trained model'''\n",
        "        return self.saver.restore(self.sess, filename)\n",
        "        \n",
        "    def plot_constellation(self, noise_std, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = self.transmit(range(self.M), noise_std)\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(self.n_complex):\n",
        "            image = plt.figure(figsize=(6,6))\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-maxrange,maxrange)\n",
        "            plt.ylim(-maxrange,maxrange)\n",
        "            for i in range(self.M):       \n",
        "              if (i <=150):\n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\")\n",
        "                #plt.annotate(i, (x[i,k,0],x[i,k,1]))  \n",
        "              if (i > 150):\n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"red\")\n",
        "                #plt.annotate(i, (x[i,k,0],x[i,k,1]))      \n",
        "                #plt.scatter(x[i,k,0],x[i,k,1],c=\"black\",marker='x')  \n",
        "            image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            image.suptitle('%d. complex symbol' % (k+1))\n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "        return x, image\n",
        "    \n",
        "    def save(self, filename):\n",
        "        '''Save the current model'''\n",
        "        return self.saver.save(self.sess, filename)  \n",
        "    \n",
        "    def test_step(self, batch_size, ebnodb):\n",
        "        '''Compute the BLER over a single batch and Eb/No'''\n",
        "        bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n",
        "        return bler\n",
        "    \n",
        "    def transmit(self, s, noise_std):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['x'], feed_dict={self.vars['s']: s, self.vars['noise_std']: noise_std})\n",
        "\n",
        "    def end2end(self, batch_size, ebnodb, input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['correct_s_hat'], feed_dict=self.gen_e2e_feed_dict(batch_size, ebnodb, input_s, lr=0)) \n",
        "        #print(self.sess.run(self.vars['correct_s_hat'], feed_dict={self.vars['s']: input_s}))     \n",
        "\n",
        "    #print(self.sess.run(self.vars['s_hat'], feed_dict={self.vars['s']: s}))\n",
        "    def end2end_bler(self, batch_size, ebnodb, input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['bler'], feed_dict=self.gen_e2e_feed_dict(batch_size, ebnodb, input_s, lr=0)) \n",
        "\n",
        "\n",
        "    def train(self, training_params, validation_params):  \n",
        "        \n",
        "        '''Training and validation loop'''\n",
        "        for index, params in enumerate(training_params):            \n",
        "            s,batch_size, lr, ebnodb, iterations = params            \n",
        "            print('\\nBatch Size: ' + str(batch_size) +\n",
        "                  ', Learning Rate: ' + str(lr) +\n",
        "                  ', EbNodB: ' + str(ebnodb) +\n",
        "                  ', Iterations: ' + str(iterations))\n",
        "            \n",
        "            val_size, val_ebnodb, val_steps = validation_params[index]\n",
        "            \n",
        "            for i in range(iterations):\n",
        "                self.train_step(s,batch_size, ebnodb, lr)    \n",
        "                if (i%val_steps==0):\n",
        "                    #bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_new_feed_dict(val_size, val_ebnodb, lr, s_input))\n",
        "                    bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size, val_ebnodb, lr))\n",
        "                    print(bler)                           \n",
        "        return       \n",
        "    \n",
        "    def train_step(self,s, batch_size, ebnodb, lr):\n",
        "        '''A single training step'''\n",
        "        #self.sess.run(self.vars['train_op'], feed_dict=self.gen_new_feed_dict(batch_size, ebnodb, lr, s_input))\n",
        "        self.sess.run(self.vars['train_op'], feed_dict={self.vars['s']: s, self.vars['batch_size']: batch_size,  self.vars['noise_std']: self.EbNo2Sigma(ebnodb), self.vars['lr']: lr})\n",
        "        return \n",
        "    \n",
        "    def weight_variable(self, shape):\n",
        "        '''Xavier-initialized weights optimized for ReLU Activations'''\n",
        "        (fan_in, fan_out) = shape\n",
        "        low = np.sqrt(6.0/(fan_in + fan_out)) \n",
        "        high = -np.sqrt(6.0/(fan_in + fan_out))\n",
        "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n",
        "    \n",
        "    def bler_sim(self, ebnodbs, batch_size, iterations):\n",
        "        '''Monte Carlo simulations of BLER for a range of Eb/No\n",
        "           Sometimes we to compute statistics for batch sizes that do not fit into the GPUs memory.\n",
        "           You can average over multiple batches with small size instead.           \n",
        "        '''\n",
        "        BLER = np.zeros_like(ebnodbs)\n",
        "        for i in range(iterations):\n",
        "            bler = np.array([self.sess.run(self.vars['bler'],\n",
        "                            feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0)) for ebnodb in ebnodbs])\n",
        "            BLER = BLER + bler/iterations\n",
        "        return BLER\n",
        "\n",
        "\n",
        "\n",
        "    def bler_sim1(self,s,  ebnodbs, batch_size, iterations):\n",
        "        '''Monte Carlo simulations of BLER for a range of Eb/No\n",
        "           Sometimes we to compute statistics for batch sizes that do not fit into the GPUs memory.\n",
        "           You can average over multiple batches with small size instead.           \n",
        "        '''\n",
        "        BLER = np.zeros_like(ebnodbs)\n",
        "        for i in range(iterations):\n",
        "          #for ebnodb in ebnodbs:\n",
        "            #print(s, batch_size, ebnodb) \n",
        "          bler = np.array([self.sess.run(self.vars['bler'],\n",
        "                            feed_dict=self.gen_e2e_feed_dict(batch_size, np.transpose(np.tile(np.ones(1000)*ebnodb, (2,2,1))), s, lr=0)) for ebnodb in ebnodbs])\n",
        "                                        \n",
        "          BLER = BLER + bler/iterations\n",
        "        return BLER   \n",
        "    \n",
        "    def plot_bler(self, EbNodB, BLER):\n",
        "        '''Plot a BLER curve'''\n",
        "        image = plt.figure(figsize=(10,8))\n",
        "        plt.plot(EbNodB, BLER, '-r', linewidth=2.0)\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('EbNo (dB)', fontsize=18)\n",
        "        plt.ylabel('Block-error rate', fontsize=18)\n",
        "        plt.grid(True)\n",
        "        plt.ylim([1e-5,1])\n",
        "        return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bANUdLIsf7hM"
      },
      "source": [
        "## Training parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeHghWVRf7hO",
        "outputId": "7f060938-8bd2-4123-f680-c5bb2e0b9f56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor(\"embedding_lookup/Identity:0\", shape=(?, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(?, 4, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(200000, 256), dtype=float32)\n",
            "Tensor(\"Reshape_2:0\", shape=(200000, 256), dtype=int32)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/neelabhro/.local/lib/python3.8/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
            "/home/neelabhro/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch Size: 200000, Learning Rate: 0.01, EbNodB: [[[50.26966531 50.26966531]\n",
            "  [50.26966531 50.26966531]\n",
            "  [50.26966531 50.26966531]\n",
            "  [50.26966531 50.26966531]]\n",
            "\n",
            " [[14.38417412 14.38417412]\n",
            "  [14.38417412 14.38417412]\n",
            "  [14.38417412 14.38417412]\n",
            "  [14.38417412 14.38417412]]\n",
            "\n",
            " [[ 7.64734018  7.64734018]\n",
            "  [ 7.64734018  7.64734018]\n",
            "  [ 7.64734018  7.64734018]\n",
            "  [ 7.64734018  7.64734018]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[10.9987172  10.9987172 ]\n",
            "  [10.9987172  10.9987172 ]\n",
            "  [10.9987172  10.9987172 ]\n",
            "  [10.9987172  10.9987172 ]]\n",
            "\n",
            " [[15.48459774 15.48459774]\n",
            "  [15.48459774 15.48459774]\n",
            "  [15.48459774 15.48459774]\n",
            "  [15.48459774 15.48459774]]\n",
            "\n",
            " [[16.34535622 16.34535622]\n",
            "  [16.34535622 16.34535622]\n",
            "  [16.34535622 16.34535622]\n",
            "  [16.34535622 16.34535622]]], Iterations: 10000\n",
            "0.89868\n",
            "0.073994994\n",
            "0.07406503\n",
            "0.07345998\n",
            "0.073440015\n",
            "0.073454976\n",
            "0.07209003\n",
            "0.07172501\n",
            "0.07205498\n",
            "0.07301003\n",
            "Tensor(\"embedding_lookup/Identity:0\", shape=(?, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(?, 4, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(200000, 256), dtype=float32)\n",
            "Tensor(\"Reshape_2:0\", shape=(200000, 256), dtype=int32)\n",
            "INFO:tensorflow:Restoring parameters from models/ae_k_8_n_8\n"
          ]
        }
      ],
      "source": [
        "train_EbNodB = ad_noise_SNR\n",
        "val_EbNodB = train_EbNodB\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "epoch = [10000]\n",
        "\n",
        "training_params = [\n",
        "    #batch_size, lr, ebnodb, iterations\n",
        "    [s, batch_size , 0.01, train_EbNodB, 10000]\n",
        "#    [10000 , 0.001, train_EbNodB, 10000]    \n",
        "    ]\n",
        "\n",
        "validation_params = [\n",
        "    #batch_size, ebnodb, val_steps \n",
        "    [batch_size, val_EbNodB, 1000],\n",
        "    [batch_size, val_EbNodB, 1000],\n",
        "    [batch_size, val_EbNodB, 1000]\n",
        "    ]\n",
        "\n",
        "model_file_uw = 'models/ae_k_{}_n_{}'.format(k,n)\n",
        "ae = AE(k,n,seed)\n",
        "ae.train(training_params, validation_params)\n",
        "ae.save(model_file_uw);\n",
        "ae = AE(k,n,seed, filename=model_file_uw)\n",
        "  #ae.plot_constellation();\n",
        "  #ae.end2end(tr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKoeXIMOcpbJ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl3hENsvf7hW"
      },
      "source": [
        "## Create and train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzqpcwGaf7hm"
      },
      "source": [
        "## Evaluate trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y33vV4xKf7hn",
        "outputId": "c4fb70d1-9843-4706-9c98-4bc50705279f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor(\"embedding_lookup/Identity:0\", shape=(?, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(?, 4, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(200000, 256), dtype=float32)\n",
            "Tensor(\"Reshape_2:0\", shape=(200000, 256), dtype=int32)\n",
            "INFO:tensorflow:Restoring parameters from models/ae_k_8_n_8\n"
          ]
        }
      ],
      "source": [
        "ae_uw = AE(k,n,seed, filename=model_file_uw) #Load a pretrained model that you have saved if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fasS-Rz1lapW"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AE_Weighted(object):\n",
        "    def __init__(self, k, n, seed=None, filename=None):\n",
        "        assert (n%2 == 0), \"Channel only allows complex symbols -> n must be a multiple of 2\"\n",
        "        self.k = k \n",
        "        self.n = n\n",
        "        self.n_complex = int(self.n/2)\n",
        "        self.bits_per_symbol = self.k/self.n_complex\n",
        "        self.M = 2**self.k\n",
        "        self.seed = seed if (seed is not None) else int(time.time())           \n",
        "        self.graph = None\n",
        "        self.sess = None   \n",
        "        self.vars = None\n",
        "        self.saver = None   \n",
        "        self.constellations = None\n",
        "        self.blers = None\n",
        "        self.create_graph()\n",
        "        self.create_session()\n",
        "        if filename is not None:    \n",
        "            self.load(filename)       \n",
        "        return\n",
        "\n",
        "    \n",
        "    def create_graph(self):\n",
        "        '''This function creates the computation graph of the autoencoder'''\n",
        "        self.graph = tf.Graph()        \n",
        "        with self.graph.as_default():    \n",
        "            tf.set_random_seed(self.seed)\n",
        "            batch_size = tf.placeholder(tf.int32, shape=())\n",
        "            \n",
        "            # Transmitter\n",
        "            s = (tf.random_uniform(shape=[batch_size], minval=0, maxval=self.M, dtype=tf.int64))\n",
        "\n",
        "            ad_noise_std = tf.random_uniform(shape=[batch_size], minval=0, maxval=M, dtype=tf.int64)\n",
        "            ones = tf.ones([2,4,1],dtype=tf.int64)\n",
        "            ad_noise_std = tf.transpose(ad_noise_std*ones)\n",
        "            x = self.encoder(s) \n",
        "\n",
        "                         \n",
        "            \n",
        "            # Channel\n",
        "            #noise_std = tf.placeholder(tf.float32, shape=())\n",
        "            noise_std = tf.placeholder(tf.float32, shape=[200000,4,2])\n",
        "            noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=noise_std) \n",
        "\n",
        "            fade = tf.random.normal(shape=tf.shape(x))\n",
        "            sparr1,sparr2 = tf.split(fade,num_or_size_splits=2, axis=2)\n",
        "            complex_fade = tf.complex(sparr1, sparr2)\n",
        "            fade = tf.abs(complex_fade)\n",
        "            #fade = tf.math.sqrt(1/2)*fade\n",
        "\n",
        "            fade = 1\n",
        "            y = tf.multiply(x,fade) + noise\n",
        "            #y = x + noise\n",
        "            #fade = 1            \n",
        "           \n",
        "            # Receiver\n",
        "            s_hat = self.decoder(y)\n",
        "            correct_s_hat = tf.argmax(tf.nn.softmax(s_hat), axis=1)\n",
        "\n",
        "            \n",
        "           # Loss function\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat, weights= (s+1)**(1))\n",
        "\n",
        "            #Defining the i_matrix\n",
        "            limit = 256\n",
        "            i = tf.range(limit)\n",
        "            multiply = tf.constant([200000])\n",
        "            i_matrix = tf.reshape(tf.tile(i, multiply), [ multiply[0], tf.shape(i)[0]])\n",
        "            print(i_matrix)\n",
        "            #e_si = (tf.cast(s[:, tf.newaxis], tf.float32)) - (tf.cast(i_matrix,tf.float32))\n",
        "            #e_si = tf.math.square(tf.math.divide((tf.cast(s, tf.float32)) - tf.transpose(tf.cast(i_matrix,tf.float32)), tf.transpose(tf.cast(i_matrix,tf.float32))))\n",
        "            e_si = tf.math.square((tf.cast(s, tf.float32)) - tf.transpose(tf.cast(i_matrix,tf.float32)))\n",
        "            e_si = tf.transpose(e_si)\n",
        "            b = tf.nn.softmax(s_hat)\n",
        "\n",
        "            cost_func = tf.math.multiply(b,e_si)\n",
        "\n",
        "            sum_cf = tf.math.multiply( tf.math.sqrt(tf.reduce_sum(cost_func, axis = 1) + 0.000001 ), (tf.cast(1/(s+ 1), tf.float32)))            \n",
        "            #cross_entropy_0 = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat, weights= (s+1)**(4))\n",
        "      \n",
        "           # Performance metrics\n",
        "            #correct_predictions_0 = tf.equal(tf.argmax(tf.nn.softmax([s_hat[x] for x in s_ind_0]), axis=1), [s[x] for x in s_ind_0])\n",
        "\n",
        "            correct_predictions = tf.equal(tf.argmax(tf.nn.softmax(s_hat), axis=1), s)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "            bler = 1-accuracy\n",
        "\n",
        "            lr = tf.placeholder(tf.float32, shape=()) # We can feed in any desired learning rate for each step     \n",
        "            train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy + sum_cf)\n",
        "            # References to graph variables we need to access later \n",
        "            self.vars = {\n",
        "                'accuracy': accuracy,\n",
        "                'batch_size': batch_size,\n",
        "                'bler': bler,\n",
        "                'cross_entropy': cross_entropy,\n",
        "                'init': tf.global_variables_initializer(),\n",
        "                'lr': lr,\n",
        "                'noise_std': noise_std,\n",
        "                'train_op': train_op,\n",
        "                's': s,\n",
        "                's_hat': s_hat,\n",
        "                'correct_s_hat': correct_s_hat,\n",
        "                'x': x,\n",
        "            }            \n",
        "            self.saver = tf.train.Saver()\n",
        "        return\n",
        "    \n",
        "    \n",
        "    def create_session(self):\n",
        "        '''Create a session for the autoencoder instance with the compuational graph'''\n",
        "        self.sess = tf.Session(graph=self.graph)        \n",
        "        self.sess.run(self.vars['init'])\n",
        "        return\n",
        "    \n",
        "    def encoder(self, input):\n",
        "        '''The transmitter'''\n",
        "        self.weight_var_rec = self.weight_variable((self.M,self.M)) # shape = (8,8)\n",
        "        self.embedding_lookup_rec = tf.nn.embedding_lookup(self.weight_var_rec, input)\n",
        "        print(self.embedding_lookup_rec)\n",
        "        x = tf.nn.elu(self.embedding_lookup_rec)\n",
        "        #x = tf.layers.dense(self.embedding_lookup_rec, self.M, activation=tf.nn.relu)\n",
        "        x = tf.layers.dense(x, self.M, activation=None)\n",
        "        x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        x = tf.reshape(x, shape=[-1,self.n_complex,2])\n",
        "        print(x);\n",
        "        #Average power normalization\n",
        "        x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x))) \n",
        "        return x\n",
        "    \n",
        "    def decoder(self, input):\n",
        "        '''The Receiver'''\n",
        "        #input = self.flip_decoder(input)\n",
        "        y = tf.reshape(input, shape=[-1,self.n])\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        print(y);\n",
        "        return y\n",
        "    \n",
        "    def EbNo2Sigma(self, ebnodb):\n",
        "        '''Convert Eb/No in dB to noise standard deviation'''\n",
        "        ebno = 10**(ebnodb/10)\n",
        "        return 1/np.sqrt(2*self.bits_per_symbol*ebno)\n",
        "    \n",
        "    def gen_feed_dict(self, batch_size, ebnodb, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['lr']: lr,\n",
        "        }\n",
        "\n",
        "    def gen_e2e_feed_dict(self, batch_size, ebnodb, s_input, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['s']: s_input,\n",
        "            self.vars['lr']: lr,\n",
        "        }    \n",
        "    \n",
        "    def load(self, filename):\n",
        "        '''Load an pre_trained model'''\n",
        "        return self.saver.restore(self.sess, filename)\n",
        "        \n",
        "    def plot_constellation(self,noise_std, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = self.transmit(range(self.M), noise_std)\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(self.n_complex):\n",
        "            image = plt.figure(figsize=(6,6))\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-maxrange,maxrange)\n",
        "            plt.ylim(-maxrange,maxrange)\n",
        "            for i in range(self.M):       \n",
        "              if (i <=150):\n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\")\n",
        "                #plt.annotate(i, (x[i,k,0],x[i,k,1]))  \n",
        "              if (i > 150):\n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"red\")\n",
        "                #plt.annotate(i, (x[i,k,0],x[i,k,1]))      \n",
        "                #plt.scatter(x[i,k,0],x[i,k,1],c=\"black\",marker='x')   \n",
        "            image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            image.suptitle('%d. complex symbol' % (k+1))\n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "        return x, image\n",
        "    \n",
        "    def save(self, filename):\n",
        "        '''Save the current model'''\n",
        "        return self.saver.save(self.sess, filename)  \n",
        "    \n",
        "    def test_step(self, batch_size, ebnodb):\n",
        "        '''Compute the BLER over a single batch and Eb/No'''\n",
        "        bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n",
        "        return bler\n",
        "    \n",
        "    def transmit(self, s, noise_std):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['x'], feed_dict={self.vars['s']: s, self.vars['noise_std']: noise_std})\n",
        "\n",
        "    def end2end(self, batch_size, ebnodb, input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['correct_s_hat'], feed_dict=self.gen_e2e_feed_dict(batch_size, ebnodb, input_s, lr=0)) \n",
        "        #print(self.sess.run(self.vars['correct_s_hat'], feed_dict={self.vars['s']: input_s}))     \n",
        "\n",
        "    def end2end_bler(self, batch_size, ebnodb, input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['bler'], feed_dict=self.gen_e2e_feed_dict(batch_size, ebnodb, input_s, lr=0)) \n",
        "\n",
        "    def train(self, training_params, validation_params):  \n",
        "        \n",
        "        '''Training and validation loop'''\n",
        "        for index, params in enumerate(training_params):            \n",
        "            s,batch_size, lr, ebnodb, iterations = params            \n",
        "            print('\\nBatch Size: ' + str(batch_size) +\n",
        "                  ', Learning Rate: ' + str(lr) +\n",
        "                  ', EbNodB: ' + str(ebnodb) +\n",
        "                  ', Iterations: ' + str(iterations))\n",
        "            \n",
        "            val_size, val_ebnodb, val_steps = validation_params[index]\n",
        "            \n",
        "            for i in range(iterations):\n",
        "                self.train_step(s,batch_size, ebnodb, lr)    \n",
        "                if (i%val_steps==0):\n",
        "                    #bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_new_feed_dict(val_size, val_ebnodb, lr, s_input))\n",
        "                    bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size, val_ebnodb, lr))\n",
        "                    print(bler)                           \n",
        "        return       \n",
        "    \n",
        "    def train_step(self,s, batch_size, ebnodb, lr):\n",
        "        '''A single training step'''\n",
        "        #self.sess.run(self.vars['train_op'], feed_dict=self.gen_new_feed_dict(batch_size, ebnodb, lr, s_input))\n",
        "        self.sess.run(self.vars['train_op'], feed_dict={self.vars['s']: s, self.vars['batch_size']: batch_size,  self.vars['noise_std']: self.EbNo2Sigma(ebnodb), self.vars['lr']: lr})\n",
        "        return \n",
        "    \n",
        "    def weight_variable(self, shape):\n",
        "        '''Xavier-initialized weights optimized for ReLU Activations'''\n",
        "        (fan_in, fan_out) = shape\n",
        "        low = np.sqrt(6.0/(fan_in + fan_out)) \n",
        "        high = -np.sqrt(6.0/(fan_in + fan_out))\n",
        "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n",
        "    \n",
        "    def bler_sim(self, ebnodbs, batch_size, iterations):\n",
        "        '''Monte Carlo simulations of BLER for a range of Eb/No\n",
        "           Sometimes we to compute statistics for batch sizes that do not fit into the GPUs memory.\n",
        "           You can average over multiple batches with small size instead.           \n",
        "        '''\n",
        "        BLER = np.zeros_like(ebnodbs)\n",
        "        for i in range(iterations):\n",
        "            bler = np.array([self.sess.run(self.vars['bler'],\n",
        "                            feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0)) for ebnodb in ebnodbs])\n",
        "            BLER = BLER + bler/iterations\n",
        "        return BLER\n",
        "\n",
        "\n",
        "\n",
        "    def bler_sim1(self,s,  ebnodbs, batch_size, iterations):\n",
        "        '''Monte Carlo simulations of BLER for a range of Eb/No\n",
        "           Sometimes we to compute statistics for batch sizes that do not fit into the GPUs memory.\n",
        "           You can average over multiple batches with small size instead.           \n",
        "        '''\n",
        "        BLER = np.zeros_like(ebnodbs)\n",
        "        for i in range(iterations):\n",
        "          #for ebnodb in ebnodbs:\n",
        "            #print(s, batch_size, ebnodb) \n",
        "          bler = np.array([self.sess.run(self.vars['bler'],\n",
        "                            feed_dict=self.gen_e2e_feed_dict(batch_size, np.transpose(np.tile(np.ones(1000)*ebnodb, (2,2,1))), s, lr=0)) for ebnodb in ebnodbs])\n",
        "                                        \n",
        "          BLER = BLER + bler/iterations\n",
        "        return BLER        \n",
        "    \n",
        "    def plot_bler(self, EbNodB, BLER):\n",
        "        '''Plot a BLER curve'''\n",
        "        image = plt.figure(figsize=(10,8))\n",
        "        plt.plot(EbNodB, BLER, '-r', linewidth=2.0)\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('EbNo (dB)', fontsize=18)\n",
        "        plt.ylabel('Block-error rate', fontsize=18)\n",
        "        plt.grid(True)\n",
        "        plt.ylim([1e-5,1])\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjh_J3MIlyrF",
        "outputId": "eea91292-725c-4351-bf2e-1337c0d3b111"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor(\"embedding_lookup/Identity:0\", shape=(?, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(?, 4, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(200000, 256), dtype=float32)\n",
            "Tensor(\"Reshape_2:0\", shape=(200000, 256), dtype=int32)\n",
            "\n",
            "Batch Size: 200000, Learning Rate: 0.01, EbNodB: [[[50.26966531 50.26966531]\n",
            "  [50.26966531 50.26966531]\n",
            "  [50.26966531 50.26966531]\n",
            "  [50.26966531 50.26966531]]\n",
            "\n",
            " [[14.38417412 14.38417412]\n",
            "  [14.38417412 14.38417412]\n",
            "  [14.38417412 14.38417412]\n",
            "  [14.38417412 14.38417412]]\n",
            "\n",
            " [[ 7.64734018  7.64734018]\n",
            "  [ 7.64734018  7.64734018]\n",
            "  [ 7.64734018  7.64734018]\n",
            "  [ 7.64734018  7.64734018]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[10.9987172  10.9987172 ]\n",
            "  [10.9987172  10.9987172 ]\n",
            "  [10.9987172  10.9987172 ]\n",
            "  [10.9987172  10.9987172 ]]\n",
            "\n",
            " [[15.48459774 15.48459774]\n",
            "  [15.48459774 15.48459774]\n",
            "  [15.48459774 15.48459774]\n",
            "  [15.48459774 15.48459774]]\n",
            "\n",
            " [[16.34535622 16.34535622]\n",
            "  [16.34535622 16.34535622]\n",
            "  [16.34535622 16.34535622]\n",
            "  [16.34535622 16.34535622]]], Iterations: 10000\n",
            "0.94162\n",
            "0.081274986\n",
            "0.082840025\n",
            "0.08284497\n",
            "0.08305001\n",
            "0.081120014\n",
            "0.07960498\n",
            "0.08069003\n",
            "0.081285\n",
            "0.08213502\n",
            "Tensor(\"embedding_lookup/Identity:0\", shape=(?, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(?, 4, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(200000, 256), dtype=float32)\n",
            "Tensor(\"Reshape_2:0\", shape=(200000, 256), dtype=int32)\n",
            "INFO:tensorflow:Restoring parameters from models/ae_k_8_n_8\n"
          ]
        }
      ],
      "source": [
        "train_EbNodB = ad_noise_SNR\n",
        "val_EbNodB = train_EbNodB\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "epoch = [10000]\n",
        "\n",
        "training_params = [\n",
        "    #batch_size, lr, ebnodb, iterations\n",
        "    [s, batch_size , 0.01, train_EbNodB, 10000]\n",
        "#    [10000 , 0.001, train_EbNodB, 10000]    \n",
        "    ]\n",
        "\n",
        "validation_params = [\n",
        "    #batch_size, ebnodb, val_steps \n",
        "    [batch_size, val_EbNodB, 1000],\n",
        "    [batch_size, val_EbNodB, 1000],\n",
        "    [batch_size, val_EbNodB, 1000]\n",
        "    ]\n",
        "\n",
        "model_file = 'models/ae_k_{}_n_{}'.format(k,n)\n",
        "ae_Weighted = AE_Weighted(k,n,seed)\n",
        "ae_Weighted.train(training_params, validation_params)\n",
        "ae_Weighted.save(model_file);\n",
        "ae_Weighted = AE_Weighted(k,n,seed, filename=model_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "v3Yddt94DMAO"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AE_Tb(object):\n",
        "    def __init__(self, k, n, seed=None, filename=None):\n",
        "        assert (n%2 == 0), \"Channel only allows complex symbols -> n must be a multiple of 2\"\n",
        "        self.k = k \n",
        "        self.n = n\n",
        "        self.n_complex = int(self.n/2)\n",
        "        self.bits_per_symbol = self.k/self.n_complex\n",
        "        self.M = 2**self.k\n",
        "        self.seed = seed if (seed is not None) else int(time.time())           \n",
        "        self.graph = None\n",
        "        self.sess = None   \n",
        "        self.vars = None\n",
        "        self.saver = None   \n",
        "        self.constellations = None\n",
        "        self.blers = None\n",
        "        self.create_graph()\n",
        "        self.create_session()\n",
        "        if filename is not None:    \n",
        "            self.load(filename)       \n",
        "        return\n",
        "    \n",
        "    \n",
        "    def create_graph(self):\n",
        "        '''This function creates the computation graph of the autoencoder'''\n",
        "        self.graph = tf.Graph()        \n",
        "        with self.graph.as_default():    \n",
        "            tf.set_random_seed(self.seed)\n",
        "            batch_size = tf.placeholder(tf.int32, shape=())\n",
        "            \n",
        "            # Transmitter\n",
        "            s = (tf.random_uniform(shape=[batch_size], minval=0, maxval=self.M, dtype=tf.int64))\n",
        "\n",
        "            ad_noise_std = tf.random_uniform(shape=[batch_size], minval=0, maxval=M, dtype=tf.int64)\n",
        "            ones = tf.ones([2,4,1],dtype=tf.int64)\n",
        "            ad_noise_std = tf.transpose(ad_noise_std*ones)\n",
        "            #ad_noise_std = tf.transpose(tf.tile(ad_noise_std, [2,2,1]))           \n",
        "            x = self.encoder(s)                         \n",
        "            \n",
        "            # Channel\n",
        "            #noise_std = tf.placeholder(tf.float32, shape=())\n",
        "            noise_std = tf.placeholder(tf.float32, shape=[200000,4,2])\n",
        "            noise = tf.random_normal(tf.shape(x), mean=0.0, stddev=noise_std) \n",
        "\n",
        "            fade = tf.random.normal(shape=tf.shape(x))\n",
        "            sparr1,sparr2 = tf.split(fade,num_or_size_splits=2, axis=2)\n",
        "            complex_fade = tf.complex(sparr1, sparr2)\n",
        "            fade = tf.abs(complex_fade)\n",
        "            #fade = tf.math.sqrt(1/2)*fade\n",
        "\n",
        "            fade = 1\n",
        "            y = tf.multiply(x,fade) + noise\n",
        "            #y = x + noise\n",
        "       \n",
        "           \n",
        "            # Receiver\n",
        "            s_hat = self.decoder(y)\n",
        "            correct_s_hat = tf.argmax(tf.nn.softmax(s_hat), axis=1)\n",
        "\n",
        "            \n",
        "            # Loss function\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            #cross_entropy_0 = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat)\n",
        "            #cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=s, logits=s_hat, weights= (s+1)**(4))\n",
        "      \n",
        "           # Performance metrics\n",
        "            #correct_predictions_0 = tf.equal(tf.argmax(tf.nn.softmax([s_hat[x] for x in s_ind_0]), axis=1), [s[x] for x in s_ind_0])\n",
        "\n",
        "            correct_predictions = tf.equal(tf.argmax(tf.nn.softmax(s_hat), axis=1), s)\n",
        "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "            bler = 1-accuracy\n",
        "\n",
        "            lr = tf.placeholder(tf.float32, shape=()) # We can feed in any desired learning rate for each step     \n",
        "            train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
        "            #train_op_0 = tf.train.AdamOptimizer(lr).minimize(cross_entropy_0)\n",
        "            #lr = tf.train.exponential_decay(1e-10, global_step=cross_entropy, decay_steps=100, decay_rate=1.30)\n",
        "\n",
        "            # References to graph variables we need to access later \n",
        "            self.vars = {\n",
        "                'accuracy': accuracy,\n",
        "                'batch_size': batch_size,\n",
        "                'bler': bler,\n",
        "                'cross_entropy': cross_entropy,\n",
        "                'init': tf.global_variables_initializer(),\n",
        "                'lr': lr,\n",
        "                'noise_std': noise_std,\n",
        "                'train_op': train_op,\n",
        "                #'s': (np.floor(np.random.uniform(0,256, 1000))).astype('int64'),\n",
        "                's': s,\n",
        "                's_hat': s_hat,\n",
        "                'correct_s_hat': correct_s_hat,\n",
        "                'x': x,\n",
        "            }            \n",
        "            self.saver = tf.train.Saver()\n",
        "        return\n",
        "    \n",
        "    def create_session(self):\n",
        "        '''Create a session for the autoencoder instance with the compuational graph'''\n",
        "        self.sess = tf.Session(graph=self.graph)        \n",
        "        self.sess.run(self.vars['init'])\n",
        "        return\n",
        "    \n",
        "    def encoder(self, input):\n",
        "        '''The transmitter'''\n",
        "        self.weight_var_rec = self.weight_variable((self.M,self.M)) # shape = (8,8)\n",
        "        self.embedding_lookup_rec = tf.nn.embedding_lookup(self.weight_var_rec, input)\n",
        "        print(self.embedding_lookup_rec)\n",
        "        x = tf.nn.elu(self.embedding_lookup_rec)\n",
        "        #x = tf.layers.dense(self.embedding_lookup_rec, self.M, activation=tf.nn.relu)\n",
        "        x = tf.layers.dense(x, self.M, activation=None)\n",
        "        x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        #x = tf.layers.dense(x, self.n, activation=None)\n",
        "        x = tf.reshape(x, shape=[-1,self.n_complex,2])\n",
        "        print(x);\n",
        "        #Average power normalization\n",
        "        x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x))) \n",
        "        return x\n",
        "    \n",
        "    def decoder(self, input):\n",
        "        '''The Receiver'''\n",
        "        #input = self.flip_decoder(input)\n",
        "        y = tf.reshape(input, shape=[-1,self.n])\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        #y = tf.layers.dense(y, self.M, activation=None)\n",
        "        print(y);\n",
        "        return y\n",
        "    \n",
        "    def EbNo2Sigma(self, ebnodb):\n",
        "        '''Convert Eb/No in dB to noise standard deviation'''\n",
        "        ebno = 10**(ebnodb/10)\n",
        "        return 1/np.sqrt(2*self.bits_per_symbol*ebno)\n",
        "    \n",
        "    def gen_feed_dict(self, batch_size, ebnodb, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['lr']: lr,\n",
        "        }\n",
        "\n",
        "    def gen_e2e_feed_dict(self, batch_size, ebnodb, s_input, lr):\n",
        "        '''Generate a feed dictionary for training and validation'''        \n",
        "        return {\n",
        "            self.vars['batch_size']: batch_size,\n",
        "            self.vars['noise_std']: self.EbNo2Sigma(ebnodb),\n",
        "            self.vars['s']: s_input,\n",
        "            self.vars['lr']: lr,\n",
        "        }   \n",
        "    \n",
        "    def load(self, filename):\n",
        "        '''Load an pre_trained model'''\n",
        "        return self.saver.restore(self.sess, filename)\n",
        "        \n",
        "    def plot_constellation(self, noise_std, maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = self.transmit(range(self.M), noise_std)\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(self.n_complex):\n",
        "            image = plt.figure(figsize=(6,6))\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-maxrange,maxrange)\n",
        "            plt.ylim(-maxrange,maxrange)\n",
        "            for i in range(self.M):\n",
        "              if (i <=150):\n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"black\")\n",
        "                #plt.annotate(i, (x[i,k,0],x[i,k,1]))  \n",
        "              if (i > 150):\n",
        "                plt.scatter(x[i,k,0],x[i,k,1],c=\"red\")\n",
        "                #plt.annotate(i, (x[i,k,0],x[i,k,1]))      \n",
        "                #plt.scatter(x[i,k,0],x[i,k,1],c=\"black\",marker='x')   \n",
        "            image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            image.suptitle('%d. complex symbol' % (k+1))\n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "        return x, image\n",
        "    \n",
        "    def save(self, filename):\n",
        "        '''Save the current model'''\n",
        "        return self.saver.save(self.sess, filename)  \n",
        "    \n",
        "    def test_step(self, batch_size, ebnodb):\n",
        "        '''Compute the BLER over a single batch and Eb/No'''\n",
        "        bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0))\n",
        "        return bler\n",
        "    \n",
        "    def transmit(self, s, noise_std):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['x'], feed_dict={self.vars['s']: s, self.vars['noise_std']: noise_std})\n",
        "\n",
        "    def end2end(self, batch_size, ebnodb, input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['correct_s_hat'], feed_dict=self.gen_e2e_feed_dict(batch_size, ebnodb, input_s, lr=0)) \n",
        "        #print(self.sess.run(self.vars['correct_s_hat'], feed_dict={self.vars['s']: input_s}))     \n",
        "\n",
        "    def end2end_bler(self, batch_size, ebnodb, input_s):\n",
        "        '''Returns the transmitted sigals corresponding to message indices'''\n",
        "        return self.sess.run(self.vars['bler'], feed_dict=self.gen_e2e_feed_dict(batch_size, ebnodb, input_s, lr=0))\n",
        "\n",
        "    def train(self, training_params, validation_params):  \n",
        "        \n",
        "        '''Training and validation loop'''\n",
        "        for index, params in enumerate(training_params):            \n",
        "            s,batch_size, lr, ebnodb, iterations = params            \n",
        "            print('\\nBatch Size: ' + str(batch_size) +\n",
        "                  ', Learning Rate: ' + str(lr) +\n",
        "                  ', EbNodB: ' + str(ebnodb) +\n",
        "                  ', Iterations: ' + str(iterations))\n",
        "            \n",
        "            val_size, val_ebnodb, val_steps = validation_params[index]\n",
        "            \n",
        "            for i in range(iterations):\n",
        "                self.train_step(s,batch_size, ebnodb, lr)    \n",
        "                if (i%val_steps==0):\n",
        "                    #bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_new_feed_dict(val_size, val_ebnodb, lr, s_input))\n",
        "                    bler = self.sess.run(self.vars['bler'], feed_dict=self.gen_feed_dict(val_size, val_ebnodb, lr))\n",
        "                    print(bler)                           \n",
        "        return       \n",
        "    \n",
        "    def train_step(self,s, batch_size, ebnodb, lr):\n",
        "        '''A single training step'''\n",
        "        #self.sess.run(self.vars['train_op'], feed_dict=self.gen_new_feed_dict(batch_size, ebnodb, lr, s_input))\n",
        "        self.sess.run(self.vars['train_op'], feed_dict={self.vars['s']: s, self.vars['batch_size']: batch_size,  self.vars['noise_std']: self.EbNo2Sigma(ebnodb), self.vars['lr']: lr})\n",
        "        return \n",
        "    \n",
        "    def weight_variable(self, shape):\n",
        "        '''Xavier-initialized weights optimized for ReLU Activations'''\n",
        "        (fan_in, fan_out) = shape\n",
        "        low = np.sqrt(6.0/(fan_in + fan_out)) \n",
        "        high = -np.sqrt(6.0/(fan_in + fan_out))\n",
        "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))\n",
        "    \n",
        "    def bler_sim(self, ebnodbs, batch_size, iterations):\n",
        "        '''Monte Carlo simulations of BLER for a range of Eb/No\n",
        "           Sometimes we to compute statistics for batch sizes that do not fit into the GPUs memory.\n",
        "           You can average over multiple batches with small size instead.           \n",
        "        '''\n",
        "        BLER = np.zeros_like(ebnodbs)\n",
        "        for i in range(iterations):\n",
        "            bler = np.array([self.sess.run(self.vars['bler'],\n",
        "                            feed_dict=self.gen_feed_dict(batch_size, ebnodb, lr=0)) for ebnodb in ebnodbs])\n",
        "            BLER = BLER + bler/iterations\n",
        "        return BLER\n",
        "\n",
        "    def bler_sim1(self,s,  ebnodbs, batch_size, iterations):\n",
        "        '''Monte Carlo simulations of BLER for a range of Eb/No\n",
        "           Sometimes we to compute statistics for batch sizes that do not fit into the GPUs memory.\n",
        "           You can average over multiple batches with small size instead.           \n",
        "        '''\n",
        "        BLER = np.zeros_like(ebnodbs)\n",
        "        for i in range(iterations):\n",
        "          #for ebnodb in ebnodbs:\n",
        "            #print(s, batch_size, ebnodb) \n",
        "          bler = np.array([self.sess.run(self.vars['bler'],\n",
        "                            feed_dict=self.gen_e2e_feed_dict(batch_size, np.transpose(np.tile(np.ones(1000)*ebnodb, (2,2,1))), s, lr=0)) for ebnodb in ebnodbs])\n",
        "                                        \n",
        "          BLER = BLER + bler/iterations\n",
        "        return BLER   \n",
        "\n",
        "\n",
        "    def plot_bler(self, EbNodB, BLER):\n",
        "        '''Plot a BLER curve'''\n",
        "        image = plt.figure(figsize=(10,8))\n",
        "        plt.plot(EbNodB, BLER, '-r', linewidth=2.0)\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('EbNo (dB)', fontsize=18)\n",
        "        plt.ylabel('Block-error rate', fontsize=18)\n",
        "        plt.grid(True)\n",
        "        plt.ylim([1e-5,1])\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqkN-OmfD1GW",
        "outputId": "a6649422-0392-4997-ed5e-15ef668e4118"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor(\"embedding_lookup/Identity:0\", shape=(?, 256), dtype=float32)\n",
            "Tensor(\"Reshape:0\", shape=(?, 4, 2), dtype=float32)\n",
            "Tensor(\"dense_4/BiasAdd:0\", shape=(200000, 256), dtype=float32)\n",
            "\n",
            "Batch Size: 200000, Learning Rate: 0.01, EbNodB: [[[ 6.58104672  6.58104672]\n",
            "  [ 6.58104672  6.58104672]\n",
            "  [ 6.58104672  6.58104672]\n",
            "  [ 6.58104672  6.58104672]]\n",
            "\n",
            " [[ 8.29166908  8.29166908]\n",
            "  [ 8.29166908  8.29166908]\n",
            "  [ 8.29166908  8.29166908]\n",
            "  [ 8.29166908  8.29166908]]\n",
            "\n",
            " [[ 7.11233156  7.11233156]\n",
            "  [ 7.11233156  7.11233156]\n",
            "  [ 7.11233156  7.11233156]\n",
            "  [ 7.11233156  7.11233156]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 6.09230861  6.09230861]\n",
            "  [ 6.09230861  6.09230861]\n",
            "  [ 6.09230861  6.09230861]\n",
            "  [ 6.09230861  6.09230861]]\n",
            "\n",
            " [[10.60364056 10.60364056]\n",
            "  [10.60364056 10.60364056]\n",
            "  [10.60364056 10.60364056]\n",
            "  [10.60364056 10.60364056]]\n",
            "\n",
            " [[14.9047518  14.9047518 ]\n",
            "  [14.9047518  14.9047518 ]\n",
            "  [14.9047518  14.9047518 ]\n",
            "  [14.9047518  14.9047518 ]]], Iterations: 10000\n",
            "0.907355\n"
          ]
        }
      ],
      "source": [
        "train_EbNodB = seven_noise_SNR\n",
        "val_EbNodB = train_EbNodB\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "epoch = [10000]\n",
        "\n",
        "training_params = [\n",
        "    #batch_size, lr, ebnodb, iterations\n",
        "    [s, batch_size , 0.01, train_EbNodB, 10000]\n",
        "    #[s, 1000 , 0.001, train_EbNodB, 10000]\n",
        "    #[1000 , 0.001, train_EbNodB, 10000]    \n",
        "    ]\n",
        "\n",
        "validation_params = [\n",
        "    #batch_size, ebnodb, val_steps \n",
        "    [batch_size, val_EbNodB, 1000],\n",
        "    [batch_size, val_EbNodB, 1000],\n",
        "    [batch_size, val_EbNodB, 1000]\n",
        "    ]\n",
        "\n",
        "model_file_Tb = 'models/ae_k_{}_n_{}'.format(k,n)\n",
        "ae_Tb = AE_Tb(k,n,seed)\n",
        "ae_Tb.train(training_params, validation_params)\n",
        "ae_Tb.save(model_file_Tb);\n",
        "ae_Tb = AE_Tb(k,n,seed, filename=model_file_Tb)  #ae.plot_constellation();\n",
        "  #ae.end2end(tr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsoF-I7Rf7hy"
      },
      "source": [
        "### Plot of learned constellations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YjGW8S_df7h0",
        "outputId": "4d7f92fb-566c-46ea-adf6-9f48340f8713"
      },
      "outputs": [],
      "source": [
        "import matplotlib.cm as cm\n",
        "import itertools\n",
        "def plot_constellation_2(ae, arr, noise_std,  maxrange=None):\n",
        "        '''Generate a plot of the current constellation'''\n",
        "        x = ae.transmit(range(ae.M), noise_std)\n",
        "        if (maxrange is None):\n",
        "            maxrange = np.max(np.abs(x))\n",
        "        for k in range(ae.n_complex):\n",
        "            image = plt.figure(figsize=(6,6))\n",
        "            plt.grid(True)\n",
        "            plt.xlim(-2,2)\n",
        "            plt.ylim(-2,2)\n",
        "            for i in range(ae.M):\n",
        "              plt.scatter(x[i,k,0],x[i,k,1],color=cm.jet_r(i/255.0))\n",
        "              #if (i <=150):\n",
        "                #plt.scatter(x[i,k,0],x[i,k,1],c=\"black\")\n",
        "                #plt.annotate(i, (x[i,k,0],x[i,k,1]))  \n",
        "              #if (i > 150):\n",
        "                #plt.scatter(x[i,k,0],x[i,k,1],c=\"red\")\n",
        "                #plt.annotate(i, (x[i,k,0],x[i,k,1]))      \n",
        "                #plt.scatter(x[i,k,0],x[i,k,1],c=\"black\",marker='x')   \n",
        "            image.axes[0].set_xticks(np.array([-2,-1,0,1,2]))\n",
        "            image.axes[0].set_yticks(np.array([-2,-1,0,1,2]))\n",
        "            #image.suptitle('%d. complex symbol' % (k+1))\n",
        "            plt.xlabel('Re')\n",
        "            plt.ylabel('Im')\n",
        "        return x, image\n",
        "\n",
        "plot_constellation_2(ae,range(0,ae.M), const_noise_SNR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMbROTHNMa79"
      },
      "source": [
        "**THE RMSE Calculations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "B7-W-Be2auJV",
        "outputId": "294df816-6d74-4c61-da40-fd61e1a18e74"
      },
      "outputs": [],
      "source": [
        "def rmse(predictions,targets):\n",
        "  return (np.sqrt((np.subtract(predictions,targets) ** 2).mean()))   \n",
        "\n",
        "iter = 10\n",
        "rmse_uw = {}\n",
        "rmse_w = {}\n",
        "rmse_Tb = {}\n",
        "sum_uw = {}\n",
        "sum_w = {}\n",
        "sum_Tb = {}\n",
        "sum_uw[0] = 0\n",
        "#for i in range(1,iter):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tr_hat = ae.end2end(len(tr), tr_noise_SNR, tr)\n",
        "tr_hat_w = ae_Weighted.end2end(len(tr), tr_noise_SNR, tr)\n",
        "tr_hat_Tb = ae_Tb.end2end(len(tr), tr_noise_SNR, tr)\n",
        "\n",
        "#print(abs(tr_hat - tr))\n",
        "\n",
        "for j in range(1,M):\n",
        "#    rmse_uw[j,i] = rmse(([tr_hat[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "  rmse_uw[j] = rmse(([tr_hat[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "for j in range(1,M):\n",
        " #   rmse_w[j,i] = rmse(([tr_hat_w[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        " rmse_w[j] = rmse(([tr_hat_w[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "for j in range(1,M):\n",
        "#    rmse_Tb[j,i] = rmse(([tr_hat_Tb[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "  rmse_Tb[j] = rmse(([tr_hat_Tb[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "  #sum_uw[i] = sum_uw[i] + sum_uw[i-1]\n",
        "\n",
        "print(rmse_uw)\n",
        "message_factor = (np.arange(1, M))\n",
        "\n",
        "rmse_uwA = []\n",
        "for i in rmse_uw.values():\n",
        "  rmse_uwA.append(i)\n",
        "\n",
        "rmse_wA = []\n",
        "for i in rmse_w.values():\n",
        "  rmse_wA.append(i)\n",
        "\n",
        "rmse_TbA = []\n",
        "for i in rmse_Tb.values():\n",
        "  rmse_TbA.append(i)\n",
        "\n",
        "#rmse_uw = np.log2(np.divide(rmse_uwA,message_factor) +1)\n",
        "#rmse_uw = (np.divide(rmse_uwA,message_factor) +1)\n",
        "rmse_uw = rmse_uwA\n",
        "\n",
        "#rmse_w = np.log2(np.divide(rmse_wA,message_factor) + 1)\n",
        "#rmse_w = (np.divide(rmse_wA,message_factor) + 1)\n",
        "rmse_w = rmse_wA\n",
        "\n",
        "\n",
        "#rmse_Tb = np.log2(np.divide(rmse_TbA,message_factor) +1)\n",
        "#rmse_Tb = (np.divide(rmse_TbA,message_factor))\n",
        "rmse_Tb = rmse_TbA\n",
        "\n",
        "message = np.arange(1,M)\n",
        "\n",
        "print(rmse_uw)\n",
        "plt.plot(message, rmse_uw, '-o');\n",
        "plt.plot(message, rmse_w, '-*');\n",
        "plt.plot(message, rmse_Tb, '-+');\n",
        "plt.xlabel('Distance',fontsize=15)\n",
        "plt.ylabel('Absolute RMSE',fontsize=15)\n",
        "plt.ylim([0, 100])\n",
        "#plt.ylabel('log2 Norm RMSE deviation')\n",
        "plt.legend(['SPL', 'Weighted SPL','Baseline'],fontsize=15)\n",
        "#plt.legend(['Mixed LF', 'Cross Entropy','TB with CE'])\n",
        "plt.rcParams.update({'font.size': 15})\n",
        "\n",
        "print(np.sum(rmse_uw))\n",
        "print(np.sum(rmse_w))\n",
        "print(np.sum(rmse_Tb))\n",
        "#print(np.sum(rmse_uwA))\n",
        "#print(np.sum(rmse_wA))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "6QIbpnhmnFLO",
        "outputId": "76ee7540-e8fe-4426-ab79-f09e3fd6dd15"
      },
      "outputs": [],
      "source": [
        "def nsj_bler(predictions,targets):\n",
        "  return 1 - (((np.equal(predictions,targets)).mean()))   \n",
        "\n",
        "iter = 10\n",
        "bler_uw = {}\n",
        "bler_w = {}\n",
        "bler_Tb = {}\n",
        "sum_uw = {}\n",
        "sum_w = {}\n",
        "sum_Tb = {}\n",
        "\n",
        "\n",
        "\n",
        "for j in range(1,M):\n",
        "#    rmse_uw[j,i] = rmse(([tr_hat[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "  bler_uw[j] = nsj_bler(([tr_hat[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "for j in range(1,M):\n",
        " #   rmse_w[j,i] = rmse(([tr_hat_w[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "  bler_w[j] = nsj_bler(([tr_hat_w[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "for j in range(1,M):\n",
        "#    rmse_Tb[j,i] = rmse(([tr_hat_Tb[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "  bler_Tb[j] = nsj_bler(([tr_hat_Tb[x] for x in s_ind[j]]), ([tr[x] for x in s_ind[j]]))\n",
        "  #sum_uw[i] = sum_uw[i] + sum_uw[i-1]\n",
        "\n",
        "print(bler_w)\n",
        "message_factor = (np.arange(1, M))\n",
        "\n",
        "bler_uwA = []\n",
        "for i in bler_uw.values():\n",
        "  bler_uwA.append(i)\n",
        "\n",
        "bler_wA = []\n",
        "for i in bler_w.values():\n",
        "  bler_wA.append(i)\n",
        "\n",
        "bler_TbA = []\n",
        "for i in bler_Tb.values():\n",
        "  bler_TbA.append(i)\n",
        "\n",
        "#rmse_uw = np.log2(np.divide(rmse_uwA,message_factor) +1)\n",
        "#rmse_uw = (np.divide(rmse_uwA,message_factor) +1)\n",
        "bler_uw = bler_uwA\n",
        "\n",
        "#rmse_w = np.log2(np.divide(rmse_wA,message_factor) + 1)\n",
        "#rmse_w = (np.divide(rmse_wA,message_factor) + 1)\n",
        "bler_w = bler_wA\n",
        "\n",
        "\n",
        "#rmse_Tb = np.log2(np.divide(rmse_TbA,message_factor) +1)\n",
        "#rmse_Tb = (np.divide(rmse_TbA,message_factor))\n",
        "bler_Tb = bler_TbA\n",
        "bler_w[0] = 0\n",
        "message = np.arange(1,M)\n",
        "\n",
        "#print(bler_uw)\n",
        "plt.plot(message, bler_uw, '-o');\n",
        "plt.plot(message, bler_w, '-*');\n",
        "plt.plot(message, bler_Tb, '-+');\n",
        "plt.xlabel('Distance',fontsize=15)\n",
        "plt.ylabel('Absolute bler',fontsize=15)\n",
        "plt.ylim([0, 1])\n",
        "#plt.ylabel('log2 Norm RMSE deviation')\n",
        "plt.legend(['SPL', 'Weighted SPL','Baseline'],fontsize=15)\n",
        "#plt.legend(['Mixed LF', 'Cross Entropy','TB with CE'])\n",
        "plt.rcParams.update({'font.size': 15})\n",
        "\n",
        "print(np.sum(bler_uw))\n",
        "print(np.sum(bler_w))\n",
        "print(np.sum(bler_Tb))\n",
        "#print(np.sum(rmse_uwA))\n",
        "#print(np.sum(rmse_wA))\n",
        "#np.save('bler_w_100mW.npy',bler_w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sxostCb-vY1i",
        "outputId": "c85adb64-0394-4f07-c421-328677e4bf7e"
      },
      "outputs": [],
      "source": [
        "ae.plot_constellation(const_noise_SNR);\n",
        "#plot_constellation_2(ae,range(0,ae.M))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "75eUVLFnq1JO",
        "outputId": "97810c4e-99c3-4ee2-e569-1a6b07c6be87"
      },
      "outputs": [],
      "source": [
        "plot_constellation_2(ae_Weighted,range(0,ae_Weighted.M), const_noise_SNR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lFZkk0N3JD48",
        "outputId": "c9c6f7a1-8d0a-4542-bf6f-190df4c34adb"
      },
      "outputs": [],
      "source": [
        "plot_constellation_2(ae_Tb,range(0,ae_Tb.M), const_noise_SNR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFZCQNe9f7iD"
      },
      "source": [
        "### BLER Simulations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMl-Rljrf7iR"
      },
      "outputs": [],
      "source": [
        "#IEEE 802.11p Max TX Power 30dBm\n",
        "#Path loss model corresponding: 20log10(d(m))  (We take n=2, FSPL)\n",
        "#0.1W corresponds to 20dBm or -10dB\n",
        "#7dB corresponds to 5W\n",
        "#IEEE 802.11p is an approved amendment to the IEEE 802.11 standard to add wireless access in vehicular environments (WAVE), a vehicular communication system. \n",
        "#IEEE 802.11p standard typically uses channels of 10 MHz bandwidth in the 5.9 GHz band (5.850–5.925 GHz).\n",
        "# Noise Power -100dBm, TX Power is 20dBm and that \\\n",
        "#256m- -94dBm, 128m- -88dBm, 1m\n",
        "#2.8 Path loss exponent, -63dB at 5m, 20dBm TX Power, -95dBm NOise figure, net 52dB SNR at 5m distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.save('bler_w_100mW_n4.npy',bler_w)\n",
        "np.save('bler_uw_100mW_n4.npy',bler_uw)\n",
        "np.save('bler_Tb_100mW_n4.npy',bler_Tb)\n",
        "\n",
        "np.save('rmse_w_100mW_n4.npy',rmse_w)\n",
        "np.save('rmse_uw_100mW_n4.npy',rmse_uw)\n",
        "np.save('rmse_Tb_100mW_n4.npy',rmse_Tb)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
